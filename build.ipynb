{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e545aa12-cc06-4cb8-bfbc-dc539f2c0c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3540e265-b5bc-4632-9ca9-ff4e6551b2b0",
   "metadata": {},
   "source": [
    "## original_gpt2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e685b37b-3f62-44ac-945f-8d78d078b90e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/ShenakhtPajouh/gpt2-keras/blob/master/builder/original_gpt2.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib.training import HParams\n",
    "\n",
    "# def default_hparams():\n",
    "#     return HParams(\n",
    "#         n_vocab=0,\n",
    "#         n_ctx=1024,\n",
    "#         n_embd=768,\n",
    "#         n_head=12,\n",
    "#         n_layer=12,\n",
    "#     )\n",
    "\n",
    "def default_hparams():\n",
    "    return {\n",
    "        'n_vocab': 0,\n",
    "        'n_ctx': 1024,\n",
    "        'n_embd': 768,\n",
    "        'n_head': 12,\n",
    "        'n_layer': 12,\n",
    "  }\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - tf.reduce_max(x, axis=axis, keepdims=True)\n",
    "    ex = tf.exp(x)\n",
    "    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        n_state = x.shape[-1].value\n",
    "        g = tf.compat.v1.get_variable('g', [n_state], initializer=tf.constant_initializer(1))\n",
    "        b = tf.compat.v1.get_variable('b', [n_state], initializer=tf.constant_initializer(0))\n",
    "        u = tf.reduce_mean(x, axis=axis, keepdims=True)\n",
    "        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n",
    "        x = (x - u) * tf.rsqrt(s + epsilon)\n",
    "        x = x*g + b\n",
    "        return x\n",
    "\n",
    "def split_states(x, n):\n",
    "    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
    "    *start, m = shape_list(x)\n",
    "    return tf.reshape(x, start + [n, m//n])\n",
    "\n",
    "def merge_states(x):\n",
    "    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
    "    *start, a, b = shape_list(x)\n",
    "    return tf.reshape(x, start + [a*b])\n",
    "\n",
    "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        *start, nx = shape_list(x)\n",
    "        w = tf.compat.v1.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n",
    "        b = tf.compat.v1.get_variable('b', [nf], initializer=tf.constant_initializer(0))\n",
    "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "        return c\n",
    "\n",
    "def attention_mask(nd, ns, *, dtype):\n",
    "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "\n",
    "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "    \"\"\"\n",
    "    i = tf.range(nd)[:,None]\n",
    "    j = tf.range(ns)\n",
    "    m = i >= j - ns + nd\n",
    "    return tf.cast(m, dtype)\n",
    "\n",
    "\n",
    "def attn(x, scope, n_state, *, past, hparams):\n",
    "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
    "    assert n_state % hparams['n_head'] == 0\n",
    "    if past is not None:\n",
    "        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "    def split_heads(x):\n",
    "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "        return tf.transpose(split_states(x, hparams['n_head']), [0, 2, 1, 3])\n",
    "\n",
    "    def merge_heads(x):\n",
    "        # Reverse of split_heads\n",
    "        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
    "\n",
    "    def mask_attn_weights(w):\n",
    "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "        _, _, nd, ns = shape_list(w)\n",
    "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "        b = tf.reshape(b, [1, 1, nd, ns])\n",
    "        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
    "        return w\n",
    "\n",
    "    def multihead_attn(q, k, v):\n",
    "        # q, k, v have shape [batch, heads, sequence, features]\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n",
    "\n",
    "        w = mask_attn_weights(w)\n",
    "        w = softmax(w)\n",
    "        a = tf.matmul(w, v)\n",
    "        return a\n",
    "\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3)\n",
    "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
    "        present = tf.stack([k, v], axis=1)\n",
    "        if past is not None:\n",
    "            pk, pv = tf.unstack(past, axis=1)\n",
    "            k = tf.concat([pk, k], axis=-2)\n",
    "            v = tf.concat([pv, v], axis=-2)\n",
    "        a = multihead_attn(q, k, v)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state)\n",
    "        return a, present\n",
    "\n",
    "\n",
    "def mlp(x, scope, n_state, *, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1].value\n",
    "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "        h2 = conv1d(h, 'c_proj', nx)\n",
    "        return h2\n",
    "\n",
    "\n",
    "def block(x, scope, *, past, hparams):\n",
    "    print(\"..block: x=\", x)\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1].value\n",
    "        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
    "        x = x + a\n",
    "        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "        x = x + m\n",
    "        return x, present\n",
    "\n",
    "def past_shape(*, hparams, batch_size=None, sequence=None):\n",
    "    return [batch_size, hparams['n_layer'], 2, hparams['n_head'], sequence, hparams['n_embd'] // hparams['n_head']]\n",
    "\n",
    "def expand_tile(value, size):\n",
    "    \"\"\"Add a new axis of given size.\"\"\"\n",
    "    value = tf.convert_to_tensor(value, name='value')\n",
    "    ndims = value.shape.ndims\n",
    "    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
    "\n",
    "def positions_for(tokens, past_length):\n",
    "    batch_size = tf.shape(tokens)[0]\n",
    "    nsteps = tf.shape(tokens)[1]\n",
    "    return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
    "\n",
    "\n",
    "def modelx(hparams, X, past=None, scope='model', reuse=False):\n",
    "    print(\"..modelx: hparams=\", hparams)\n",
    "    print(\"..modelx: X=\", X)\n",
    "    with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        wpe = tf.compat.v1.get_variable('wpe', [hparams['n_ctx'], hparams['n_embd']],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        wte = tf.compat.v1.get_variable('wte', [hparams['n_vocab'], hparams['n_embd']],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "        print(\"..modelxx: h=\", h)\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams['n_layer']\n",
    "        assert len(pasts) == hparams['n_layer']\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f')\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams['n_embd']])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams['n_vocab']])\n",
    "        results['logits'] = logits\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd9a6978-4f94-4a78-8b4f-fcbdea39f013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## builder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45fc569b-f9e1-4317-9ec7-7ab49966627a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From https://github.com/ShenakhtPajouh/gpt2-keras/blob/master/builder/builder.py\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gpt2\n",
    "# import original_gpt2\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "class ReArrange(object):\n",
    "    \"\"\"\n",
    "    Map original model weights to our tf.keras model weights.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def Embedding(cls, weights):\n",
    "        weights = [weights[1], weights[0]]\n",
    "        new_weights = []\n",
    "        for w in weights:\n",
    "            name = w.name.split(\":\")[0]\n",
    "            name = name + \"/rearrange\"\n",
    "            new_w = tf.identity(w, name=name)\n",
    "            new_weights.append(new_w)\n",
    "        return new_weights\n",
    "\n",
    "    @classmethod\n",
    "    def LayerNorm(cls, weights):\n",
    "        weights = [weights[1], weights[0]]\n",
    "        new_weights = []\n",
    "        for w in weights:\n",
    "            name = w.name.split(\":\")[0]\n",
    "            name = name + \"/rearrange\"\n",
    "            new_w = tf.identity(w, name=name)\n",
    "            new_weights.append(new_w)\n",
    "        return new_weights\n",
    "\n",
    "    @classmethod\n",
    "    def SelfAttention(cls, weights):\n",
    "        kernels = []\n",
    "        biases = []\n",
    "        new_names = [\"query\", \"key\", \"value\"]\n",
    "        kernel = weights[0]\n",
    "        name = kernel.name.split(\":\")[0]\n",
    "        shape = kernel.shape.as_list()\n",
    "        new_shape = [shape[1], 3, shape[2] // 3]\n",
    "        kernel = tf.reshape(kernel, new_shape)\n",
    "        _kernels = tf.unstack(kernel, axis=1)\n",
    "        for kernel, nm in zip(_kernels, new_names):\n",
    "            _kernel = tf.identity(kernel, name=name + \"/rearrange/\" + nm)\n",
    "            kernels.append(_kernel)\n",
    "        bias = weights[1]\n",
    "        name = bias.name.split(\":\")[0]\n",
    "        shape = bias.shape.as_list()\n",
    "        new_shape = [3, shape[0] // 3]\n",
    "        bias = tf.reshape(bias, new_shape)\n",
    "        _biases = tf.unstack(bias, axis=0)\n",
    "        for bias, nm in zip(_biases, new_names):\n",
    "            _bias = tf.identity(bias, name=name + \"/rearrange/\" + nm)\n",
    "            biases.append(_bias)\n",
    "        weights = []\n",
    "        for kernel, bias in zip(kernels, biases):\n",
    "            weights.append(kernel)\n",
    "            weights.append(bias)\n",
    "        return weights\n",
    "\n",
    "    @classmethod\n",
    "    def Dense(cls, weights):\n",
    "        kernel = weights[0]\n",
    "        name = kernel.name.split(\":\")[0]\n",
    "        kernel = tf.squeeze(kernel, 0, name=name + \"/rearrange\")\n",
    "        bias = weights[1]\n",
    "        name = bias.name.split(\":\")[0]\n",
    "        bias = tf.identity(bias, name=name + \"/rearrange\")\n",
    "        return [kernel, bias]\n",
    "\n",
    "    @classmethod\n",
    "    def Attention(cls, weights):\n",
    "        new_weights = []\n",
    "        new_weights = new_weights + cls.LayerNorm(weights[0:2])\n",
    "        new_weights = new_weights + cls.SelfAttention(weights[2:4])\n",
    "        new_weights = new_weights + cls.Dense(weights[4:6])\n",
    "        return new_weights\n",
    "\n",
    "    @classmethod\n",
    "    def MLP(cls, weights):\n",
    "        new_weights = []\n",
    "        new_weights = new_weights + cls.LayerNorm(weights[0:2])\n",
    "        new_weights = new_weights + cls.Dense(weights[2:4])\n",
    "        new_weights = new_weights + cls.Dense(weights[4:6])\n",
    "        return new_weights\n",
    "\n",
    "    @classmethod\n",
    "    def Block(cls, weights):\n",
    "        return cls.Attention(weights[0:6]) + cls.MLP(weights[6:12])\n",
    "\n",
    "    @classmethod\n",
    "    def Transformer(cls, weights):\n",
    "        blocks_num = (len(weights) - 2) // 12\n",
    "        blocks_weights = weights[0:-2]\n",
    "        new_weights = []\n",
    "        for block in range(blocks_num):\n",
    "            new_weights = new_weights + cls.Block(blocks_weights[block * 12:(block + 1) * 12])\n",
    "        new_weights = new_weights + cls.LayerNorm(weights[-2:])\n",
    "        return new_weights\n",
    "\n",
    "    @classmethod\n",
    "    def GPT2(cls, weights):\n",
    "        return cls.Embedding(weights[0:2]) + cls.Transformer(weights[2:])\n",
    "\n",
    "\n",
    "def build(config, checkpoint_path, session=None, name=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Build a GPT2 model (in tf.keras format) from pre-trained original checkpoint.\n",
    "\n",
    "    Args:\n",
    "        config: A dictionary, for model hyper parameters.\n",
    "        checkpoint_path: path of original checkpoint.\n",
    "        session: since it's needed to load weights on tf.Session in Graph Execution, a session should be\n",
    "                 passed to this method, or else the method uses default session if it exist. in Eager Execution\n",
    "                 there is no need for this argument.\n",
    "        name: name of model.\n",
    "\n",
    "    Returns:\n",
    "        a GPT2 model which the pre-trained weights are loaded.\n",
    "\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = \"gpt2\"\n",
    "    # conf = tf.ConfigProto(device_count={'GPU': 0})\n",
    "    conf = tf.compat.v1.ConfigProto(device_count={'GPU': 0})\n",
    "    with open(config) as f:\n",
    "        config = json.load(f)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        x = tf.ones(shape=(1, 1), dtype=tf.int32)\n",
    "        # hparams = original_gpt2.default_hparams()\n",
    "        hparams = default_hparams()\n",
    "        # hparams.override_from_dict(config)\n",
    "        hparams.update(config)\n",
    "        # _ = original_gpt2.model(hparams, x)\n",
    "        _ = modelx(hparams, x)\n",
    "        original_weights = tf.compat.v1.global_variables()\n",
    "        original_weights = ReArrange.GPT2(original_weights)\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session(config=conf, graph=graph)\n",
    "        saver.restore(sess=sess, save_path=checkpoint_path)\n",
    "    original_weights = sess.run(original_weights)\n",
    "    sess.close()\n",
    "    eager = session is None and tf.executing_eagerly()\n",
    "    def _build():\n",
    "        x = tf.ones(shape=(1, 1), dtype=tf.int32)\n",
    "        model = gpt2.GPT2(config=config, name=name)\n",
    "        y = model(x)\n",
    "        weights = model.weights\n",
    "        assigns = [u.assign(v) for u, v in zip(weights, original_weights)]\n",
    "        return model, assigns\n",
    "    if eager:\n",
    "        model, _ = _build()\n",
    "    else:\n",
    "        if session is None:\n",
    "            try:\n",
    "                session = tf.get_default_session()\n",
    "            except:\n",
    "                raise Exception(\"No session is given and there is no default session\")\n",
    "        with session.graph.as_default():\n",
    "            model, assigns = _build()\n",
    "        session.run(assigns)\n",
    "    return model\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--config\", help=\"config file path\", required=True)\n",
    "#     parser.add_argument(\"--checkpoint\", help=\"checkpoint file path\", required=True)\n",
    "#     parser.add_argument(\"--target\", help=\"target h5 file path\", required=True)\n",
    "#     args = parser.parse_args()\n",
    "#     # conf = tf.ConfigProto(device_count={'GPU': 0})\n",
    "#     conf = tf.compat.v1.ConfigProto(device_count={'GPU': 0})\n",
    "#     tf.enable_eager_execution(config=conf)\n",
    "#     model = build(args.config, args.checkpoint, name=\"bert\")\n",
    "#     model.save_weights(args.target, save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a6287d3-d702-4012-b867-704e260a231c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = \"ch05/01_main-chapter-code/gpt2/124M/hparams.json\"\n",
    "checkpoint =\" ch05/01_main-chapter-code/gpt2/124M/model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00d23352-3c7c-4a25-82dd-2534e011751c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..modelx: hparams= {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "..modelx: X= Tensor(\"ones:0\", shape=(1, 1), dtype=int32)\n",
      "..modelxx: h= Tensor(\"model/add_1:0\", shape=(1, 1, 768), dtype=float32)\n",
      "..block: x= Tensor(\"model/add_1:0\", shape=(1, 1, 768), dtype=float32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m zmodel \u001b[38;5;241m=\u001b[39m \u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 143\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(config, checkpoint_path, session, name)\u001b[0m\n\u001b[1;32m    141\u001b[0m hparams\u001b[38;5;241m.\u001b[39mupdate(config)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# _ = original_gpt2.model(hparams, x)\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodelx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m original_weights \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mglobal_variables()\n\u001b[1;32m    145\u001b[0m original_weights \u001b[38;5;241m=\u001b[39m ReArrange\u001b[38;5;241m.\u001b[39mGPT2(original_weights)\n",
      "Cell \u001b[0;32mIn[18], line 177\u001b[0m, in \u001b[0;36mmodelx\u001b[0;34m(hparams, X, past, scope, reuse)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pasts) \u001b[38;5;241m==\u001b[39m hparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_layer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer, past \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pasts):\n\u001b[0;32m--> 177\u001b[0m     h, present \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     presents\u001b[38;5;241m.\u001b[39mappend(present)\n\u001b[1;32m    179\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpresent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstack(presents, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 136\u001b[0m, in \u001b[0;36mblock\u001b[0;34m(x, scope, past, hparams)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..block: x=\u001b[39m\u001b[38;5;124m\"\u001b[39m, x)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mvariable_scope(scope):\n\u001b[0;32m--> 136\u001b[0m     nx \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n\u001b[1;32m    137\u001b[0m     a, present \u001b[38;5;241m=\u001b[39m attn(norm(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mln_1\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn\u001b[39m\u001b[38;5;124m'\u001b[39m, nx, past\u001b[38;5;241m=\u001b[39mpast, hparams\u001b[38;5;241m=\u001b[39mhparams)\n\u001b[1;32m    138\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m a\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "zmodel = build(config=config, checkpoint_path=checkpoint, name=\"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d55f7-c8e7-40ac-9408-00fd8f2d639a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
