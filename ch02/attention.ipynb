{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "211966b2-c71f-45a2-8526-04fdbeca8b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5baf34-1dd2-471e-a9b4-1867c4954aa3",
   "metadata": {},
   "source": [
    "### 3.3.1 A Simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b19e19e-b1c6-460d-b9e4-a7770a0c3d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeds = [\n",
    "    [0.43, 0.15, 0.89], # Your \n",
    "    [0.55, 0.87, 0.66], # journey\n",
    "    [0.57, 0.85, 0.64], # starts\n",
    "    [0.22, 0.58, 0.33], # with\n",
    "    [0.77, 0.25, 0.10], # one\n",
    "    [0.05, 0.80, 0.55]  # step\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41522242-7857-465d-b765-7cfd38c5271c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor(embeds)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6924bf6-8a60-47c3-b56b-fa6a2271bdee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores_2: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "attn_weights_2: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "row num: 0 weight: tensor(0.1385) row: tensor([0.4300, 0.1500, 0.8900]) weighted row: tensor([0.0596, 0.0208, 0.1233])\n",
      "row num: 1 weight: tensor(0.2379) row: tensor([0.5500, 0.8700, 0.6600]) weighted row: tensor([0.1308, 0.2070, 0.1570])\n",
      "row num: 2 weight: tensor(0.2333) row: tensor([0.5700, 0.8500, 0.6400]) weighted row: tensor([0.1330, 0.1983, 0.1493])\n",
      "row num: 3 weight: tensor(0.1240) row: tensor([0.2200, 0.5800, 0.3300]) weighted row: tensor([0.0273, 0.0719, 0.0409])\n",
      "row num: 4 weight: tensor(0.1082) row: tensor([0.7700, 0.2500, 0.1000]) weighted row: tensor([0.0833, 0.0270, 0.0108])\n",
      "row num: 5 weight: tensor(0.1581) row: tensor([0.0500, 0.8000, 0.5500]) weighted row: tensor([0.0079, 0.1265, 0.0870])\n",
      "context_vec_2: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "# attn_scores_2 is row 1 (\"journey\") dotted with each other row \n",
    "for i, x_i in enumerate(inputs):\n",
    "    # print(i, x_i)\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(\"attn_scores_2:\", attn_scores_2)\n",
    "# attn_weights_2 is softmax of attn_scores_2\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"attn_weights_2:\", attn_weights_2)\n",
    "context_vec_2 = torch.zeros(query.shape) # tensor([0., 0., 0.])\n",
    "# weight every input row by its attention weight\n",
    "for i, x_i in enumerate(inputs):\n",
    "    print(\"row num:\", i, \"weight:\", attn_weights_2[i], \"row:\", x_i, \"weighted row:\", attn_weights_2[i] * x_i)\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "# context_vec_2 is the weighted sum of all the input vectors,\n",
    "# obtained by multiplying each input vector by its corresponding attention weight.\n",
    "print(\"context_vec_2:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6a244-27c6-4d79-9262-63f1399106bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3.2 Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f213eaa-fc5c-4617-b8fe-ce1d9c21c9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "attn_scores:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "attn_weights:\n",
      " tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "all_context_vecs:\n",
      " tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6,6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "      attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(\"attn_scores:\\n\", attn_scores)\n",
    "# equivalently, using matrix multiplication:\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(\"attn_scores:\\n\", attn_scores)\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(\"attn_weights:\\n\", attn_weights)\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(\"all_context_vecs:\\n\", all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c809ea-16b3-4ed2-ac28-329f664d4b77",
   "metadata": {},
   "source": [
    "### 3.3.2 bis - SelfAttention_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80bb77d0-3f74-4a88-a39c-32fcb3febbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttenion_v0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        keys = x\n",
    "        queries = x\n",
    "        values = x\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        all_context_vecs = attn_weights @ values\n",
    "        return all_context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab738682-87f3-4413-bd78-e35a90734a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v0 = SelfAttenion_v0()\n",
    "c = sa_v0(inputs)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db65a1f-7b3e-4697-8b12-3c6ec8ae4edb",
   "metadata": {},
   "source": [
    "## 3.4 Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a4a62-4e1d-4a3d-8d1a-8ed3389b878e",
   "metadata": {},
   "source": [
    "### 3.4.1 Computing the attention weights step by step (pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c5e02ae-32c8-4b7a-9f75-a8c555a30c38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_2: tensor([0.5500, 0.8700, 0.6600])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "print(\"x_2:\", x_2)\n",
    "d_in = inputs.shape[1] # input embedding size = 3\n",
    "d_out = 2\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b710837-c160-4247-86fc-7acf08f9d61c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0842f4cf-3072-46a9-bc78-1df76d23f323",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5500) tensor([0.2961, 0.5166])\n",
      "tensor(0.8700) tensor([0.2517, 0.6886])\n",
      "tensor(0.6600) tensor([0.0740, 0.8665])\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(x_2):\n",
    "  print(v, W_query[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "552d00f2-e283-4435-be07-921530cb7d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cae013de-0ffe-42ae-9b3a-8eb726e68e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores_2: tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
      "attn_weights_2:\n",
      " tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "context_vec_2:\n",
      " tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "for i, key_i in enumerate(keys):\n",
    "    # print(i, x_i)\n",
    "    attn_scores_2[i] = torch.dot(key_i, query_2)\n",
    "print(\"attn_scores_2:\", attn_scores_2)\n",
    "d_k = keys.shape[-1] # 2\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1) # scale by 1/1.414\n",
    "print(\"attn_weights_2:\\n\", attn_weights_2)\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"context_vec_2:\\n\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21a913b4-73be-4051-9b08-2d86109200f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    def get_query_weights(self):\n",
    "        return self.W_query.detach().numpy()\n",
    "    def get_key_weights(self):\n",
    "        return self.W_key.detach().numpy()\n",
    "    def get_value_weights(self):\n",
    "        return self.W_value.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6844b08a-17aa-41a9-9915-60d02fe20d07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out) # 3,2\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "48996cb4-9c85-422d-acc3-4c3df3adb3b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07563531, 0.19663817],\n",
       "       [0.31641197, 0.40174013],\n",
       "       [0.1185683 , 0.8273954 ]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v1.get_value_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420973a7-27f1-48e0-9d59-709474c6d424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Try with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "261ceb3e-ceb1-4224-a456-15db4c31bc66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54a9ad50-7870-4963-8048-4e12a2df6065",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights:\n",
      " tf.Tensor(\n",
      "[[0.20983477 0.20058146 0.19814923 0.12422822 0.12204872 0.14515765]\n",
      " [0.13854757 0.2378913  0.23327403 0.12399159 0.10818186 0.15811361]\n",
      " [0.1390076  0.23692146 0.23260196 0.1242044  0.11080021 0.15646443]\n",
      " [0.1435269  0.20739442 0.20455202 0.14619224 0.12629524 0.1720392 ]\n",
      " [0.15261085 0.19583867 0.19749065 0.13668667 0.18785891 0.12951429]\n",
      " [0.13847117 0.21836372 0.21275944 0.14204757 0.09880637 0.18955176]], shape=(6, 6), dtype=float32)\n",
      "all_context_vecs:\n",
      " tf.Tensor(\n",
      "[[0.44205943 0.59309864 0.57898915]\n",
      " [0.44186574 0.651482   0.56830883]\n",
      " [0.44312754 0.6495946  0.5670731 ]\n",
      " [0.4303897  0.6298281  0.55102706]\n",
      " [0.46710175 0.59099275 0.52659655]\n",
      " [0.4177245  0.6503232  0.56453526]], shape=(6, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# queries, keys and values are all the same matrix?\n",
    "queries = np.array(embeds)\n",
    "keys = np.array(embeds)\n",
    "values = np.array(embeds)\n",
    "\n",
    "# attn_scores = np.matmul(embeds, keys.T)\n",
    "attn_scores = tf.matmul(embeds, keys.T)\n",
    "\n",
    "attn_weights = tf.keras.activations.softmax(attn_scores, axis=1)\n",
    "print(\"attn_weights:\\n\", attn_weights)\n",
    "all_context_vecs = tf.matmul(attn_weights, keys)\n",
    "print(\"all_context_vecs:\\n\", all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b1065-18c0-46c0-9296-7e9aff4e8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pure hack starting here..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3b6c4b2-a7f0-48ac-b41b-61bb402953cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parms = nn.Parameter(torch.rand(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "906345d6-6386-4eb7-ba4c-d33ef8803ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13657987, 0.10247904],\n",
       "       [0.18405646, 0.72644675],\n",
       "       [0.3152539 , 0.68710667]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parms.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b9d51fa-4418-4b0c-bec3-e2225388070b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = torch.rand(3, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ce78b54-4cf3-4efe-a84a-e65812b2e4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2744953 , 0.6583756 ],\n",
       "       [0.27754194, 0.85732484],\n",
       "       [0.89932823, 0.03901386]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_parms = t.numpy()\n",
    "fake_parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82380cab-2856-458a-9f87-343d9130a745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# out_dim = 5 input_shape=(3,4)\n",
    "fake_weights = np.array([\n",
    "  [0.00, 0.00, 0.00, 0.00, 0.00], # 0\n",
    "  [0.11, 0.11, 0.11, 0.11, 0.11], # 1\n",
    "  [0.22, 0.22, 0.22, 0.22, 0.22], # 2\n",
    "  [0.33, 0.33, 0.33, 0.33, 0.33], # 3\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ecbb7ad-49cb-498a-b578-2aa582d347e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.2744953 , 0.6583756 ],\n",
       "        [0.27754194, 0.85732484],\n",
       "        [0.89932823, 0.03901386]], dtype=float32)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dim = 2\n",
    "\n",
    "linear = tf.keras.layers.Dense(name='my_dense', units=out_dim, use_bias=False, input_shape=(3,3), weights=[fake_parms])\n",
    "model = tf.keras.Sequential([linear])\n",
    "model.compile()\n",
    "linear.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c3476df-9271-48ee-af60-14142b58f09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_inputs = tf.constant(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a6d1552-ac30-4363-af03-a538111690b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 2), dtype=float32, numpy=\n",
       "array([[0.9600664 , 0.44642258],\n",
       "       [0.9859906 , 1.1337284 ],\n",
       "       [0.967943  , 1.1289691 ],\n",
       "       [0.5181416 , 0.65496564],\n",
       "       [0.3706797 , 0.72518176],\n",
       "       [0.7303888 , 0.7402362 ]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(tf_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a6b6f794-8298-4121-a38f-ea3921a839aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Tensorflow equivalent !?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c643046f-8a33-41db-8bf7-efc7f578c44a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "class SelfAttention_v2_TF(layers.Layer):\n",
    "\n",
    "    def __init__(self, d_out, qkv_bias=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the self-attention layer.\n",
    "        \n",
    "        Args:\n",
    "            d_out (int): The dimension of the query, key, and value vectors (d_k and d_v).\n",
    "            qkv_bias (bool): Whether to include a bias in the Q, K, V linear layers.\n",
    "            **kwargs: Standard Keras layer arguments.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_out = d_out\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.scale_factor = np.sqrt(d_out)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Creates the weights of the layer.\n",
    "        input_shape is typically (batch_size, sequence_length, d_in)\n",
    "        \"\"\"\n",
    "        d_in = input_shape[-1]\n",
    "        \n",
    "        # Keras uses 'kernel' for weights and 'bias' for bias\n",
    "        self.W_query = layers.Dense(self.d_out, use_bias=self.qkv_bias, name='W_query')\n",
    "        self.W_key = layers.Dense(self.d_out, use_bias=self.qkv_bias, name='W_key')\n",
    "        self.W_value = layers.Dense(self.d_out, use_bias=self.qkv_bias, name='W_value')\n",
    "        \n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Performs the self-attention mechanism.\n",
    "        x shape: (batch_size, sequence_length, d_in)\n",
    "        \"\"\"\n",
    "        # 1. Project input (x) into Q, K, V\n",
    "        keys = self.W_key(x)      # keys shape:   (B, L, d_out)\n",
    "        queries = self.W_query(x) # queries shape: (B, L, d_out)\n",
    "        values = self.W_value(x)  # values shape:  (B, L, d_out)\n",
    "        \n",
    "        # 2. Calculate Attention Scores (Q @ K^T)\n",
    "        # Use tf.transpose(keys, perm=[0, 2, 1]) to transpose the last two dimensions (L and D)\n",
    "        # tf.matmul performs batch matrix multiplication on the last two dimensions.\n",
    "        # attn_scores shape: (B, L, L)\n",
    "        attn_scores = tf.matmul(queries, keys, transpose_b=True)\n",
    "        \n",
    "        # 3. Scale, Softmax to get Attention Weights\n",
    "        # Note: In TensorFlow, dividing by a constant scales the tensor.\n",
    "        scaled_scores = attn_scores / self.scale_factor\n",
    "        \n",
    "        # Apply softmax across the last dimension (the sequence length dimension, L)\n",
    "        attn_weights = tf.nn.softmax(scaled_scores, axis=-1)\n",
    "\n",
    "        # 4. Calculate Context Vector (Attention_Weights @ Values)\n",
    "        # context_vec shape: (B, L, d_out)\n",
    "        context_vec = tf.matmul(attn_weights, values)\n",
    "        \n",
    "        return context_vec\n",
    "\n",
    "    def get_config(self):\n",
    "        # Keras requirement for saving/loading the model\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_out\": self.d_out,\n",
    "            \"qkv_bias\": self.qkv_bias,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Example Usage:\n",
    "# Define input with shape (Batch=16, Sequence_Length=20, d_in=128)\n",
    "# input_data = tf.random.normal(shape=(16, 20, 128))\n",
    "# \n",
    "# # Initialize the layer (d_out=64)\n",
    "# attn_layer = SelfAttention_v2_TF(d_out=64)\n",
    "# \n",
    "# # Run the forward pass\n",
    "# # output = attn_layer(input_data)\n",
    "# # print(output.shape)  # Expected shape: (16, 20, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ceaf3d03-0ea6-495b-828f-2803ba7d67f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 20, 64)\n"
     ]
    }
   ],
   "source": [
    "input_data = tf.random.normal(shape=(16, 20, 128))\n",
    "attn_layer = SelfAttention_v2_TF(d_out=64)\n",
    "output = attn_layer(input_data)\n",
    "print(output.shape)  # Expected shape: (16, 20, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a05ab8a-b055-4e53-aad2-03fd1cc15da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
