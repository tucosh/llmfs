{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be2b182-9bc2-414a-b334-44d3a0854cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8d4f8b-6c43-4128-bc96-3fe01b01bead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeds = [\n",
    "    [0.43, 0.15, 0.89], # Your \n",
    "    [0.55, 0.87, 0.66], # journey\n",
    "    [0.57, 0.85, 0.64], # starts\n",
    "    [0.22, 0.58, 0.33], # with\n",
    "    [0.77, 0.25, 0.10], # one\n",
    "    [0.05, 0.80, 0.55]  # step\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "463496dd-4fc5-4db9-9e45-344eadb42537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477ebbdb-5744-430e-9dd8-d205c7b87e13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bca1c-5772-4c3f-a07c-e818daeedfdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99b5faa9-1675-406f-9242-d0c63a758db8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# torch.manual_seed(123)\n",
    "# d_in = 3\n",
    "# d_out = 2\n",
    "# q_weight     = nn.Parameter(torch.rand(d_in, d_out))\n",
    "# key_weight   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "# value_weight = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "# print(\"q_weight:\\n\", q_weight)\n",
    "\n",
    "# print(\"key_weight:\\n\", key_weight)\n",
    "\n",
    "# print(\"value_weight:\\n\", value_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "408ac255-b144-43be-91b8-91bd6fda05a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        # print(\"keys:\", keys)\n",
    "        queries = x @ self.W_query\n",
    "        # print(\"queries:\", queries)\n",
    "        values = x @ self.W_value\n",
    "        # print(\"values:\", values) \n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        # print(\"attn_scores:\", attn_scores) \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        # print(\"attn_weights:\", attn_weights) \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "043d247d-a5c1-4598-9711-5bf3c528271f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sa_v1.W_query\n",
    "# sa_v1.W_key\n",
    "# sa_v1.W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "402d9d7c-6090-444b-9dc3-4ce33f99c8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b610ca47-803d-4c8e-8063-865f8f79ed2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "values:\n",
      " tensor([[[-0.4519,  0.2216],\n",
      "         [-0.7142, -0.1961],\n",
      "         [-0.7127, -0.1971],\n",
      "         [-0.3809, -0.1557],\n",
      "         [-0.4861, -0.1597],\n",
      "         [-0.4213, -0.1501]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.7142, -0.1961],\n",
      "         [-0.7127, -0.1971],\n",
      "         [-0.3809, -0.1557],\n",
      "         [-0.4861, -0.1597],\n",
      "         [-0.4213, -0.1501]]], grad_fn=<UnsafeViewBackward0>)\n",
      "attn_scores:\n",
      " tensor([[[0.3111, 0.3479, 0.3471, 0.1714, 0.2350, 0.1928],\n",
      "         [0.1655, 0.2602, 0.2576, 0.1445, 0.1384, 0.1790],\n",
      "         [0.1667, 0.2602, 0.2577, 0.1443, 0.1391, 0.1784],\n",
      "         [0.0510, 0.1080, 0.1064, 0.0643, 0.0476, 0.0835],\n",
      "         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121, 0.1174],\n",
      "         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
      "\n",
      "        [[0.3111, 0.3479, 0.3471, 0.1714, 0.2350, 0.1928],\n",
      "         [0.1655, 0.2602, 0.2576, 0.1445, 0.1384, 0.1790],\n",
      "         [0.1667, 0.2602, 0.2577, 0.1443, 0.1391, 0.1784],\n",
      "         [0.0510, 0.1080, 0.1064, 0.0643, 0.0476, 0.0835],\n",
      "         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121, 0.1174],\n",
      "         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"x:\\n\", x)\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        # For inputs where `num_tokens` exceeds `context_length`, this will result in errors\n",
    "        # in the mask creation further below.\n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method. \n",
    "        keys = self.W_key(x)        \n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        print(\"values:\\n\", values)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        print(\"attn_scores:\\n\", attn_scores)\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c62601-7350-4e18-b5fa-d7920f48c88e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6383d4cb-e363-4cab-96ce-2b942131bb0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tf.Tensor([0.43 0.15 0.89], shape=(3,), dtype=float32)\n",
      "1 tf.Tensor([0.55 0.87 0.66], shape=(3,), dtype=float32)\n",
      "2 tf.Tensor([0.57 0.85 0.64], shape=(3,), dtype=float32)\n",
      "3 tf.Tensor([0.22 0.58 0.33], shape=(3,), dtype=float32)\n",
      "4 tf.Tensor([0.77 0.25 0.1 ], shape=(3,), dtype=float32)\n",
      "5 tf.Tensor([0.05 0.8  0.55], shape=(3,), dtype=float32)\n",
      "attn_scores_2: tf.Tensor([0.95440006 1.4950001  1.4754001  0.8434     0.707      1.0865    ], shape=(6,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.constant(embeds)\n",
    "query = inputs[1]\n",
    "attn_scores_2 = np.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    print(i, x_i)\n",
    "    attn_scores_2[i] = x_i.numpy() @ query.numpy()\n",
    "attn_scores_2 = tf.constant(attn_scores_2, dtype=tf.float32)    \n",
    "print(\"attn_scores_2:\", attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24cea333-30e2-418f-ba86-5172eb617c79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights_2: tf.Tensor([0.13854761 0.23789133 0.23327404 0.1239916  0.10818188 0.15811363], shape=(6,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# attn_weights_2 is softmax of attn_scores_2\n",
    "attn_weights_2 = tf.nn.softmax(attn_scores_2)\n",
    "print(\"attn_weights_2:\", attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0df0d78a-5abc-4dfa-9ad9-7cf2f02f1306",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 3), dtype=float32, numpy=\n",
       "array([[0.43, 0.15, 0.89],\n",
       "       [0.55, 0.87, 0.66],\n",
       "       [0.57, 0.85, 0.64],\n",
       "       [0.22, 0.58, 0.33],\n",
       "       [0.77, 0.25, 0.1 ],\n",
       "       [0.05, 0.8 , 0.55]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c12e896-1114-400a-9e97-c7892c038bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.44186577, 0.65148205, 0.56830895], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "context_vec2 = tf.tensordot(attn_weights_2, inputs, axes=1)\n",
    "context_vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "675421df-8e1f-4477-a3a4-def3b4e9b21d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8a00172-d503-4ee9-8881-5b8b2b72832c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttenion_v0(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        keys = x\n",
    "        queries = x\n",
    "        values = x\n",
    "\n",
    "        attn_scores = tf.matmul(queries, keys, transpose_b=True)\n",
    "        attn_weights = tf.nn.softmax(attn_scores)\n",
    "        all_context_vecs = tf.matmul(attn_weights, values, transpose_b=False)\n",
    "        return all_context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71b8491e-9082-4c1e-a258-255c995c81f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 3), dtype=float32, numpy=\n",
       "array([[0.44205937, 0.5930985 , 0.578989  ],\n",
       "       [0.4418658 , 0.65148205, 0.568309  ],\n",
       "       [0.44312754, 0.6495946 , 0.5670731 ],\n",
       "       [0.43038973, 0.6298281 , 0.55102706],\n",
       "       [0.46710175, 0.5909928 , 0.5265966 ],\n",
       "       [0.41772443, 0.65032315, 0.56453526]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttenion_v0()\n",
    "sa(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5d59ad0-ab44-44d9-9599-e91cfd93f2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dd7cd71-53d4-4bfe-87f7-8c9d32831805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weights from LLMFS listing 3.1\n",
    "qw = np.array([[0.2961, 0.5166],\n",
    "        [0.2517, 0.6886],\n",
    "        [0.0740, 0.8665]], dtype=np.float32)\n",
    "kw = np.array([[0.1366, 0.1025],\n",
    "        [0.1841, 0.7264],\n",
    "        [0.3153, 0.6871]], dtype=np.float32)\n",
    "vw = np.array([[0.0756, 0.1966],\n",
    "        [0.3164, 0.4017],\n",
    "        [0.1186, 0.8274]], dtype=np.float32)\n",
    "# dummy_input = tf.zeros(shape=(6, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "524961da-da8d-488e-829f-015954ae1556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Corresponds to LLMFS listing 3.2\n",
    "class SelfAttenion_v1(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        b = 0\n",
    "        self.query_layer = tf.keras.layers.Dense(units=2, activation=None, name=f\"query-{b}\", use_bias=False)\n",
    "        self.query_layer.build((None, 3 ))\n",
    "        self.key_layer = tf.keras.layers.Dense(units=2, activation=None, name=f\"key-{b}\", use_bias=False)\n",
    "        self.key_layer.build((None, 3))\n",
    "        self.value_layer = tf.keras.layers.Dense(units=2, activation=None, name=f\"value-{b}\", use_bias=False)\n",
    "        self.value_layer.build((None, 3))\n",
    "    def __call__(self, x):\n",
    "        keys = self.key_layer(x)\n",
    "        # print(\"keys:\", keys)\n",
    "        queries = self.query_layer(x)\n",
    "        # print(\"queries:\", queries)\n",
    "        values = self.value_layer(x)\n",
    "        # print(\"values:\", values)\n",
    "        attn_scores = tf.matmul(queries, keys, transpose_b=True)\n",
    "        # print(\"attn_scores:\", attn_scores)\n",
    "        attn_weights = tf.nn.softmax(attn_scores / keys.shape[-1]**0.5, axis=-1)\n",
    "        # print(\"attn_weights:\", attn_weights)\n",
    "        all_context_vecs = tf.matmul(attn_weights, values, transpose_b=False)\n",
    "        return all_context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0d07a0a-90f5-4072-bdab-5032862e044d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sa = SelfAttenion_v1()\n",
    "sa.query_layer.set_weights([qw])\n",
    "sa.key_layer.set_weights([kw])\n",
    "sa.value_layer.set_weights([vw])\n",
    "# sa.query_layer.get_config()\n",
    "\n",
    "# sa.query_layer(dummy_input)\n",
    "# sa.query_layer.set_weights([qw])\n",
    "# sa.key_layer.set_weights([kw])\n",
    "# sa.value_layer.set_weights([vw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fe02ebd-8983-47f8-a278-52cfa17d8f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.constant(embeds)\n",
    "result = sa(inputs)\n",
    "# LLMFS 3.4.2\n",
    "np.allclose(result[0], np.array([0.2996, 0.8053]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39456efc-8ba2-4089-9b63-de4710fe006e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9808d0e0-7ca7-4c93-a270-3368e3bc97bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qw:\n",
      " [[-0.13615717  0.10756382]\n",
      " [ 0.18532233  0.15787685]\n",
      " [ 0.4082695   0.55729234]]\n",
      "kw:\n",
      " [[-0.2603904   0.41260317]\n",
      " [ 0.18287641  0.4611045 ]\n",
      " [-0.25687245 -0.53230095]]\n",
      "vw:\n",
      " [[ 0.49285263  0.23768058]\n",
      " [ 0.27569306  0.47995073]\n",
      " [ 0.25159022 -0.07623307]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(123)\n",
    "d_in = 3\n",
    "d_out = 2\n",
    "qkv_bias = False\n",
    "qw = nn.Linear(d_in, d_out, bias=qkv_bias).weight.detach().numpy().T\n",
    "kw = nn.Linear(d_in, d_out, bias=qkv_bias).weight.detach().numpy().T\n",
    "vw = nn.Linear(d_in, d_out, bias=qkv_bias).weight.detach().numpy().T\n",
    "\n",
    "qw = nn.Linear(d_in, d_out, bias=qkv_bias).weight.detach().numpy().T\n",
    "kw = nn.Linear(d_in, d_out, bias=qkv_bias).weight.detach().numpy().T\n",
    "vw = nn.Linear(d_in, d_out, bias=qkv_bias).weight.detach().numpy().T\n",
    "\n",
    "print(\"qw:\\n\", qw)\n",
    "print(\"kw:\\n\", kw)\n",
    "print(\"vw:\\n\", vw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "722720b2-3b91-4fd8-af68-6c801400d9c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# corresponds to weights in CausalAttention LLMFS listing 3.3\n",
    "qw = np.array([[-0.23542964,  0.21772662],\n",
    " [ 0.01912448, -0.4919342 ],\n",
    " [-0.28674594,  0.42322308]])\n",
    "kw = np.array([[-0.4196414,   0.2614782 ],\n",
    " [-0.45901766, -0.2133264 ],\n",
    " [-0.36482018,  0.21605217]])\n",
    "vw = np.array([[-0.49001414, -0.11346072],\n",
    " [-0.35029206, -0.44043937],\n",
    " [-0.2119892,   0.37804362]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a104f42-374b-4d3f-9a16-10ad6782a948",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23542964,  0.21772662],\n",
       "       [ 0.01912448, -0.4919342 ],\n",
       "       [-0.28674594,  0.42322308]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fec24e54-5fb6-4c53-8303-f0a1ea0de3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Generates a boolean causal mask of shape (seq_len, seq_len).\n",
    "    \"\"\"\n",
    "    # Create a tensor of shape (seq_len, seq_len)\n",
    "    mask = tf.ones((seq_len, seq_len), dtype=tf.bool)\n",
    "    \n",
    "    # Use band_part to keep only the lower triangle (including the diagonal)\n",
    "    # num_lower: -1 means keep all lower-triangular parts\n",
    "    # num_upper: 0 means keep 0 upper-triangular parts (only the diagonal)\n",
    "    lower_triangular = tf.linalg.band_part(\n",
    "        mask, \n",
    "        num_lower=-1, \n",
    "        num_upper=0\n",
    "    )\n",
    "    \n",
    "    return lower_triangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a5f1096-85b3-4d5b-bc0c-75375f4699b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# causal_mask = create_causal_mask(6)\n",
    "# additive_mask = 1.0 - tf.cast(causal_mask, dtype=tf.float32)\n",
    "# additive_mask\n",
    "# large_negative_value = -1e9 \n",
    "# additive_mask_applied = additive_mask * large_negative_value\n",
    "# additive_mask_applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0767607e-a093-4ea5-b8a2-98c83c04c8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Corresponds to LLMFS listing 3.3\n",
    "class CausalAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        b = 0\n",
    "        self.query_layer = tf.keras.layers.Dense(units=2, activation=None, name=f\"query-{b}\", use_bias=False)\n",
    "        self.query_layer.build((None, 3))\n",
    "        self.key_layer = tf.keras.layers.Dense(units=2, activation=None, name=f\"key-{b}\", use_bias=False)\n",
    "        self.key_layer.build((None, 3))\n",
    "        self.value_layer = tf.keras.layers.Dense(units=2, activation=None, name=f\"value-{b}\", use_bias=False)\n",
    "        self.value_layer.build((None, 3))\n",
    "        causal_mask = create_causal_mask(6)\n",
    "        additive_mask = 1.0 - tf.cast(causal_mask, dtype=tf.float32)\n",
    "        large_negative_value = -1e9 \n",
    "        self.additive_mask_applied = additive_mask * large_negative_value\n",
    "    def __call__(self, x):\n",
    "        # b, \n",
    "        # print(\"x:\\n\", x)\n",
    "        print(\"x.shape:\", x.shape)\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.key_layer(x)\n",
    "        # print(\"self.key_layer:\\n\", self.key_layer.get_weights())\n",
    "        # print(\"keys:\\n\", keys)\n",
    "        queries = self.query_layer(x)\n",
    "        # print(\"queries:\", queries)\n",
    "        values = self.value_layer(x)\n",
    "        # print(\"values:\", values)\n",
    "        # attn_scores = tf.matmul(queries, keys, transpose_b=True)\n",
    "        coefficients = tf.matmul(queries, tf.transpose(keys, perm=[0,2,1]))\n",
    "        # coefficients = tf.matmul(queries, tf.transpose(keys, perm=[1,0]))\n",
    "        # print(\"coefficients:\", coefficients)\n",
    "\n",
    "        # mask = tf.linalg.band_part(tf.ones((num_tokens, num_tokens)), -1, 0)\n",
    "        # print(\"mask:\", mask)\n",
    "        attn_scores = coefficients + self.additive_mask_applied\n",
    "        # print(\"attn_scores:\", attn_scores)\n",
    "        attn_weights = tf.nn.softmax(attn_scores / keys.shape[-1]**0.5, axis=-1)\n",
    "        # print(\"attn_weights:\", attn_weights)\n",
    "        all_context_vecs = tf.matmul(attn_weights, values, transpose_b=False)\n",
    "        return all_context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6faa04d-8631-4bd0-af7a-2aceb3b3154e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ca = CausalAttention()\n",
    "ca.query_layer.set_weights([qw])\n",
    "ca.key_layer.set_weights([kw])\n",
    "ca.value_layer.set_weights([vw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b23a440-3daf-47f5-950b-3b12f9191f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant(embeds)\n",
    "batch = tf.stack((inputs, inputs))\n",
    "assert batch.shape == [2,6,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4f07456-6ffc-4420-bdbd-062679de4321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (2, 6, 3)\n"
     ]
    }
   ],
   "source": [
    "result = ca(batch)\n",
    "assert result.shape == [2,6,2]\n",
    "assert np.allclose(result[0][0], np.array([-0.4519,  0.2216]), rtol=1e-3, atol=1e-3) # LLMFS 3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "776ec317-c05c-469b-acb4-c25df0dca140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = []\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(CausalAttention())\n",
    "        \n",
    "\n",
    "    def __call__(self, x):\n",
    "        return tf.concat([head(x) for head in self.heads], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01dc327e-915b-4c49-99e1-ef2fe2d18ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (2, 6, 3)\n",
      "x.shape: (2, 6, 3)\n"
     ]
    }
   ],
   "source": [
    "#mhaw = MultiHeadAttentionWrapper(d_in=3, d_out=2, context_length=6, dropout=0, num_heads=2, qkv_bias=False)\n",
    "mhaw = MultiHeadAttentionWrapper(3, 2, 6, 0, num_heads=2, qkv_bias=False)\n",
    "mhaw.heads[0].query_layer.set_weights([qw])\n",
    "mhaw.heads[0].key_layer.set_weights([kw])\n",
    "mhaw.heads[0].value_layer.set_weights([vw])\n",
    "mhaw.heads[1].query_layer.set_weights([qw])\n",
    "mhaw.heads[1].key_layer.set_weights([kw])\n",
    "mhaw.heads[1].value_layer.set_weights([vw])\n",
    "result = mhaw(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a717ba56-0d55-43c8-9ac4-3414e698670f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6, 4), dtype=float32, numpy=\n",
       "array([[[-0.45192027,  0.22160481, -0.45192027,  0.22160481],\n",
       "        [-0.58743507,  0.0057761 , -0.58743507,  0.0057761 ],\n",
       "        [-0.63002306, -0.06318259, -0.63002306, -0.06318259],\n",
       "        [-0.5674567 , -0.08425315, -0.5674567 , -0.08425315],\n",
       "        [-0.55256176, -0.09806819, -0.55256176, -0.09806819],\n",
       "        [-0.5299009 , -0.10806764, -0.5299009 , -0.10806764]],\n",
       "\n",
       "       [[-0.45192027,  0.22160481, -0.45192027,  0.22160481],\n",
       "        [-0.58743507,  0.0057761 , -0.58743507,  0.0057761 ],\n",
       "        [-0.63002306, -0.06318259, -0.63002306, -0.06318259],\n",
       "        [-0.5674567 , -0.08425315, -0.5674567 , -0.08425315],\n",
       "        [-0.55256176, -0.09806819, -0.55256176, -0.09806819],\n",
       "        [-0.5299009 , -0.10806764, -0.5299009 , -0.10806764]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e30559c8-d64d-4b49-bbe2-43a6cdc9c979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f64a4ca-09a0-4c45-82b2-27df9ca25a58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # corresponds to weights in CausalAttention LLMFS listing 3.3\n",
    "# qw = np.array([[-0.23542964,  0.21772662],\n",
    "#  [ 0.01912448, -0.4919342 ],\n",
    "#  [-0.28674594,  0.42322308]])\n",
    "# kw = np.array([[-0.4196414,   0.2614782 ],\n",
    "#  [-0.45901766, -0.2133264 ],\n",
    "#  [-0.36482018,  0.21605217]])\n",
    "# vw = np.array([[-0.49001414, -0.11346072],\n",
    "#  [-0.35029206, -0.44043937],\n",
    "#  [-0.2119892,   0.37804362]])\n",
    "# out_proj_weight = np.array([[-0.1668, 0.5000], [0.2270,0.1317] ])\n",
    "\n",
    "# # [[-0.1668,  0.2270],\n",
    "# #         [ 0.5000,  0.1317]]\n",
    "# out_proj_bias = np.array([0.1934, 0.6825])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba584d-7309-4240-a413-9c33659c9188",
   "metadata": {},
   "source": [
    "## MultiHeadAttention\n",
    "\n",
    "Tensorflow implementation of the Pytorch MultiHeadAttention class in section 3.6 of https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/01_main-chapter-code/ch03.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54427e36-efef-4822-afe7-856b6d0f3cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        b = 0\n",
    "        self.query_layer = tf.keras.layers.Dense(units=d_out, activation=None, name=f\"query-{b}\", use_bias=qkv_bias)\n",
    "        self.query_layer.build((None, d_in))\n",
    "        self.key_layer = tf.keras.layers.Dense(units=d_out, activation=None, name=f\"key-{b}\", use_bias=qkv_bias)\n",
    "        self.key_layer.build((None, d_in))\n",
    "        self.value_layer = tf.keras.layers.Dense(units=d_out, activation=None, name=f\"value-{b}\", use_bias=qkv_bias)\n",
    "        self.value_layer.build((None, d_in))\n",
    "\n",
    "        self.out_proj = tf.keras.layers.Dense(units=d_out, activation=None, name=f\"proj-{b}\", use_bias=True)\n",
    "        self.out_proj.build((None, d_out))\n",
    "\n",
    "        mask = tf.ones((context_length, context_length), dtype=tf.bool) # square matrix of True\n",
    "        causal_mask = tf.linalg.band_part(mask, num_lower=-1, num_upper=0) # upper right becomes False\n",
    "        additive_mask = 1.0 - tf.cast(causal_mask, dtype=tf.float32) # upper right becomes 1.0\n",
    "        self.additive_mask_applied = additive_mask * -1e9   # upper right is large negative value\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "        # print(x.shape)\n",
    "        # print(\"x:\", x.shape, \"\\n\", x)\n",
    "        assert x.shape == [2,6,3]\n",
    "        assert np.allclose(x[0][0], np.array([0.43, 0.15, 0.89]), rtol=1e-3, atol=1e-3) # LLMFS 3.5.3\n",
    "\n",
    "        keys = self.key_layer(x)\n",
    "        assert keys.shape == [2,6,2]        \n",
    "        # print(\"keys:\", keys.shape, \"\\n\", keys)\n",
    "\n",
    "        queries = self.query_layer(x)\n",
    "        assert queries.shape == [2,6,2]\n",
    "        # print(\"queries:\", queries.shape, \"\\n\", queries)\n",
    "\n",
    "        values = self.value_layer(x)\n",
    "        assert values.shape == [2,6,2]\n",
    "        # print(\"values:\", values.shape, \"\\n\", values)\n",
    "\n",
    "\n",
    "        print(\".....reshape.......................\")\n",
    "        keys = tf.reshape(keys, [batch_size, num_tokens, self.num_heads, self.head_dim ]) # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim): 2, 6, 2 -> 2, 6, 2, 1\n",
    "        assert keys.shape == [2,6,2,1]\n",
    "        # print(\"keys reshaped:\", keys.shape, \"\\n\", keys)\n",
    "        assert np.allclose(keys[0][0][0], np.array([-0.5740]), rtol=1e-3, atol=1e-3) # LLMFS 3.5.3\n",
    "        assert np.allclose(keys[0][0][1], np.array([0.2727]), rtol=1e-3, atol=1e-3) # LLMFS 3.5.3\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        queries = tf.reshape(queries, [batch_size, num_tokens, self.num_heads, self.head_dim ]) # 2, 6, 2, 1\n",
    "        assert queries.shape == [2,6,2,1]\n",
    "        # print(\"queries 2:\", queries.shape, \"\\n\", queries)\n",
    "        values = tf.reshape(values, [batch_size, num_tokens, self.num_heads, self.head_dim ]) # 2, 6, 2, 1\n",
    "        # print(\"values 2:\", values.shape, \"\\n\", values)\n",
    "        assert values.shape == [2,6,2,1]\n",
    "\n",
    "\n",
    "        print(\"..........transpose.............\")\n",
    "        # (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = tf.transpose(keys, perm=[0, 2, 1, 3])      # [2,6,2,1] ->  [2, 2, 6, 1]\n",
    "        assert keys.shape == [2, 2, 6, 1]\n",
    "        # print(\"keys:\", keys.shape, \"\\n\", keys) \n",
    "        \n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])      # [2,6,2,1] ->  [2, 2, 6, 1]\n",
    "        assert values.shape == [2, 2, 6, 1]\n",
    "        # print(\"values:\", values.shape, \"\\n\", values) \n",
    "\n",
    "        queries = tf.transpose(queries, perm=[0, 2, 1, 3])      # [2,6,2,1] ->  [2, 2, 6, 1]\n",
    "        assert queries.shape == [2, 2, 6, 1]\n",
    "        # print(\"queries:\", queries.shape, \"\\n\", queries) \n",
    "        \n",
    "        # attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"..........attn_scores.............\")\n",
    "        attn_scores = tf.matmul(queries, tf.transpose(keys, perm=[0, 1, 3, 2]))\n",
    "        assert attn_scores.shape == [2, 2, 6, 6]\n",
    "        # print(\"attn_scores:\", attn_scores.shape, \"\\n\", attn_scores)\n",
    "        assert np.allclose(attn_scores[0][0][0][:3], np.array([ 2.0295e-01,  3.0793e-01,  3.0508e-01]), rtol=1e-3, atol=1e-3) # LLMFS 3.5.3\n",
    "\n",
    "        print(\"..........attn_scores after masking.............\")\n",
    "        print(\"attn_scores.shape:\", attn_scores.shape)\n",
    "        print(\"self.additive_mask_applied.shape:\", self.additive_mask_applied.shape)\n",
    "        trimmed_additive_mask_applied = self.additive_mask_applied[:num_tokens, :num_tokens]\n",
    "        print(\"trimmed_additive_mask_applied.shape:\", trimmed_additive_mask_applied.shape)\n",
    "        attn_scores = attn_scores + trimmed_additive_mask_applied\n",
    "        # print(\"attn_scores:\", attn_scores)\n",
    "        attn_weights = tf.nn.softmax(attn_scores / keys.shape[-1]**0.5, axis=-1)\n",
    "        # print(\"attn_weights:\", attn_weights)\n",
    "\n",
    "        context_vec = tf.matmul(attn_weights, values)        \n",
    "        # print(\"context_vec:\", context_vec)\n",
    "        context_vec = tf.transpose(context_vec, perm=[0, 2, 1, 3])\n",
    "        # print(\"context_vec:\", context_vec)\n",
    "        # print(context_vec.shape)\n",
    "        # print(\"context_vec:\", context_vec)\n",
    "        context_vec = tf.reshape(context_vec, [batch_size, num_tokens, self.d_out]) # (b, num_tokens, self.d_out)\n",
    "        print(context_vec.shape)\n",
    "        assert context_vec.shape == [2, 6, 2]\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        assert context_vec.shape == [2, 6, 2]\n",
    "        assert np.allclose(context_vec[0][0], np.array([0.3190, 0.4858]), rtol=1e-3, atol=1e-3) # LLMFS 3.5.3\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d9b4e3d-a114-459a-994e-183c21f09f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# corresponds to weights in CausalAttention LLMFS listing 3.3\n",
    "qw = np.array([[-0.23542964,  0.21772662],\n",
    " [ 0.01912448, -0.4919342 ],\n",
    " [-0.28674594,  0.42322308]])\n",
    "kw = np.array([[-0.4196414,   0.2614782 ],\n",
    " [-0.45901766, -0.2133264 ],\n",
    " [-0.36482018,  0.21605217]])\n",
    "vw = np.array([[-0.49001414, -0.11346072],\n",
    " [-0.35029206, -0.44043937],\n",
    " [-0.2119892,   0.37804362]])\n",
    "out_proj_weight = np.array([[-0.1668, 0.5000], [0.2270,0.1317] ])\n",
    "out_proj_bias = np.array([0.1934, 0.6825])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6fd103b5-fd1b-4cee-bf35-888162383110",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23542964,  0.21772662],\n",
       "       [ 0.01912448, -0.4919342 ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qw[:2,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07dafdf5-a225-4e31-af59-26fb602d36ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.constant(embeds)\n",
    "batch = tf.stack((inputs, inputs))\n",
    "assert batch.shape == [2,6,3]\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in=d_in, d_out=d_out, context_length=context_length+1, dropout=0, num_heads=2)\n",
    "\n",
    "mha.query_layer.set_weights([qw])\n",
    "mha.key_layer.set_weights([kw])\n",
    "mha.value_layer.set_weights([vw])\n",
    "mha.out_proj.set_weights([out_proj_weight, out_proj_bias])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22bb6dbc-3d2a-4592-b4cd-7c7f5b1c3f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc8cd878-ec72-438a-ae83-7bb47268a37f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....reshape.......................\n",
      "..........transpose.............\n",
      "..........attn_scores.............\n",
      "..........attn_scores after masking.............\n",
      "attn_scores.shape: (2, 2, 6, 6)\n",
      "self.additive_mask_applied.shape: (7, 7)\n",
      "trimmed_additive_mask_applied.shape: (6, 6)\n",
      "(2, 6, 2)\n"
     ]
    }
   ],
   "source": [
    "context_vecs = mha(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82983ff6-c4e1-47a5-9f77-ee95a4db2708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6, 2), dtype=float32, numpy=\n",
       "array([[[0.31908458, 0.48572522],\n",
       "        [0.2944123 , 0.38965017],\n",
       "        [0.28564087, 0.35925537],\n",
       "        [0.2693265 , 0.38730368],\n",
       "        [0.2639324 , 0.39277285],\n",
       "        [0.25753415, 0.40275958]],\n",
       "\n",
       "       [[0.31908458, 0.48572522],\n",
       "        [0.2944123 , 0.38965017],\n",
       "        [0.28564087, 0.35925537],\n",
       "        [0.2693265 , 0.38730368],\n",
       "        [0.2639324 , 0.39277285],\n",
       "        [0.25753415, 0.40275958]]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beac7f7-0f49-418c-be8a-ee6028b0b78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
