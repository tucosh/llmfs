{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f793526-3c5f-4f4a-a010-bf639c158852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda5a7c-f36e-4384-9f61-adcec16958ef",
   "metadata": {},
   "source": [
    "## gpt2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a395063b-f082-483f-bcc9-fa7919d0288a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From https://github.com/ShenakhtPajouh/gpt2-keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_tensor_shape(x):\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    static_shape = x.shape.as_list()\n",
    "    if tf.executing_eagerly():\n",
    "        return static_shape\n",
    "    dynamic_shape = tf.shape(x)\n",
    "    if static_shape is None:\n",
    "        return dynamic_shape\n",
    "    dynamic_shape = tf.unstack(dynamic_shape)\n",
    "    shape = []\n",
    "    for st, dyn in zip(static_shape, dynamic_shape):\n",
    "        if st is None:\n",
    "            shape.append(dyn)\n",
    "        else:\n",
    "            shape.append(st)\n",
    "    return shape\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "def dropout_fn(x, dropout):\n",
    "    if dropout is None or dropout == 0.0:\n",
    "        return x\n",
    "    else:\n",
    "        return tf.nn.dropout(x, rate=dropout)\n",
    "\n",
    "\n",
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, trainable=True, name=None):\n",
    "        super().__init__(name=name, trainable=trainable)\n",
    "        self.beta = None\n",
    "        self.gamma = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.beta = self.add_weight(name=\"beta\", shape=input_shape[-1:], initializer=tf.zeros_initializer())\n",
    "        self.gamma = self.add_weight(name=\"gamma\", shape=input_shape[-1:], initializer=tf.ones_initializer())\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, axis=-1, epsilon=1e-5):\n",
    "        # mean, variance = tf.nn.moments(inputs, axis, keep_dims=True)\n",
    "        mean, variance = tf.nn.moments(inputs, axis, keepdims=True)\n",
    "        rdev = tf.math.rsqrt(variance + epsilon)\n",
    "        x = (inputs - mean) * rdev\n",
    "        output = x * self.gamma + self.beta\n",
    "        return output\n",
    "\n",
    "    def __call__(self, inputs, axis=-1, epsilon=1e-5):\n",
    "        return super().__call__(inputs=inputs,\n",
    "                                axis=axis, epsilon=epsilon)\n",
    "\n",
    "\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_attention_heads=1, size_per_head=512,\n",
    "                 one_sided=True,\n",
    "                 query_act=None,\n",
    "                 initializer_range=0.02,\n",
    "                 value_act=None,\n",
    "                 key_act=None,\n",
    "                 trainable=True,\n",
    "                 name=None):\n",
    "        super().__init__(name=name, trainable=trainable)\n",
    "        # `query_layer` = [B*F, N*H]\n",
    "        self.attention_size = num_attention_heads * size_per_head\n",
    "        self.query_layer = tf.keras.layers.Dense(\n",
    "            num_attention_heads * size_per_head,\n",
    "            activation=query_act,\n",
    "            name=\"query\",\n",
    "            kernel_initializer=tf.random_normal_initializer(stddev=initializer_range)\n",
    "        )\n",
    "        # `key_layer` = [B*T, N*H]\n",
    "        self.key_layer = tf.keras.layers.Dense(\n",
    "            num_attention_heads * size_per_head,\n",
    "            activation=key_act,\n",
    "            name=\"key\",\n",
    "            kernel_initializer=tf.random_normal_initializer(stddev=initializer_range)\n",
    "        )\n",
    "        # `value_layer` = [B*T, N*H]\n",
    "        self.value_layer = tf.keras.layers.Dense(\n",
    "            num_attention_heads * size_per_head,\n",
    "            activation=value_act,\n",
    "            name=\"value\",\n",
    "            kernel_initializer=tf.random_normal_initializer(stddev=initializer_range)\n",
    "        )\n",
    "        self.size_per_head = size_per_head\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.one_sided = one_sided\n",
    "\n",
    "    def reshape(self, x, use_2d=False, shape=None):\n",
    "        if use_2d:\n",
    "            batch_size, seq_length = shape[0], shape[1]\n",
    "        else:\n",
    "            _shape = get_tensor_shape(x)\n",
    "            batch_size, seq_length = _shape[0], _shape[1]\n",
    "        x = tf.reshape(x, [batch_size, seq_length, self.num_attention_heads, self.size_per_head])\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        return x\n",
    "\n",
    "    def final_shape(self, x, use_2d=False):\n",
    "        shape = get_tensor_shape(x)\n",
    "        batch_size, seq_length = shape[0], shape[2]\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        if use_2d:\n",
    "            x = tf.reshape(x, [batch_size * seq_length, self.num_attention_heads * self.size_per_head])\n",
    "        else:\n",
    "            x = tf.reshape(x, [batch_size, seq_length, self.num_attention_heads * self.size_per_head])\n",
    "        return x\n",
    "\n",
    "    def get_mask(self, inputs_shape, cache_length=None, mask=None):\n",
    "        batch_size, seq_length = inputs_shape[0], inputs_shape[2]\n",
    "        if self.one_sided:\n",
    "            rng = tf.range(seq_length)\n",
    "            one_sided_mask = tf.less_equal(rng, tf.expand_dims(rng, 1))\n",
    "            if cache_length is not None:\n",
    "                prev_mask = tf.ones([seq_length, cache_length], tf.bool)\n",
    "                one_sided_mask = tf.concat([prev_mask, one_sided_mask], 1)\n",
    "        if mask is not None:\n",
    "            if cache_length is not None:\n",
    "                prev_mask = tf.ones([batch_size, cache_length], tf.bool)\n",
    "                mask = tf.concat([prev_mask, mask], 1)\n",
    "            if cache_length is None:\n",
    "                cache_length = 0\n",
    "            mask = tf.reshape(mask, [batch_size, 1, 1, seq_length + cache_length])\n",
    "        if self.one_sided:\n",
    "            if mask is not None:\n",
    "                one_sided_mask = tf.logical_and(mask, one_sided_mask)\n",
    "            return one_sided_mask\n",
    "        else:\n",
    "            return mask\n",
    "\n",
    "    def attend(self, query, key, value, mask=None, dropout=None):\n",
    "        dim = tf.cast(self.size_per_head, query.dtype)\n",
    "        _sqrt = tf.math.sqrt(dim)\n",
    "        _sqrt = tf.cast(_sqrt, query.dtype)\n",
    "        coefficients = tf.matmul(query, key, transpose_b=True) / _sqrt\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, coefficients.dtype)\n",
    "            coefficients = coefficients * mask - (1 - mask) * 1e5\n",
    "        coefficients = tf.math.softmax(coefficients, -1)\n",
    "        coefficients = dropout_fn(coefficients, dropout)\n",
    "        results = tf.matmul(coefficients, value)\n",
    "        return results\n",
    "\n",
    "    def call(self, inputs, cache=None, mask=None,\n",
    "             attention_dropout=None, return_cache=False,\n",
    "             use_2d=False, shape=None):\n",
    "        \"\"\"\n",
    "        inputs: a tensor of shape [batch_size, seq_length, dim] if use_2d is false,\n",
    "                else a tensor of shape [batch_size * seq_length, dim]\n",
    "        cache: A dictionary consist of key and value from previous calls.\n",
    "        mask: a boolean tensor of shape [batch_size, seq_length]\n",
    "        attention_probs_dropout_prob: dropout use for attention mechanism\n",
    "        return_cache: if True, it returns key and values as besides layer output\n",
    "        use_2d: if it is True, the model uses 2D matrices as inputs and outputs\n",
    "        shape: if use_2d is True, then the shape is [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        query = self.query_layer(inputs)\n",
    "        key = self.key_layer(inputs)\n",
    "        value = self.value_layer(inputs)\n",
    "        if use_2d and shape is None:\n",
    "            raise ValueError(\"if use_2d is True, then the shape must be specified\")\n",
    "        query = self.reshape(query, use_2d, shape)\n",
    "        key = self.reshape(key, use_2d, shape)\n",
    "        value = self.reshape(value, use_2d, shape)\n",
    "        cache_length = None\n",
    "        if cache is not None:\n",
    "            key = tf.concat([cache[\"key\"], key], 2)\n",
    "            value = tf.concat([cache[\"value\"], value], 2)\n",
    "            cache_length = get_tensor_shape(cache[\"key\"])[2]\n",
    "        inputs_shape = get_tensor_shape(query)\n",
    "        mask = self.get_mask(inputs_shape, cache_length, mask)\n",
    "        result = self.attend(query, key, value, mask, attention_dropout)\n",
    "        result = self.final_shape(result, use_2d)\n",
    "        if return_cache:\n",
    "            cache = {\"key\": key, \"value\": value}\n",
    "            return result, cache\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def __call__(self, inputs, cache=None, mask=None,\n",
    "             attention_dropout=None, return_cache=False,\n",
    "             use_2d=False, shape=None):\n",
    "        \"\"\"\n",
    "        inputs: a tensor of shape [batch_size, seq_length, dim] if use_2d is false,\n",
    "                else a tensor of shape [batch_size * seq_length, dim]\n",
    "        cache: A dictionary consist of key and value from previous calls.\n",
    "        mask: a boolean tensor of shape [batch_size, seq_length]\n",
    "        attention_probs_dropout_prob: dropout use for attention mechanism\n",
    "        return_cache: if True, it returns key and values as besides layer output\n",
    "        use_2d: if it is True, the model uses 2D matrices as inputs and outputs\n",
    "        shape: if use_2d is True, then the shape is [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        return super().__call__(\n",
    "            inputs=inputs,\n",
    "            cache=cache,\n",
    "            mask=mask,\n",
    "            attention_dropout=attention_dropout,\n",
    "            return_cache=return_cache,\n",
    "            use_2d=use_2d,\n",
    "            shape=shape\n",
    "        )\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, config, name=None, trainable=True, initializer_range=0.02):\n",
    "        super().__init__(name=name, trainable=trainable)\n",
    "        self.layer_norm = LayerNormalization(name=\"layer_norm\")\n",
    "        self.self_attention = SelfAttention(num_attention_heads=config[\"n_head\"],\n",
    "                                            size_per_head=config[\"n_embd\"] // config[\"n_head\"],\n",
    "                                            initializer_range=initializer_range,\n",
    "                                            name=\"self\"\n",
    "                                            )\n",
    "        self.projection = tf.keras.layers.Dense(units=config[\"n_embd\"],\n",
    "                                                kernel_initializer=tf.random_normal_initializer(stddev=initializer_range),\n",
    "                                                name=\"projection\")\n",
    "\n",
    "\n",
    "    def call(self, inputs, cache=None, dropout=None, attention_dropout=None,\n",
    "             return_cache=False, use_2d=False, shape=None):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: a tensor of shape [batch_size, seq_length, dim] if use_2d is False, else [batch_size * seq_length, dim]\n",
    "        cache: (Optional): a dictionary of tensors key and value from previous calls.\n",
    "        return_cache: if True, returns a dictionary of key and value tensors besides layer output.\n",
    "        use_2d: if is True then the inputs and outputs are 2D tensors instead of 3D (for tpu performance)\n",
    "        shape: if use_2d then it's [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        x = self.layer_norm(inputs)\n",
    "        x = self.self_attention(x, attention_dropout=attention_dropout,\n",
    "                                cache=cache,\n",
    "                                return_cache=return_cache,\n",
    "                                use_2d=use_2d,\n",
    "                                shape=shape)\n",
    "        if return_cache:\n",
    "            x, cache = x\n",
    "        x = self.projection(x)\n",
    "        x = dropout_fn(x, dropout)\n",
    "        if return_cache:\n",
    "            return x, cache\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def __call__(self, inputs, cache=None, dropout=None, attention_dropout=None,\n",
    "                 return_cache=False, use_2d=False, shape=None):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: a tensor of shape [batch_size, seq_length, dim] if use_2d is False, else [batch_size * seq_length, dim]\n",
    "        cache: (Optional): a dictionary of tensors key and value from previous calls.\n",
    "        return_cache: if True, returns a dictionary of key and value tensors besides layer output.\n",
    "        use_2d: if is True then the inputs and outputs are 2D tensors instead of 3D (for tpu performance)\n",
    "        shape: if use_2d then it's [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        return super().__call__(\n",
    "            inputs=inputs,\n",
    "            cache=cache,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            return_cache=return_cache,\n",
    "            use_2d=use_2d,\n",
    "            shape=shape\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, activation_fn=None, embedding_size=768,\n",
    "                 perceptron_size=3072, trainable=True,\n",
    "                 initializer_range=0.02, name=None):\n",
    "        super().__init__(name=name, trainable=trainable)\n",
    "        self.layer_norm = LayerNormalization(name=\"layer_norm\")\n",
    "        self.perceptron = tf.keras.layers.Dense(units=perceptron_size,\n",
    "                                                activation=activation_fn,\n",
    "                                                kernel_initializer=tf.random_normal_initializer(stddev=initializer_range),\n",
    "                                                name=\"perceptron\")\n",
    "        self.projection = tf.keras.layers.Dense(units=embedding_size,\n",
    "                                                kernel_initializer=tf.random_normal_initializer(stddev=initializer_range),\n",
    "                                                name=\"projection\")\n",
    "\n",
    "    def call(self, inputs, dropout=None):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: tensor of [batch_size, seq_length, dim]\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.layer_norm(inputs)\n",
    "        x = self.perceptron(x)\n",
    "        x = self.projection(x)\n",
    "        x = dropout_fn(x, dropout)\n",
    "        return x\n",
    "\n",
    "    def __call__(self, inputs, dropout=None):\n",
    "        return super().__call__(inputs=inputs,\n",
    "                                dropout=dropout)\n",
    "\n",
    "\n",
    "class Block(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, config, trainable=True, initializer_range=0.02, name=None):\n",
    "        super().__init__(name=name, trainable=trainable)\n",
    "        self.attention = AttentionLayer(config=config,\n",
    "                                        initializer_range=initializer_range,\n",
    "                                        name=\"attention\")\n",
    "        self.mlp = MultiLayerPerceptron(activation_fn=gelu,\n",
    "                                        embedding_size=config[\"n_embd\"],\n",
    "                                        perceptron_size=4 * config[\"n_embd\"],\n",
    "                                        initializer_range=initializer_range,\n",
    "                                        name=\"mlp\")\n",
    "\n",
    "    def call(self, inputs, cache=None, dropout=None, attention_dropout=None,\n",
    "            return_cache=False, use_2d=False, shape=None):\n",
    "        x = inputs\n",
    "        a = self.attention(inputs=x,\n",
    "                           cache=cache,\n",
    "                           dropout=dropout,\n",
    "                           attention_dropout=attention_dropout,\n",
    "                           return_cache=return_cache,\n",
    "                           use_2d=use_2d,\n",
    "                           shape=shape)\n",
    "        if return_cache:\n",
    "            a, cache = a\n",
    "        x = x + a\n",
    "        m = self.mlp(inputs=x,\n",
    "                     dropout=dropout)\n",
    "        x = x + m\n",
    "        if return_cache:\n",
    "            return x, cache\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def __call__(self, inputs, cache=None, dropout=None, attention_dropout=None,\n",
    "                 return_cache=False, use_2d=False, shape=None):\n",
    "        return super().__call__(inputs=inputs,\n",
    "                                cache=cache,\n",
    "                                dropout=dropout,\n",
    "                                attention_dropout=attention_dropout,\n",
    "                                return_cache=return_cache,\n",
    "                                use_2d=use_2d,\n",
    "                                shape=shape)\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, config, trainable=True, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.trainable = trainable\n",
    "        self.blocks = []\n",
    "        self.blocks_num = config[\"n_layer\"]\n",
    "        for ids in range(self.blocks_num):\n",
    "            block = Block(config=config,\n",
    "                          name=\"block_%d\" % ids)\n",
    "            self.blocks.append(block)\n",
    "        self.layer_norm = LayerNormalization(name=\"layer_norm\")\n",
    "\n",
    "    def call(self, inputs, cache=None, dropout=None, attention_dropout=None,\n",
    "             return_cache=False, blocks=None, use_2d=False, shape=None):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: a tensor of shape [batch_size, seq_length, dim], if use_2d is False, else [batch_size * seq_length, dim]\n",
    "        cache: a list of dictionaries. key and values from previous calls.\n",
    "        blocks: a list. if it is specified, the output will be a dictionary {layer_num: layer_output}\n",
    "        return_cache: if it is true, it will returns cache for blocks\n",
    "        use_2d: if it is True, then the operations will define base on 2D tensors. (for tpu performance)\n",
    "        shape: if use_2d is True, then it is [batch_size, seq_length]\n",
    "\n",
    "        \"\"\"\n",
    "        if blocks is None:\n",
    "            max_block = self.blocks_num - 1\n",
    "        elif len(blocks) == 0:\n",
    "            max_block = self.blocks_num - 1\n",
    "            blocks = None\n",
    "        else:\n",
    "            _blocks = []\n",
    "            for i in blocks:\n",
    "                if i >= 0:\n",
    "                    k = i\n",
    "                else:\n",
    "                    k = self.blocks_num - i\n",
    "                if k >= self.blocks_num or k < 0:\n",
    "                    raise ValueError(\"output blocks should be in range [\" + str(0) + \", \" +\n",
    "                                     str(self.blocks_num - 1) + \"]\")\n",
    "                _blocks.append(k)\n",
    "            _blocks = list(sorted(_blocks))\n",
    "            blocks = _blocks\n",
    "            max_block = blocks[-1]\n",
    "        if blocks is not None:\n",
    "            outputs = {}\n",
    "        if return_cache:\n",
    "            new_cache = []\n",
    "        output = inputs\n",
    "        for ids in range(max_block + 1):\n",
    "            if cache is None:\n",
    "                _cache = None\n",
    "            else:\n",
    "                _cache = cache[ids]\n",
    "            output = self.blocks[ids](inputs=output,\n",
    "                                      cache=_cache,\n",
    "                                      dropout=dropout,\n",
    "                                      attention_dropout=attention_dropout,\n",
    "                                      return_cache=return_cache,\n",
    "                                      use_2d=use_2d,\n",
    "                                      shape=shape)\n",
    "            if return_cache:\n",
    "                output, _cache = output\n",
    "                new_cache.append(_cache)\n",
    "            if blocks is not None:\n",
    "                if ids in blocks:\n",
    "                    outputs[ids] = output\n",
    "        if blocks is None:\n",
    "            output = self.layer_norm(output)\n",
    "            result = output\n",
    "        else:\n",
    "            result = outputs\n",
    "        if return_cache:\n",
    "            return result, new_cache\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def __call__(self, inputs, cache=None, dropout=None, attention_dropout=None,\n",
    "                 return_cache=False, blocks=None, use_2d=False, shape=None):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: a tensor of shape [batch_size, seq_length, dim], if use_2d is False, else [batch_size * seq_length, dim]\n",
    "        cache: a list of dictionaries. key and values from previous calls.\n",
    "        blocks: a list. if it is specified, the output will be a dictionary {layer_num: layer_output}\n",
    "        return_cache: if it is true, it will returns cache for blocks\n",
    "        use_2d: if it is True, then the operations will define base on 2D tensors. (for tpu performance)\n",
    "        shape: if use_2d is True, then it is [batch_size, seq_length]\n",
    "\n",
    "        \"\"\"\n",
    "        return super().__call__(\n",
    "            inputs=inputs,\n",
    "            cache=cache,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            return_cache=return_cache,\n",
    "            blocks=blocks,\n",
    "            use_2d=use_2d,\n",
    "            shape=shape\n",
    "        )\n",
    "\n",
    "\n",
    "class Embedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size, max_position_length,\n",
    "                 trainable=True, name=None, initializer_range=0.02,\n",
    "                 dtype=None):\n",
    "        if dtype is None:\n",
    "            dtype = tf.float32\n",
    "        super().__init__(name=name, trainable=trainable, dtype=dtype)\n",
    "        self.word_embedding = None\n",
    "        self.position_embedding = None\n",
    "        self.initializer_range = initializer_range\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_position_length = max_position_length\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.word_embedding = self.add_weight(\n",
    "            name=\"word_embedding\",\n",
    "            shape=(self.vocab_size, self.embedding_size),\n",
    "            initializer=tf.random_normal_initializer(stddev=self.initializer_range),\n",
    "        )\n",
    "        self.position_embedding = self.add_weight(\n",
    "            name=\"position_embedding\",\n",
    "            shape=(self.max_position_length, self.embedding_size),\n",
    "            initializer=tf.random_normal_initializer(stddev=self.initializer_range),\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, start=None):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: integer tensor of [batch_size, seq_length]\n",
    "        start: start of positional embedding\n",
    "\n",
    "        \"\"\"\n",
    "        shape = get_tensor_shape(inputs)\n",
    "        x = tf.gather(self.word_embedding, inputs)\n",
    "        if start is None:\n",
    "            start = 0\n",
    "        end = start + shape[1]\n",
    "        pe = self.position_embedding[start:end]\n",
    "        x = x + pe\n",
    "        return x\n",
    "\n",
    "    def __call__(self, inputs, start=None):\n",
    "        \"\"\"\n",
    "\n",
    "        if use_one_hot_keys is True, then inputs are one_hot tensors of shape [batch_size, seq_length, vocab_size],\n",
    "        else it is an integer tensor of [batch_size, seq_length] of token ids.\n",
    "        start: start of positional embedding\n",
    "\n",
    "        \"\"\"\n",
    "        return super().__call__(inputs=inputs, start=start)\n",
    "\n",
    "\n",
    "class GPT2(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, config, name=None, trainable=True, dtype=None):\n",
    "        super().__init__(name=name)\n",
    "        self.trainable = trainable\n",
    "        self.embedding = Embedding(\n",
    "            embedding_size=config['n_embd'],\n",
    "            vocab_size=config['n_vocab'],\n",
    "            max_position_length=config['n_ctx'],\n",
    "            name=\"embedding\",\n",
    "            dtype=dtype\n",
    "        )\n",
    "        self.transformer = Transformer(config, name=\"transformer\")\n",
    "\n",
    "    def call(self, inputs, cache=None,\n",
    "             dropout=None, attention_dropout=None,\n",
    "             return_cache=False, return_logits=True, use_2d=False):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: an integer tensor of shape [batch_size, seq_length] if not use_2d is False\n",
    "                else a one_hot tensor of shape [batch_size, seq_length, vocab_size]\n",
    "        cache: a list of dictionaries {\"key\": key, \"value\": value} of previous keys and values. it uses for generation\n",
    "        use_one_hot_keys: if True it uses one hot tensors for embedding layer.\n",
    "        return_cache: if True returns new keys and values alongside output. it uses for generation.\n",
    "        return_logits: if True, return logits, else return last layer embedding.\n",
    "        use_2d: for tpu performances: use 2D tensors for operations and return the output in 2D shape: [batch_size * seq_length, -1]\n",
    "\n",
    "        \"\"\"\n",
    "        if cache is not None:\n",
    "            _cache = cache[0][\"key\"]\n",
    "            start = get_tensor_shape(_cache)[2]\n",
    "        else:\n",
    "            start = None\n",
    "        x = self.embedding(inputs, start)\n",
    "        if use_2d:\n",
    "            shape = get_tensor_shape(x)\n",
    "            x = tf.reshape(x, [shape[0] * shape[1], shape[2]])\n",
    "            shape = shape[0:2]\n",
    "        else:\n",
    "            shape = None\n",
    "        x = self.transformer(\n",
    "            inputs=x,\n",
    "            cache=cache,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            return_cache=return_cache,\n",
    "            use_2d=use_2d,\n",
    "            shape=shape\n",
    "        )\n",
    "        if return_cache:\n",
    "            x, cache = x\n",
    "        if return_logits:\n",
    "            shape = get_tensor_shape(x)\n",
    "            if not use_2d:\n",
    "                x = tf.reshape(x, [shape[0] * shape[1], shape[2]])\n",
    "            logits = tf.matmul(x, self.embedding.word_embedding, transpose_b=True)\n",
    "            if not use_2d:\n",
    "                logits = tf.reshape(logits, [shape[0], shape[1], self.embedding.vocab_size])\n",
    "            result = logits\n",
    "        else:\n",
    "            result = x\n",
    "        if return_cache:\n",
    "            return result, cache\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def __call__(self, inputs, cache=None,\n",
    "                 dropout=None, attention_dropout=None,\n",
    "                 return_cache=False, return_logits=True,\n",
    "                 use_2d=False):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: an integer tensor of shape [batch_size, seq_length]\n",
    "        cache: a list of dictionaries {\"key\": key, \"value\": value} of previous keys and values. it uses for generation\n",
    "        use_one_hot_keys: if True it uses one hot tensors for embedding layer.\n",
    "        return_cache: if True returns new keys and values alongside output. it uses for generation.\n",
    "        return_logits: if True, return logits, else return last layer embedding.\n",
    "        use_2d: for tpu performances: use 2D tensors for operations and return the output in 2D shape: [batch_size * seq_length, -1]\n",
    "\n",
    "        \"\"\"\n",
    "        return super().__call__(\n",
    "            inputs=inputs,\n",
    "            cache=cache,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            return_cache=return_cache,\n",
    "            return_logits=return_logits,\n",
    "            use_2d=use_2d\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f72660-ef8e-4002-82aa-a54b2874e5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config124M = {'n_embd': 768, 'n_vocab': 50257, 'n_ctx': 1024, 'n_layer': 12, 'n_head': 12}\n",
    "config = {'n_embd': 3, 'n_vocab': 10, 'n_ctx': 5, 'n_layer': 12, 'n_head': 4}\n",
    "gpt2 = GPT2(name=\"mygpt2\", config=config124M)\n",
    "x=tf.constant([[1]])\n",
    "gpt2.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0cd9eed-3d04-4b10-9482-9f386a54fc67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 50257), dtype=float32, numpy=\n",
       "array([[[-0.37915823, -0.24756916,  0.07592225, ..., -0.78908527,\n",
       "         -1.2582364 ,  0.27532193]]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33fbdc7b-271d-4e09-b108-07c69fffa529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(gpt2)\n",
    "model_dir = 'tf_ckpts'\n",
    "save_path = checkpoint.save(model_dir + \"/ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "781d3649-e8fe-42fe-8fbb-974e88a1ac17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "for name, v in tf.train.list_variables(tf_ckpt_path):\n",
    "    #print(name)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24678145-3d29-40a8-b11e-5a4b8c29c37b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7cb6561-3175-4f95-bf0b-34ecbf8258de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "settings = {\"n_layer\": 12}\n",
    "\n",
    "model_dir=\"ch05/01_main-chapter-code/gpt2/124M\"\n",
    "tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fe81a62-1d5f-4cfd-9c13-ded6b99f97e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8babea10-7191-4e0d-a266-a93cc13db58d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params['wpe'].shape # (1024, 768) position embedding\n",
    "params['wte'].shape # (50257, 768) token embedding, out_head.weight\n",
    "#len(params['blocks']) 12\n",
    "#params['blocks'][0].keys() # dict_keys(['attn', 'ln_1', 'ln_2', 'mlp'])\n",
    "# params['b'].shape # (768,) final_norm.shift (beta)\n",
    "# params['g'].shape # (768,) final_norm.scale (gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c611bc-bdfa-4cb6-a4a2-f602ae14b77d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'mygpt2',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'config': {'n_embd': 768,\n",
       "  'n_vocab': 50257,\n",
       "  'n_ctx': 1024,\n",
       "  'n_layer': 12,\n",
       "  'n_head': 12}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5728a2e8-1d03-4a3b-b829-e36ce6078e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_layer = gpt2.get_layer(index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5428127d-3074-47e5-9eef-0ad11609af8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# embedding_layer.get_weights()[0].shape # word_embedding: (50257, 768) self.vocab_size, self.embedding_size\n",
    "#embedding_layer.get_weights()[1].shape # position_embedding: (1024, 768) max_position_length, self.embedding_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a67f1b9-dee5-4e4d-bde5-40f4a32b2c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word_embedding     = np.zeros((50257, 768))\n",
    "word_embedding     = params['wte']\n",
    "# position_embedding = np.zeros((1024, 768))\n",
    "position_embedding = params['wpe']\n",
    "embedding_layer.set_weights([word_embedding, position_embedding])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "50eccec0-e6e0-473c-ac97-3156fbd3a363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = embedding_layer(tf.constant([[1]]))\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "084fe293-d61b-4a0d-9bc3-10004cb67743",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GPT2 at 0x7f7aa93ae010>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "311a5ca5-a067-4152-8e6c-d8603c8736f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a GPT Model has an Embedding layer and a Transformer Model\n",
    "# transformer_layer = gpt2.get_layer(index=1)\n",
    "embedding_layer   = gpt2.embedding\n",
    "transformer_layer = gpt2.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f99fbf07-14c6-4dab-9996-2feaf1cc81f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The Embedding Layer has word_embedding, position_embedding, initializer_range, embedding_size, vocab_size, max_position_length\n",
    "embedding_layer.word_embedding     = params['wte']\n",
    "embedding_layer.position_embedding = params['wpe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "80ec62ac-9f2a-4547-ba93-efd786cdb7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = embedding_layer(tf.constant([[1]]))\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6bd708e9-c2da-458e-9abe-412f9d02ed79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The Transformer has boolean trainable int blocks_num, a list of (12) Blocks and a LayerNormalization\n",
    "transformer_layer.trainable\n",
    "transformer_layer.blocks_num\n",
    "transformer_layer_blocks     = transformer_layer.blocks\n",
    "transformer_layer_layer_norm = transformer_layer.layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "264ca85a-48cb-4816-a0dd-a9a9649065a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "block0 = transformer_layer_blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d672d02a-ec06-45de-93db-cd97756d9199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Each Block Layer has an AttentionLayer and a MultiLayerPerceptron Layer \n",
    "block0_attn = block0.attention\n",
    "block0_mlp = block0.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "556b39fe-7153-4eb8-9a9a-4a40c8976389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A MultiLayerPerceptron layer has a LayerNormalization layer and 2 Dense Layers\n",
    "block0_mlp_layer_norm = block0_mlp.layer_norm\n",
    "block0_mlp_perceptron = block0_mlp.perceptron\n",
    "block0_mlp_projection = block0_mlp.projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69892cf0-1798-47a0-a66d-150a54a4e17c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Each AttentionLayer has a LayerNormalization layer and a SelfAttentionLayer and a Dense layer\n",
    "block0_attn_layer_norm     = block0_attn.layer_norm\n",
    "block0_attn_self_attention = block0_attn.self_attention\n",
    "block0_attn_projection     = block0_attn.projection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d293d1-ddda-4255-acc5-56c7cf774ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Each LayerNormalization has beta, gamma\n",
    "# block0_attn_layer_norm.beta\n",
    "# block0_attn_layer_norm.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "970451d8-70bd-4f32-a2af-9a028fa53f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each SelfAttentionLayer has..\n",
    "block0_attn_self_attention.attention_size\n",
    "block0_attn_self_attention_query_layer = block0_attn_self_attention.query_layer\n",
    "block0_attn_self_attention_key_layer = block0_attn_self_attention.key_layer\n",
    "block0_attn_self_attention_value_layer = block0_attn_self_attention.value_layer\n",
    "\n",
    "block0_attn_self_attention.size_per_head\n",
    "block0_attn_self_attention.num_attention_heads\n",
    "block0_attn_self_attention.one_sided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1063b-5243-4112-9607-32c9b0474cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
