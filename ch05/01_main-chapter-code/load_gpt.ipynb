{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168cf05e-54fa-4d66-bde1-fd714056cd77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f01e16b-0464-4a57-a47f-e9fc7dc9d9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.7.3\n",
      "numpy version: 1.26.3\n",
      "tiktoken version: 0.11.0\n",
      "torch version: 2.1.1+cu121\n",
      "tensorflow version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505e795-c2b5-42ea-9f3b-53110889a46d",
   "metadata": {},
   "source": [
    "### 5.1.1 Using GPT to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3567851a-451c-4820-820c-06b28e63d7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "# If the `previous_chapters.py` file is not available locally,\n",
    "# you can import it from the `llms-from-scratch` PyPI package.\n",
    "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# E.g.,\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95d824b-cdda-4f3e-a878-b53e64588d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch04 import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0bf60-6741-4b1d-9675-88da51bf170d",
   "metadata": {},
   "source": [
    "### 5.3.3 Modifying the text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dafb71fd-cdd6-4ab9-8c4e-217b29e0b306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for i in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        # print(\"i=\", i, \"idx_cond=\", idx_cond)\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        # print(\"  logits=\", logits.shape)\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "        # print(\"  idx_next=\", idx_next)\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f658e9-e550-49ba-9af6-ede4bdf1706b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you inferred rolleduint fabricationagos remarkably hereuced saints freewaylookOkayRand salary baseless\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538d86a-fb76-4b0c-b67e-061fe1121c95",
   "metadata": {},
   "source": [
    "## 5.5 Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b85fccf-4103-41b8-9fcc-99a9fcef28f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n",
      "tqdm version: 4.66.1\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a82943-e2ef-404b-b547-2275f65d4ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 01:20:10.501383: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-29 01:20:10.501478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-29 01:20:10.503077: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-29 01:20:10.511459: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-29 01:20:11.668150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26d6d7a-20c0-4acd-94d2-9cd72c01aaa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4691c14a-4cb4-4659-8dd1-7413ddfaf231",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a2eecc9-8e4d-41fb-8224-343191d3d3c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78ab43a0-d4cb-46be-8034-67f5899d47b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43abbca0-14e5-40d1-956c-070353e309a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "416ae36f-eff8-446c-aecf-c048f9e2424c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff4a3017-d70b-471e-a383-a908f70fa52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7c80fa-1151-4d98-abff-266c7ad1b59c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92ff7031-eda4-4bef-811a-08f13d15ebd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
      "\n",
      "This would remove you from a battle\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beb6f298-e0c0-45d5-bd2f-6e707ec40636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tok_emb = gpt.tok_emb\n",
    "pos_emb = gpt.pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2d0e8d5-3e81-4e22-b548-fb05cbc35b30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.1520e-02, -2.4603e-01,  5.0275e-02, -8.7654e-02,  1.4641e-01,\n",
       "          -2.8243e-02, -1.8328e-01, -1.7902e-01,  1.0139e-02, -2.1085e-02,\n",
       "           3.7207e-02,  3.6843e-02,  9.7960e-02, -9.4776e-02,  5.5265e-02,\n",
       "          -1.0032e-01,  1.2712e-01, -1.1722e-01,  4.8445e-02,  3.1913e-02,\n",
       "           1.7899e-01, -1.1668e-01,  8.2194e-02, -1.9288e-02, -5.1700e-02,\n",
       "           5.5912e-02, -3.7492e-02, -6.7709e-02,  9.7862e-02,  1.2342e-01,\n",
       "          -1.2243e-01, -1.2199e-01, -6.9925e-02, -5.6776e-02, -1.1048e-01,\n",
       "          -1.0435e-01, -1.0710e+00,  1.5164e-01,  8.7364e-03,  1.4190e-02,\n",
       "           1.1161e-01,  7.3609e-03,  9.0015e-02, -9.7361e-02,  8.4453e-02,\n",
       "           1.1034e-01,  8.9273e-02, -1.4693e-01,  2.1908e-02,  2.7703e-01,\n",
       "           9.7465e-02,  5.7503e-02,  7.3013e-01,  1.1243e-01,  4.3085e-02,\n",
       "           3.9380e+00, -7.5768e-03, -1.3168e-01,  2.3872e-02, -1.5426e-01,\n",
       "          -3.2426e-02,  7.7189e-02, -1.9591e-01, -4.2413e-02, -2.1815e-01,\n",
       "           7.1094e-02,  6.7588e-02, -6.1569e-01,  9.4331e-02,  2.5988e-02,\n",
       "          -8.4457e-02,  3.9358e-02,  9.9586e-02,  1.0670e-01,  1.1509e-01,\n",
       "           9.8717e-02, -6.5775e-02,  8.3857e-02,  4.0498e-02, -1.2527e-01,\n",
       "          -3.4667e-02, -3.2739e-02, -1.2078e-01, -2.4394e-02, -3.3013e-03,\n",
       "          -1.0312e-01, -1.1204e+00, -2.8259e-01,  4.9613e-02, -1.0952e-01,\n",
       "           9.6671e-02, -1.5465e-01, -1.7759e-01, -8.8136e-02,  6.8201e-02,\n",
       "           6.5695e-02, -5.0184e-02,  3.8242e-02,  3.9946e-02,  1.0463e-01,\n",
       "           1.1129e-01, -1.0148e-02, -2.0070e-01, -2.4118e-01,  5.9838e-02,\n",
       "           1.3520e-02,  6.4089e-02,  2.8901e-01, -1.4720e-01, -1.0363e-01,\n",
       "          -8.3911e-02, -7.9449e-02, -1.1099e-01,  1.5712e-02, -8.7946e-02,\n",
       "           1.1674e-02, -5.3009e-01,  1.8729e-02, -9.9629e-02,  2.3693e-01,\n",
       "           3.3986e-02, -3.1472e-02, -6.5480e-02, -1.0667e-01,  2.2075e-01,\n",
       "           8.7799e-02, -2.6985e-02, -4.4410e-02, -4.0906e-02,  1.2979e-01,\n",
       "           2.7036e-02, -4.8814e-02,  6.5816e-02,  1.5100e-01, -5.2035e-03,\n",
       "          -4.0783e-02, -1.6730e-02, -1.8372e-01,  5.3093e-01, -6.6305e-01,\n",
       "          -1.1174e-01,  1.6870e-02,  1.2118e-01, -1.3403e-01, -5.1616e-03,\n",
       "          -2.4688e-01,  1.8623e-02, -1.2151e-01,  4.7673e-02,  4.5839e-02,\n",
       "          -1.5287e-01, -5.2010e-02, -1.3779e-01, -6.6048e-02,  4.1179e-02,\n",
       "           2.9814e-02,  3.3740e-02, -8.6583e-03, -4.0995e-03,  1.9883e-02,\n",
       "          -2.0593e-01,  9.0361e-02, -1.0438e-01, -6.2707e-02,  1.6281e-01,\n",
       "           1.5483e-01, -3.1980e-02,  2.4527e-01, -1.2167e-01,  1.7468e-01,\n",
       "          -1.2738e-02, -8.7777e-02, -4.6207e-02, -3.3036e-03,  2.9179e-01,\n",
       "          -1.0280e-01, -6.8339e-01, -1.4685e-01,  7.5447e-02,  1.5735e-01,\n",
       "          -9.2017e-02,  4.1295e-02,  1.0951e-01,  7.0703e-02,  1.2756e-01,\n",
       "          -5.0965e-02,  1.1128e-01, -1.8006e-02,  6.9714e-02, -1.3179e-01,\n",
       "           8.4935e-02,  4.0499e-02, -2.0209e-01,  1.9172e-01, -3.6676e-03,\n",
       "           7.8970e-02,  1.6729e-01, -4.9629e-03, -6.2612e-02,  1.6599e-01,\n",
       "          -1.2913e-01, -1.1019e-01, -1.2085e-02, -1.2539e-01,  1.4491e-01,\n",
       "           3.2811e-02,  1.3606e-01, -9.2051e-02,  5.6634e-02,  1.3434e-01,\n",
       "           2.0176e-02,  1.0102e-01,  5.9425e-02, -9.6159e-02,  7.7501e-03,\n",
       "           8.4317e-02, -2.7486e-02, -1.2915e-01, -3.9365e-02,  4.2549e-02,\n",
       "           4.6717e-03, -1.4714e-02, -7.1219e-02, -9.2707e-03, -6.7242e-02,\n",
       "          -1.0393e-01,  1.0110e-01, -3.8065e-02,  5.9325e-03, -1.4639e-01,\n",
       "          -2.0748e-04, -7.9379e-02, -1.8679e-01, -1.5995e-01,  4.9332e-02,\n",
       "           1.6166e-02, -2.2477e-02, -1.3317e-01, -1.3501e-02,  1.2269e-01,\n",
       "          -7.8366e-03,  1.0812e-01,  9.7350e-02, -9.2229e-02, -7.9378e-02,\n",
       "           4.0728e-02,  1.1546e-03, -7.6189e-02,  1.4978e-01,  1.0249e-01,\n",
       "           1.0757e-01,  1.3310e-01, -8.1293e-02,  1.0452e-01,  2.5188e-02,\n",
       "           1.1924e-02,  2.1340e-01,  1.3443e-01, -1.6333e-02,  8.4733e-03,\n",
       "           1.5091e-02, -3.7803e-02,  9.5561e-02,  2.8058e-02, -1.4621e-01,\n",
       "          -5.0996e-02, -2.8036e-01, -2.0491e-01, -8.3732e-02, -1.0220e-01,\n",
       "           2.3429e+00,  6.5232e-01, -3.2181e-02,  7.5030e-04, -1.2072e-01,\n",
       "          -3.2495e-02, -4.2800e-01,  1.6383e-02, -3.8921e-02,  1.3289e-02,\n",
       "           1.3109e-01, -4.3211e-02, -1.3379e-01,  2.3828e-01,  1.0792e-01,\n",
       "          -1.3591e-01, -1.1393e-01,  9.2412e-02, -6.4750e-01,  7.3080e-01,\n",
       "           2.7628e-02, -7.7983e-03, -6.7410e-02,  1.2062e-01,  2.0565e-01,\n",
       "          -1.6272e-01, -9.8374e-02,  1.0053e-01, -8.9034e-03, -1.9015e-01,\n",
       "           3.0850e+00,  8.8202e-03, -5.6855e-02, -9.1163e-02, -8.0486e-02,\n",
       "          -2.1492e-02,  1.4108e-02,  5.3891e-02, -1.2534e-01, -9.2600e-03,\n",
       "          -4.0689e-02,  3.1525e-01,  8.7139e-02, -2.2731e-02,  1.4071e-01,\n",
       "          -1.2226e-03,  2.3899e-02, -9.9960e-02, -4.8627e-02,  9.7601e-02,\n",
       "           3.1615e-01, -3.5107e-02, -7.0882e-02,  5.1894e-02,  3.0161e-02,\n",
       "           3.2296e-02, -2.8320e-01, -9.0528e-02, -6.7189e-02, -1.0274e-01,\n",
       "           2.5036e-02,  1.4668e-01,  1.6189e-01, -1.1943e-02, -1.1138e-02,\n",
       "          -3.9771e-03,  2.0587e-01, -1.1351e-01,  6.6936e-02, -1.3208e-02,\n",
       "          -6.7186e-02, -8.0037e-02,  1.7789e-01, -1.3625e-02,  8.0569e-02,\n",
       "           3.4686e-02,  4.8361e-02,  4.9601e-02,  2.5915e-02, -3.1130e-02,\n",
       "          -3.8234e-02, -3.9282e-01,  1.2193e-01, -6.6085e-02, -1.4099e-01,\n",
       "           5.6584e-02, -2.1871e+00,  2.2264e-02, -1.4284e-01,  1.2598e-01,\n",
       "           8.5983e-02, -8.5570e-01,  6.5140e-03, -1.1988e-01,  8.9854e-02,\n",
       "          -4.0119e-01,  4.7601e-02, -2.2801e-01,  2.1202e-01, -5.4212e-02,\n",
       "           1.3485e-01, -1.2124e-02, -3.8127e-02, -2.0925e-01, -3.2300e-01,\n",
       "          -1.5694e-02,  5.3156e-02, -1.0586e+00, -2.1093e-01,  4.2157e-02,\n",
       "          -3.5318e-02,  1.1429e-02,  1.4085e-01,  4.0626e-02,  3.9068e-02,\n",
       "           1.6725e-01, -1.5635e-02, -1.9764e-02, -9.2752e-02,  2.4929e-01,\n",
       "           5.4861e-02,  2.2525e-02,  5.3360e-02, -1.2762e-01, -5.7952e-02,\n",
       "           1.2171e-01,  2.5251e-02, -1.8662e-02,  2.2455e-02, -1.0805e-01,\n",
       "           1.4136e-01,  4.0931e-02, -5.5937e-01,  1.1862e-01,  4.1552e-02,\n",
       "          -6.8566e-03,  8.0712e-02,  7.4439e-02, -3.0647e-01, -1.6695e-03,\n",
       "          -1.5795e-03, -5.4240e-02, -3.6871e-02, -1.1819e-01, -6.2600e-02,\n",
       "           1.6395e-03,  1.9733e-02,  3.3972e-02,  6.7450e-02,  1.3216e-01,\n",
       "          -2.0758e-01, -4.5322e-02,  8.4900e-02,  5.8517e-03, -2.4321e-02,\n",
       "           1.4058e-01, -1.5736e-02,  6.5733e-06,  1.8778e-01, -7.0933e-02,\n",
       "          -3.4758e-01,  1.1355e-01,  3.0997e-01,  1.4076e-01,  8.7950e-02,\n",
       "           3.8578e-02,  1.0629e-02, -1.7365e-02,  6.7710e-02,  3.2539e-01,\n",
       "          -1.8950e-02,  1.2406e-02,  2.5829e-03,  3.4820e-02,  4.6281e-02,\n",
       "           9.9250e-02, -2.6477e-02,  2.2814e-01, -1.4189e-01,  8.4506e-03,\n",
       "           1.0000e-02, -1.0800e-01,  1.7997e-01,  4.8156e-01,  1.6707e-02,\n",
       "          -5.1928e-01,  1.3091e-01,  1.6148e-01,  1.0686e-01,  3.8851e-01,\n",
       "           2.5400e-02,  6.9876e-02,  1.1690e-01,  9.1131e-02, -2.9689e-02,\n",
       "          -7.5889e-02,  4.8689e-02,  4.5462e-02, -1.5441e-01, -5.2682e-02,\n",
       "          -7.5907e-02,  6.3956e-02,  2.0480e-01, -1.3487e-01, -2.0875e-02,\n",
       "          -7.3952e-02, -1.7092e-01,  1.3579e-01,  9.9182e-02, -6.8853e-01,\n",
       "          -3.2805e-01,  1.9224e-01,  4.6790e-02,  2.9497e-03,  2.7960e-02,\n",
       "           2.6409e-02, -3.7678e-02, -4.5223e-02,  1.3328e-02,  1.1469e-01,\n",
       "           2.8014e-03, -2.4159e-02, -4.3366e-02, -5.8127e-02,  3.3380e-03,\n",
       "           6.1464e-02, -3.9982e-01,  2.9374e-01, -5.7730e-02, -4.3928e-02,\n",
       "           1.1378e-03,  1.9943e-02,  9.0044e-02, -1.3487e+00, -4.5972e-02,\n",
       "          -6.8878e-02,  1.5338e-02,  5.6648e-02, -2.1146e-01,  1.0048e-01,\n",
       "           1.7515e-01,  5.1592e-01, -5.7865e-02,  9.5120e-03, -4.6080e-02,\n",
       "          -5.5924e-02,  9.3634e-02, -1.5386e-01, -1.4419e-02, -1.6214e-02,\n",
       "           1.1401e-01,  6.3709e-02,  1.2859e-01,  6.9160e-02, -1.6684e-02,\n",
       "          -2.8952e-01,  9.4785e-02, -8.9504e-02, -9.9456e-01, -6.4223e-02,\n",
       "          -1.4874e-01,  9.5133e-02, -3.9829e-02, -2.1381e-02, -1.2411e-01,\n",
       "           1.3077e-01,  2.0625e-01, -1.5957e-02, -1.3449e-01,  8.0388e-02,\n",
       "           1.5792e+00,  7.0674e-02,  2.3329e-02,  2.1071e-01, -5.7987e-02,\n",
       "           2.3843e-02,  4.2796e-02, -9.3418e-02, -4.4914e-02,  1.5044e-01,\n",
       "           3.4610e-02, -9.9804e-01, -4.0944e-02, -4.8871e-02,  4.3929e-01,\n",
       "          -1.4908e-01,  2.9878e-02,  6.8495e-02,  5.8718e-02, -1.4436e-01,\n",
       "          -1.0350e-01,  1.6190e-02, -3.7940e-02,  8.9373e-02,  1.1006e-01,\n",
       "           2.3921e-03, -3.9670e-01, -1.0900e-01, -4.9084e-02, -6.4455e-02,\n",
       "          -1.0964e-01,  6.6350e-02,  1.3704e-01, -2.1610e-03, -3.8166e-02,\n",
       "           5.3294e-02,  3.8895e-02,  4.0777e-02,  1.7519e-01,  6.8948e-02,\n",
       "          -6.2637e-02,  1.2581e+00, -5.8866e-02,  6.8399e-02, -6.6136e-02,\n",
       "          -1.2793e-01,  8.4759e-02,  3.9318e-02, -7.7077e-02,  1.2597e-01,\n",
       "          -9.8854e-02,  2.0387e-02,  4.0159e-02, -8.1136e-02, -1.7209e-01,\n",
       "          -6.8234e-03, -4.8119e-03, -1.1727e-01, -1.6638e-02,  3.5289e-02,\n",
       "           7.4052e-02, -2.0335e-02, -1.2485e-01, -3.7909e-02,  5.9900e-02,\n",
       "          -3.9487e-02,  8.3755e-02,  1.5148e-01, -3.1898e-01,  6.2456e-02,\n",
       "          -4.7807e-02, -3.7703e-02, -9.9825e-02,  6.8616e-02, -4.3307e-02,\n",
       "           1.6762e-01, -1.3302e-01,  7.7896e-02,  3.0673e-02,  9.7204e-02,\n",
       "          -1.2934e-01,  1.1852e-01,  6.1845e-01, -5.1463e-01,  2.3005e-01,\n",
       "           3.0645e-02, -5.5431e-03, -1.4335e+00,  8.8729e-02, -7.3442e-02,\n",
       "          -5.7116e-02, -9.7379e-02,  8.8548e-02, -3.9450e-02,  7.5034e-02,\n",
       "           1.2089e-01, -7.4261e-02,  3.2551e-05, -3.0118e-02,  1.1338e-01,\n",
       "           1.6555e-01, -1.9052e-02,  1.4445e-02,  3.1401e-01,  8.2933e-02,\n",
       "          -3.0153e-02, -2.6548e-02, -4.5970e-02,  5.6436e-02,  9.8980e-02,\n",
       "           6.8175e-02,  1.8527e-02,  7.3811e-02,  1.5216e-01, -4.0432e-02,\n",
       "          -7.1583e-01, -9.1699e-02, -1.8330e-02,  8.5090e-02, -2.3265e-01,\n",
       "           8.4102e-03,  1.4248e-01,  4.5648e-02,  8.0959e-02,  2.1947e-04,\n",
       "          -2.6604e-01, -1.2008e-01,  5.7639e-02, -2.0048e-01,  1.5771e-01,\n",
       "           3.0874e+00, -5.8884e-02, -6.6137e-02,  1.5943e-01, -4.5114e+00,\n",
       "           8.0211e-02,  2.4884e-02,  4.4406e-02, -1.1766e-01, -1.5361e+00,\n",
       "           1.2426e-01,  1.2083e-02, -1.2606e-01,  2.5280e-02,  1.4014e-01,\n",
       "          -9.1700e-02,  9.6547e-02, -3.1436e-03, -1.2889e-02,  5.5599e-02,\n",
       "           2.9231e-02,  6.3759e-02, -5.8475e-02,  8.1023e-02, -1.8552e-02,\n",
       "          -4.4211e-02,  7.0020e-02, -2.1361e-02, -2.2941e-02, -2.0339e-01,\n",
       "          -1.6706e-01, -3.9050e-01, -6.5236e-03,  1.6952e-01, -2.4976e-02,\n",
       "          -1.2066e-01,  2.2457e-01, -1.0069e-01, -5.6016e-02,  1.1107e+00,\n",
       "           3.1523e-02,  8.4614e-02, -2.8961e-02, -7.3623e-03,  1.2220e-01,\n",
       "           4.9152e-03, -5.4139e-02,  6.5395e-02, -8.7506e-01,  1.1746e-01,\n",
       "           8.6210e-01,  1.6566e-01,  1.4122e-02, -8.4103e-02,  1.4205e+00,\n",
       "           1.2281e-01,  1.2549e-01,  1.3133e-01,  4.8825e-01, -6.6338e-02,\n",
       "           1.3581e-01, -3.1892e-02,  1.2652e-01, -6.0041e-02,  8.6927e-02,\n",
       "          -8.4269e-02, -5.2598e-02,  3.3285e-02, -7.9796e-02, -1.0272e-01,\n",
       "           5.4846e-02,  1.0248e-01,  6.1835e-02,  7.8229e-02, -1.8515e-02,\n",
       "           4.5065e-01, -1.6870e-01, -2.9620e-01, -5.6125e-02, -1.8793e-02,\n",
       "           8.0743e-02,  8.4028e-02,  1.1331e-01, -4.1458e-02,  2.7252e-02,\n",
       "           4.9181e-02, -1.6958e+00, -1.5829e-01, -3.1558e-03, -8.2138e-03,\n",
       "          -5.9221e-02, -7.6174e-03, -1.2472e-01, -3.5075e-02,  1.4848e-02,\n",
       "           4.3011e-02,  3.0807e-02,  9.7680e-02]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "gpt.to(\"cpu\");\n",
    "x = torch.tensor([[1]])\n",
    "batch_size, seq_len = x.shape\n",
    "tok_embeds = tok_emb(x)\n",
    "pos_embeds = pos_emb(torch.arange(seq_len, device=x.device))\n",
    "result = tok_embeds + pos_embeds\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a93da6-f5f3-4aa5-aa50-e05a0e7e7036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0353e792-9856-4753-9345-8776a6b345b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dir(gpt.trf_blocks)\n",
    "# gpt.trf_blocks\n",
    "tb0_norm1 = gpt.trf_blocks.get_submodule(\"0.norm1\")\n",
    "# dir(tb0_norm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "815485b1-c0ca-4798-a6b9-312e3fe9233b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.8459e-03, -9.2904e-02, -4.2862e-02, -4.9714e-02,  6.5172e-02,\n",
       "          -2.5777e-02,  1.2989e-01, -5.3672e-02,  4.4092e-03, -1.8762e-02,\n",
       "           1.6200e-02,  1.9597e-02,  5.0713e-02, -5.9614e-02, -1.7790e-03,\n",
       "          -5.3112e-02,  6.1116e-02, -6.0908e-02,  1.2926e-02, -4.8621e-02,\n",
       "           9.4318e-02, -6.5233e-02,  4.2126e-02, -2.7062e-02, -4.5828e-02,\n",
       "           3.2384e-02, -3.5752e-02, -4.4581e-02,  4.3532e-02,  6.0104e-02,\n",
       "          -6.5584e-02, -4.4283e-02, -5.1269e-02, -3.0915e-02, -5.9717e-02,\n",
       "          -9.9794e-02, -1.5640e-01,  7.4537e-02, -4.0764e-03,  2.2759e-02,\n",
       "           5.3453e-02,  1.8216e-03,  4.0062e-02, -4.0538e-02,  3.2244e-02,\n",
       "           6.0359e-02,  4.2159e-02, -7.6805e-02,  8.9943e-03,  1.8858e-02,\n",
       "           4.3665e-02,  3.4360e-02,  8.4450e-02,  4.9020e-02,  2.4041e-02,\n",
       "           5.1071e-01, -1.3003e-02, -9.0332e-02,  9.0321e-03, -1.0818e-01,\n",
       "          -1.5694e-02,  3.6970e-02, -8.5053e-02, -6.9542e-02, -6.2262e-02,\n",
       "           3.1176e-02,  3.5746e-02, -1.7817e-01, -4.0666e-02,  1.7199e-02,\n",
       "          -5.2130e-02, -7.6313e-03,  4.1399e-02,  3.4135e-02,  5.0699e-02,\n",
       "           4.1008e-02, -4.4765e-02, -1.2165e-01,  1.8408e-02, -7.8910e-02,\n",
       "          -2.4064e-02, -1.6344e-02, -6.4793e-02, -2.6750e-02, -4.4058e-02,\n",
       "          -2.1750e-02, -1.4235e-01, -9.2168e-02,  1.2163e-02, -6.3422e-02,\n",
       "           4.8248e-02, -8.4245e-02, -1.4776e-01, -5.2263e-02,  2.8384e-02,\n",
       "           3.7027e-02, -2.4245e-02,  2.3713e-02,  5.5775e-03,  5.9728e-02,\n",
       "           4.5563e-02, -5.9688e-04,  4.6635e-02, -7.0858e-02,  3.9407e-02,\n",
       "           9.5266e-03,  3.1172e-02,  2.1270e-01, -7.9301e-02, -5.8602e-02,\n",
       "          -4.8408e-02, -4.9642e-02, -6.3060e-02,  2.2023e-05, -6.3465e-02,\n",
       "           8.6568e-03, -1.6058e-01,  8.6289e-03, -5.4472e-02,  4.9446e-02,\n",
       "           5.1830e-03, -2.9553e-03, -3.3826e-02, -1.8577e-02,  1.0407e-01,\n",
       "           3.1700e-02, -1.3809e-02, -3.5986e-02, -8.4885e-02,  6.9333e-02,\n",
       "           1.2383e-02, -3.0707e-02,  3.3378e-02,  8.2402e-02,  3.5802e-03,\n",
       "          -2.5878e-02, -1.6652e-02, -1.3455e-01,  1.2280e-01, -1.0187e-01,\n",
       "          -6.4995e-02,  2.0551e-02, -1.2933e-01, -8.1138e-02, -7.0557e-03,\n",
       "           5.8227e-02,  8.8971e-03, -6.9523e-02,  1.8945e-02,  1.2080e-02,\n",
       "          -8.4484e-02,  1.8538e-03, -5.1494e-02,  2.9490e-02,  2.8524e-02,\n",
       "           1.3696e-02,  2.4545e-02, -1.1333e-02, -9.6866e-03,  5.3631e-02,\n",
       "           2.8980e-02,  4.5874e-02, -6.9134e-02, -4.6595e-02,  8.5627e-02,\n",
       "           8.2454e-02, -2.0823e-02, -1.5223e-02, -5.5563e-02, -1.9908e-02,\n",
       "          -3.5546e-02, -5.8806e-02, -1.8487e-02, -6.4925e-03,  9.9195e-02,\n",
       "          -7.5481e-02, -2.0945e-01, -8.0428e-02,  3.9864e-02,  3.0362e-02,\n",
       "          -4.3019e-02,  1.6708e-02,  5.1758e-02,  3.7256e-02,  6.2160e-02,\n",
       "          -2.4893e-02,  4.2181e-02, -1.8192e-02,  2.8641e-02, -1.2254e-02,\n",
       "           4.4226e-02, -8.6046e-02, -1.0876e-01,  6.9087e-02, -1.3225e-02,\n",
       "          -4.6279e-02,  7.9790e-02, -5.4379e-02, -3.3832e-02, -3.9004e-02,\n",
       "          -5.3918e-02, -7.1344e-02, -7.4224e-03, -7.3132e-02,  7.6526e-02,\n",
       "          -1.0798e-03,  6.1680e-02, -5.9289e-02,  3.0982e-02,  6.5002e-02,\n",
       "           6.4062e-03,  6.6337e-02,  2.5292e-02, -8.3902e-03,  6.1852e-03,\n",
       "           3.6003e-02, -3.0079e-02, -6.9010e-02, -4.1765e-02,  2.3571e-02,\n",
       "          -8.9441e-04, -2.4016e-02, -5.4772e-02,  4.1421e-03,  3.2120e-02,\n",
       "          -5.5910e-02,  4.4078e-02, -1.8275e-02, -3.5238e-03, -7.3308e-02,\n",
       "          -2.1282e-03, -5.3250e-02,  9.6258e-02, -8.0586e-02,  2.5494e-02,\n",
       "           1.0894e-02, -1.2629e-02, -6.9840e-02,  1.2825e-03,  6.1599e-02,\n",
       "          -8.7622e-03,  1.0042e-01,  5.0230e-02, -4.8126e-02, -4.3579e-02,\n",
       "           1.7400e-02,  1.0948e-03, -4.1059e-02,  3.7577e-02,  4.1821e-02,\n",
       "          -7.9000e-02,  6.2106e-02, -5.1423e-02,  5.9231e-02,  3.8448e-03,\n",
       "           7.7262e-02,  1.3615e-02,  6.8125e-02, -2.6121e-02,  6.8319e-04,\n",
       "          -3.1378e-03,  4.4661e-02,  4.0944e-02,  1.2593e-02, -8.2146e-02,\n",
       "          -3.6069e-02, -8.8903e-02, -4.2464e-02, -6.9059e-02, -3.7569e-02,\n",
       "           1.2386e-02,  1.5765e-01, -2.6339e-02, -5.8306e-03, -6.3635e-02,\n",
       "          -1.6411e-02, -9.1985e-02,  9.0671e-04, -3.2924e-02, -1.2690e-03,\n",
       "           6.5374e-02, -2.9209e-02,  1.2609e-03,  9.4330e-02,  8.4934e-02,\n",
       "          -7.4449e-02, -6.5730e-02,  4.7046e-02, -9.1798e-02,  1.0231e-01,\n",
       "           2.1771e-02, -7.6057e-03, -4.0657e-02,  5.1152e-02,  8.0437e-02,\n",
       "          -8.4246e-02, -6.0401e-02,  4.7628e-02, -1.6819e-02, -1.0415e-01,\n",
       "           3.6653e-01,  5.1234e-03, -4.3202e-02, -5.3989e-02, -4.7849e-02,\n",
       "          -2.4320e-02,  6.7629e-04,  2.5185e-02, -1.0445e-02, -9.4372e-03,\n",
       "          -3.2425e-02,  6.9362e-03,  4.5720e-02, -1.4810e-02, -3.6040e-02,\n",
       "           3.1983e-03,  4.8290e-02, -5.4349e-02, -3.9403e-02,  4.4675e-02,\n",
       "          -9.1215e-02, -2.0331e-02, -4.4845e-02, -1.8388e-02,  9.1363e-03,\n",
       "           6.6567e-03,  1.5160e-02, -4.4611e-02, -4.2221e-02, -5.0579e-02,\n",
       "           2.2763e-02,  7.6008e-02,  7.0072e-02,  2.6561e-04, -1.0264e-02,\n",
       "          -6.3422e-06,  1.0647e-01, -7.0958e-02,  1.8970e-02, -5.2828e-04,\n",
       "          -4.4072e-02, -4.9602e-02,  7.5154e-02, -8.0386e-03,  6.2285e-02,\n",
       "           2.4765e-02,  3.0032e-02,  1.0913e-02,  3.8706e-03, -2.1460e-02,\n",
       "          -1.7229e-02, -3.5965e-02,  5.4299e-02, -5.6366e-02, -5.2858e-02,\n",
       "           2.7543e-02, -2.9837e-01,  1.6205e-02, -7.7747e-02,  5.3834e-02,\n",
       "           3.9980e-02, -1.6673e-01,  9.6593e-02, -6.1471e-02,  5.3828e-02,\n",
       "          -1.0766e-01,  2.7192e-02, -1.0599e-01,  8.4405e-02, -2.8959e-02,\n",
       "           6.5655e-02, -1.2940e-02, -2.7950e-02, -7.5704e-02, -1.4943e-01,\n",
       "          -4.5405e-02,  2.9406e-03, -1.3833e-01, -1.0648e-01,  2.4043e-02,\n",
       "          -2.3023e-02, -1.7269e-04,  6.5663e-02,  2.8031e-02,  6.0589e-03,\n",
       "           1.1175e-01, -1.0004e-02, -2.8630e-02, -4.4916e-02, -1.9965e-02,\n",
       "           3.2177e-02,  7.1307e-03,  2.1315e-02, -2.6732e-02, -7.6715e-02,\n",
       "           4.3385e-02, -1.3551e-02, -1.6192e-02,  7.1019e-03, -5.3702e-02,\n",
       "           5.9347e-02,  2.0946e-02, -8.8138e-02,  7.0211e-02,  1.6293e-02,\n",
       "          -9.9938e-03,  3.3974e-02,  2.4544e-02, -4.0067e-02, -3.9000e-03,\n",
       "          -6.3832e-05, -3.6624e-02, -2.7536e-02, -5.9013e-02, -3.7778e-02,\n",
       "           3.2319e-04,  7.0881e-03,  1.4015e-02,  4.1318e-02,  7.4882e-02,\n",
       "          -1.1512e-01, -2.9001e-02,  3.3863e-02, -3.8664e-02, -2.0176e-02,\n",
       "           6.3927e-02, -6.5555e-03, -1.6585e-02,  5.6738e-02, -4.6149e-02,\n",
       "          -4.5586e-03,  5.4788e-02,  1.4406e-01,  5.8396e-02,  5.0776e-02,\n",
       "           7.9774e-02,  4.4472e-03, -1.0358e-02,  2.3838e-02,  5.4149e-02,\n",
       "           5.7550e-02,  3.3864e-03,  1.1787e-01,  2.3287e-02,  3.0613e-02,\n",
       "           5.1128e-02, -2.3132e-02,  7.1498e-02, -6.8414e-02,  3.4848e-02,\n",
       "           1.0953e-02, -6.5926e-02,  6.9097e-02,  2.4089e-02,  4.4323e-02,\n",
       "          -1.1564e-01, -1.8512e-02,  8.5614e-02,  5.0755e-02,  1.3815e-02,\n",
       "           7.3409e-03,  4.3119e-02,  6.1702e-02,  4.8582e-02, -2.7253e-02,\n",
       "          -5.3384e-02,  2.2356e-02,  1.7256e-02, -8.4722e-02,  2.0976e-04,\n",
       "          -5.9305e-02,  3.4894e-02,  1.1102e-01, -7.6277e-02, -1.6876e-02,\n",
       "          -4.5413e-02, -6.5929e-02,  7.5180e-02,  1.4479e-02, -1.8556e-01,\n",
       "          -1.2388e-01,  3.3239e-02,  2.0200e-02, -7.7261e-04,  1.1103e-02,\n",
       "           1.0168e-02, -2.1307e-02, -1.8038e-02,  6.7114e-02,  5.3797e-02,\n",
       "          -8.2440e-03, -4.7116e-02, -2.6930e-02, -3.4102e-02, -1.5757e-03,\n",
       "           3.1446e-02, -5.8546e-02, -1.5622e-02, -3.0695e-02, -2.3950e-02,\n",
       "          -1.1526e-02,  2.4357e-03,  5.9625e-02, -2.3714e-01, -2.6182e-02,\n",
       "          -4.5912e-02,  7.1925e-03,  4.3293e-02, -9.5066e-02,  5.2243e-02,\n",
       "           5.9128e-02,  9.9974e-02,  4.0014e-02,  8.4916e-03, -3.3553e-02,\n",
       "          -5.3483e-02,  4.7687e-02, -1.3384e-01, -1.1220e-02, -1.7041e-02,\n",
       "           4.9539e-02,  3.0668e-02,  6.6187e-02,  4.1297e-02, -8.2077e-03,\n",
       "          -7.8034e-02, -6.9020e-02, -7.9595e-02, -7.7830e-02, -4.0476e-02,\n",
       "          -2.5653e-03,  4.2479e-02, -2.3459e-02, -1.7936e-02, -7.6796e-02,\n",
       "           6.7348e-02,  9.2668e-02, -1.1094e-02, -8.1567e-02,  3.9463e-02,\n",
       "           1.4888e-01,  4.2408e-02,  1.6182e-03, -6.1144e-02, -4.8629e-02,\n",
       "           6.8248e-03,  3.1365e-02, -3.6410e-02, -7.0814e-02, -1.0058e-03,\n",
       "           1.5583e-02, -1.1585e-01, -2.5899e-02, -2.8863e-02,  1.0811e-02,\n",
       "          -5.7379e-02, -1.4042e-03,  2.9702e-02,  2.6494e-02, -9.5670e-02,\n",
       "          -5.9945e-02,  8.5643e-03, -2.1859e-02,  3.7683e-02,  4.8881e-02,\n",
       "          -8.4125e-03, -5.7445e-02, -6.4650e-02, -4.4959e-02, -6.8047e-02,\n",
       "          -6.0976e-02,  3.1557e-02,  8.4709e-02, -8.8897e-03, -2.1095e-02,\n",
       "           1.8838e-02,  8.7381e-03,  1.8251e-02,  5.8604e-02,  3.2320e-02,\n",
       "          -3.7322e-02,  3.3061e-01, -3.4112e-02,  3.1060e-02, -3.3945e-02,\n",
       "          -7.0014e-02,  3.1501e-02,  1.4371e-02,  3.4753e-02,  4.7104e-02,\n",
       "          -6.1685e-02, -7.3771e-03,  1.9168e-02, -6.0186e-02, -7.5068e-02,\n",
       "          -5.7825e-03, -1.1486e-02, -5.9923e-02, -9.5777e-03,  1.3806e-02,\n",
       "          -2.3892e-02, -1.0990e-02, -1.1169e-01,  2.8827e-02,  3.5158e-02,\n",
       "          -2.6353e-02,  4.9600e-02,  7.6514e-02, -2.2147e-02,  3.1201e-02,\n",
       "          -4.1287e-02,  1.1299e-02, -4.9127e-02,  2.6270e-02, -2.9927e-02,\n",
       "           6.5112e-03, -5.1112e-03,  3.5478e-02, -5.5106e-02,  3.0441e-02,\n",
       "          -5.9057e-02,  4.4222e-02,  2.6596e-01, -1.2109e-01,  9.6759e-02,\n",
       "           9.2333e-03, -6.2211e-03, -4.4452e-01,  4.7858e-02, -4.4295e-02,\n",
       "          -3.5924e-02, -5.6740e-02,  6.7777e-02, -2.9575e-02,  3.2862e-02,\n",
       "          -5.8330e-02, -4.5941e-02, -4.7626e-03, -1.9442e-02,  5.4803e-02,\n",
       "           1.6185e-01, -2.8019e-04, -1.9571e-02,  7.5602e-02, -2.0908e-02,\n",
       "          -3.0257e-02, -3.1114e-02, -3.0194e-02,  3.7656e-02, -3.7747e-02,\n",
       "           3.0538e-02,  1.0822e-02,  3.7809e-02,  6.4797e-02, -2.7935e-02,\n",
       "          -1.0260e-01, -3.7790e-02, -1.9329e-02,  4.4443e-02,  2.0970e-02,\n",
       "           2.2259e-03,  7.6149e-02,  1.7106e-02,  3.4002e-02, -5.8860e-03,\n",
       "          -4.5300e-02, -6.3961e-02, -1.0417e-02,  1.7965e-02,  1.1858e-01,\n",
       "           3.8085e-01, -4.0870e-02, -5.5968e-02,  8.9587e-02, -7.8765e-01,\n",
       "           3.1667e-02,  2.1208e-02,  2.7260e-02,  3.7946e-02, -2.2684e-01,\n",
       "          -2.4489e-02,  3.0228e-03, -7.1809e-02,  1.3344e-02,  6.5386e-02,\n",
       "          -5.9601e-02,  4.9047e-02, -1.1868e-02,  7.4700e-03,  2.6040e-02,\n",
       "           2.0015e-02,  2.4992e-02, -2.5667e-02,  4.1061e-02, -2.9949e-02,\n",
       "          -3.0929e-02,  2.3569e-02, -1.4482e-02, -1.0348e-02,  3.1083e-02,\n",
       "          -8.1354e-02, -1.1312e-01, -4.4573e-03, -1.2991e-01, -2.2973e-02,\n",
       "          -6.8890e-02, -2.3960e-02, -9.9284e-02, -3.8864e-02,  1.5546e-01,\n",
       "           1.8770e-02,  3.4739e-02, -2.0489e-02, -9.8904e-04,  5.5327e-02,\n",
       "           3.6025e-03, -3.0971e-02,  2.6657e-02, -1.3525e-01,  4.9043e-02,\n",
       "           9.0457e-02, -7.2482e-02, -1.1320e-03, -4.9702e-02,  1.4879e-01,\n",
       "           7.6659e-02,  9.9901e-02,  3.0338e-02,  1.9228e-01, -4.8531e-02,\n",
       "           6.8967e-02, -4.2930e-02,  6.5459e-02, -4.3467e-02,  4.1230e-02,\n",
       "          -5.0618e-02, -3.4119e-02,  1.1872e-02, -3.9292e-02, -1.2280e-01,\n",
       "           1.9020e-02,  5.0390e-02,  2.7369e-02,  2.7672e-02, -3.3970e-02,\n",
       "           1.0405e-01, -1.3155e-01, -1.4331e-01, -3.3747e-02, -1.5717e-02,\n",
       "           3.7247e-02, -4.3495e-02,  6.1332e-02, -2.8421e-02,  8.3272e-03,\n",
       "           3.1673e-02, -1.8356e-01, -9.3043e-02, -3.7996e-03,  2.3487e-02,\n",
       "          -3.4025e-02,  9.9973e-03, -2.2887e-02, -1.4004e-02,  1.4925e-03,\n",
       "           1.1407e-02,  1.4305e-03,  3.4734e-02]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb0_norm1(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a108c-5a42-4e0b-a821-7cbdd487b911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
