{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a34884d-b8cc-4f2d-a947-4ccd738b1770",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LLMFS Load gpt2 weights into pytorch model\n",
    "Extracts from LLMFS to load weights into pytorch implementation of GPT2. Test different layers with simple values to compare with Tensorflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "168cf05e-54fa-4d66-bde1-fd714056cd77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f01e16b-0464-4a57-a47f-e9fc7dc9d9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.7.3\n",
      "numpy version: 1.26.3\n",
      "tiktoken version: 0.12.0\n",
      "torch version: 2.1.1+cu121\n",
      "tensorflow version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505e795-c2b5-42ea-9f3b-53110889a46d",
   "metadata": {},
   "source": [
    "### 5.1.1 Using GPT to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3567851a-451c-4820-820c-06b28e63d7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "# If the `previous_chapters.py` file is not available locally,\n",
    "# you can import it from the `llms-from-scratch` PyPI package.\n",
    "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# E.g.,\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95d824b-cdda-4f3e-a878-b53e64588d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch04 import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0bf60-6741-4b1d-9675-88da51bf170d",
   "metadata": {},
   "source": [
    "### 5.3.3 Modifying the text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dafb71fd-cdd6-4ab9-8c4e-217b29e0b306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for i in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        print(\"i=\", i, \"idx_cond=\", idx_cond)\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        # print(\"  logits=\", logits.shape)\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "        # print(\"  idx_next=\", idx_next)\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13f658e9-e550-49ba-9af6-ede4bdf1706b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0 idx_cond= tensor([[6109, 3626, 6100,  345]])\n",
      "i= 1 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240]])\n",
      "i= 2 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686]])\n",
      "i= 3 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686, 28611]])\n",
      "i= 4 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686, 28611, 38876]])\n",
      "i= 5 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686, 28611, 38876, 48215]])\n",
      "i= 6 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686, 28611, 38876, 48215, 21196]])\n",
      "i= 7 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686, 28611, 38876, 48215, 21196,\n",
      "           994]])\n",
      "i= 8 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686, 28611, 38876, 48215, 21196,\n",
      "           994, 19513]])\n",
      "i= 9 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686, 28611, 38876, 48215, 21196,\n",
      "           994, 19513, 31728]])\n",
      "i= 10 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686, 28611, 38876, 48215, 21196,\n",
      "           994, 19513, 31728, 39042]])\n",
      "i= 11 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686, 28611, 38876, 48215, 21196,\n",
      "           994, 19513, 31728, 39042,  5460]])\n",
      "i= 12 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686, 28611, 38876, 48215, 21196,\n",
      "           994, 19513, 31728, 39042,  5460, 16454]])\n",
      "i= 13 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686, 28611, 38876, 48215, 21196,\n",
      "           994, 19513, 31728, 39042,  5460, 16454, 38918]])\n",
      "i= 14 idx_cond= tensor([[ 6109,  3626,  6100,   345, 41240, 11686, 28611, 38876, 48215, 21196,\n",
      "           994, 19513, 31728, 39042,  5460, 16454, 38918,  9588]])\n",
      "Output text:\n",
      " Every effort moves you inferred rolleduint fabricationagos remarkably hereuced saints freewaylookOkayRand salary baseless\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538d86a-fb76-4b0c-b67e-061fe1121c95",
   "metadata": {},
   "source": [
    "## 5.5 Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b85fccf-4103-41b8-9fcc-99a9fcef28f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n",
      "tqdm version: 4.66.1\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6a82943-e2ef-404b-b547-2275f65d4ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 14:28:46.039379: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-26 14:28:46.039466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-26 14:28:46.041159: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-26 14:28:46.051397: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-26 14:28:47.447750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d26d6d7a-20c0-4acd-94d2-9cd72c01aaa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4691c14a-4cb4-4659-8dd1-7413ddfaf231",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a2eecc9-8e4d-41fb-8224-343191d3d3c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78ab43a0-d4cb-46be-8034-67f5899d47b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43abbca0-14e5-40d1-956c-070353e309a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "416ae36f-eff8-446c-aecf-c048f9e2424c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff4a3017-d70b-471e-a383-a908f70fa52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb7c80fa-1151-4d98-abff-266c7ad1b59c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92ff7031-eda4-4bef-811a-08f13d15ebd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
      "\n",
      "This would remove you from a battle\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea0a4afc-1dd7-4432-8909-3c895db4617d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=text_to_token_ids(\"Every effort moves you\", tokenizer)\n",
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ece601cf-f837-4906-8f24-8c2af7c95bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beb6f298-e0c0-45d5-bd2f-6e707ec40636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tok_emb = gpt.tok_emb\n",
    "pos_emb = gpt.pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2d0e8d5-3e81-4e22-b548-fb05cbc35b30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "gpt.to(\"cpu\");\n",
    "x = torch.tensor([[1]])\n",
    "batch_size, seq_len = x.shape\n",
    "tok_embeds = tok_emb(x)\n",
    "pos_embeds = pos_emb(torch.arange(seq_len, device=x.device))\n",
    "x_embedded = tok_embeds + pos_embeds\n",
    "# x_embedded # [ 2.1520e-02, -2.4603e-01,  5.0275e-02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0353e792-9856-4753-9345-8776a6b345b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dir(gpt.trf_blocks)\n",
    "# gpt.trf_blocks\n",
    "tb0_norm1 = gpt.trf_blocks.get_submodule(\"0.norm1\")\n",
    "# dir(tb0_norm1)\n",
    "x = torch.tensor(np.ones((1, 768) , dtype=np.float32))\n",
    "# tb0_norm1(x) # [-3.6773e-03,  2.7197e-02, -6.4041e-02\n",
    "\n",
    "tb11_norm1 = gpt.trf_blocks.get_submodule(\"11.norm1\")\n",
    "#tb11_norm1(x) # [ 5.0957e-02,  5.3063e-03,  7.1952e-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a92dc61f-20fa-4ec6-abab-79c19b2d70ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test query layer, key layer, value layer\n",
    "x = torch.tensor(np.ones((1, 768) , dtype=np.float32))\n",
    "\n",
    "# block 0\n",
    "att0 = gpt.trf_blocks.get_submodule(\"0.att\")\n",
    "query0 = att0.get_submodule(\"W_query\")\n",
    "# query0(x) # [-1.3708e+01,  1.3385e+01,  1.4323e+01\n",
    "key0 = att0.get_submodule(\"W_key\")\n",
    "# key0(x) # [ 1.8049e-01, -1.4381e-01,  6.2964e-01\n",
    "value0 = att0.get_submodule(\"W_value\")\n",
    "# value0(x)\n",
    "proj0 = att0.get_submodule(\"out_proj\")\n",
    "# proj0(x)\n",
    "# proj0(value0(key0(query0(x)))) # [-2.3273e+01, -7.9272e+02,  5.6245e+02\n",
    "\n",
    "# block 11\n",
    "att11 = gpt.trf_blocks.get_submodule(\"11.att\")\n",
    "query11 = att11.get_submodule(\"W_query\")\n",
    "# query11(x) # [-5.4209e+00,  4.6236e+00,  4.5401e+00\n",
    "key11 = att11.get_submodule(\"W_key\")\n",
    "# key11(x) # [ 5.8911e+00, -3.3184e-01,  6.3656e-01\n",
    "value11 = att11.get_submodule(\"W_value\")\n",
    "# value11(x) # [-1.2480e+00, -3.0783e+00,  5.9679e+00\n",
    "\n",
    "proj11 = att11.get_submodule(\"out_proj\")\n",
    "# proj11(x) # [-4.1535e-01,  2.1763e+00,  4.7958e-01\n",
    "\n",
    "# proj11(value11(key11(query11(x)))) # [ 3.4414e+02,  4.9568e+02,  3.8639e+02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3dd75e1-34d1-402d-9d1c-0b5dd9728c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "x = torch.tensor(np.ones((1, 768) , dtype=np.float32))\n",
    "tb0_norm2 = gpt.trf_blocks.get_submodule(\"0.norm2\")\n",
    "# tb0_norm2(x)\n",
    "\n",
    "tb0_ff = gpt.trf_blocks.get_submodule(\"0.ff\")\n",
    "#tb0_ff(x) # [-1.6735e+01, -6.9883e+00,  4.1138e+00\n",
    "\n",
    "# tb11_norm2 = gpt.trf_blocks.get_submodule(\"11.norm2\")\n",
    "# tb11_norm2(x)\n",
    "tb11_ff = gpt.trf_blocks.get_submodule(\"11.ff\")\n",
    "#tb11_ff(x) # [ 1.3675e+01,  2.2839e+01, -1.7306e+01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79d4dad9-e477-43c1-bc33-fa03f2d6bde9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-32.9011, -31.2024, -34.6623,  ..., -39.4868, -39.8732, -32.2387],\n",
       "         [-55.5208, -53.4286, -56.4767,  ..., -68.1539, -66.7709, -58.6006],\n",
       "         [-61.7969, -60.5386, -59.5503,  ..., -75.3206, -72.7731, -65.5706]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trivial = torch.tensor([[1, 2, 3]])\n",
    "gpt(x_trivial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "454417d5-c110-434d-a176-4e5aa1c7e93f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      " tensor([[[ -35.5820,  -34.9804,  -38.4522,  ...,  -42.0959,  -41.8533,\n",
      "           -35.5966],\n",
      "         [ -76.9601,  -76.6970,  -81.9309,  ...,  -88.7984,  -86.7631,\n",
      "           -78.9627],\n",
      "         [-125.3487, -126.2704, -135.0948,  ..., -132.3173, -135.2544,\n",
      "          -127.6511],\n",
      "         [-136.6002, -137.3804, -146.5556,  ..., -148.2978, -147.2155,\n",
      "          -139.5677]]], grad_fn=<UnsafeViewBackward0>)\n",
      "logits:\n",
      " tensor([[-136.6002, -137.3804, -146.5556,  ..., -148.2978, -147.2155,\n",
      "         -139.5677]], grad_fn=<SliceBackward0>)\n",
      "logits[-1]:\n",
      " tensor([-136.6002, -137.3804, -146.5556,  ..., -148.2978, -147.2155,\n",
      "        -139.5677], grad_fn=<SelectBackward0>)\n",
      "probas:\n",
      " tensor([[1.6013e-03, 7.3391e-04, 7.6013e-08,  ..., 1.3313e-08, 3.9292e-08,\n",
      "         8.2357e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "idx_next:\n",
      " tensor([2651])\n"
     ]
    }
   ],
   "source": [
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "idx = text_to_token_ids(\"Every effort moves you\", tokenizer)\n",
    "\n",
    "# x # tensor([[6109, 3626, 6100,  345]])\n",
    "idx_cond = idx[:, -context_size:]\n",
    "idx_cond # tensor([[6109, 3626, 6100,  345]])\n",
    "logits = gpt(idx)\n",
    "print(\"logits:\\n\", logits)\n",
    "logits = logits[:, -1, :]\n",
    "print(\"logits:\\n\", logits)\n",
    "print(\"logits[-1]:\\n\", logits[-1])\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(\"probas:\\n\", probas)\n",
    "idx_next = torch.argmax(probas, dim=-1)\n",
    "print(\"idx_next:\\n\", idx_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e222a5a-43ef-4049-8018-c91dea99856c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0 idx_cond= tensor([[6109, 3626, 6100,  345]], device='cuda:0')\n",
      "  logits= torch.Size([1, 50257])\n",
      "  idx_next= tensor([[2651]], device='cuda:0')\n",
      "i= 1 idx_cond= tensor([[6109, 3626, 6100,  345, 2651]], device='cuda:0')\n",
      "  logits= torch.Size([1, 50257])\n",
      "  idx_next= tensor([[13]], device='cuda:0')\n",
      "i= 2 idx_cond= tensor([[6109, 3626, 6100,  345, 2651,   13]], device='cuda:0')\n",
      "  logits= torch.Size([1, 50257])\n",
      "  idx_next= tensor([[198]], device='cuda:0')\n",
      "i= 3 idx_cond= tensor([[6109, 3626, 6100,  345, 2651,   13,  198]], device='cuda:0')\n",
      "  logits= torch.Size([1, 50257])\n",
      "  idx_next= tensor([[198]], device='cuda:0')\n",
      "i= 4 idx_cond= tensor([[6109, 3626, 6100,  345, 2651,   13,  198,  198]], device='cuda:0')\n",
      "  logits= torch.Size([1, 50257])\n",
      "  idx_next= tensor([[464]], device='cuda:0')\n",
      "tensor([[6109, 3626, 6100,  345, 2651,   13,  198,  198,  464]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gpt.to(device);\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=5,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    ")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0645b7d8-c307-433d-b5bf-276b8b739878",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you forward.\n",
      "\n",
      "The first step is to understand\n"
     ]
    }
   ],
   "source": [
    "gpt.to(\"cpu\")\n",
    "start_context = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=256\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
