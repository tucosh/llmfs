{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83796365-d871-47d6-8543-885f53fca235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from typing import Any\n",
    "# np.set_printoptions(precision=5)\n",
    "from utils import load_gpt2_params_from_tf_ckpt\n",
    "from utils import load_weights_into_gpt\n",
    "# from utils import print_layer_structure\n",
    "from utils import tldr\n",
    "\n",
    "from utils import generate_text_simple\n",
    "from utils import text_to_token_ids\n",
    "from utils import token_ids_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1b8737-e8df-4f9a-bb43-2a506be6d941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpt2t import GPT2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4912a0c3-b186-47aa-ba9d-6d9a6acb94c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SelfAttention(tf.keras.layers.Layer):\n",
    "#     def __init__(self, d_in, d_out, context_length, dropout, num_heads, b, use_outproj=False): # , use_outproj=False):\n",
    "#         super().__init__(name=\"self\")\n",
    "#         self.debug = False\n",
    "#         assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "#         qkv_bias=True # ?\n",
    "#         self.d_out = d_out\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = d_out // num_heads\n",
    "#         self.query_layer = tf.keras.layers.Dense(units=d_out, activation=None, name=f\"query-{b}\", use_bias=qkv_bias)\n",
    "#         self.query_layer.build((None, d_in))\n",
    "#         self.key_layer = tf.keras.layers.Dense(units=d_out, activation=None, name=f\"key-{b}\", use_bias=qkv_bias)\n",
    "#         self.key_layer.build((None, d_in))\n",
    "#         self.value_layer = tf.keras.layers.Dense(units=d_out, activation=None, name=f\"value-{b}\", use_bias=qkv_bias)\n",
    "#         self.value_layer.build((None, d_in))\n",
    "#         self.use_outproj = use_outproj\n",
    "\n",
    "#         self.out_proj = tf.keras.layers.Dense(units=d_out, activation=None, name=f\"proj-{b}\", use_bias=True)\n",
    "#         self.out_proj.build((None, d_out))\n",
    "\n",
    "#         mask = tf.ones((context_length, context_length), dtype=tf.bool) # square matrix of True\n",
    "#         causal_mask = tf.linalg.band_part(mask, num_lower=-1, num_upper=0) # upper right becomes False\n",
    "#         additive_mask = 1.0 - tf.cast(causal_mask, dtype=tf.float32) # upper right becomes 1.0\n",
    "#         self.additive_mask_applied = additive_mask * -1e9   # upper right is large negative value\n",
    "        \n",
    "#     def __call__(self, inputs):\n",
    "#         if self.debug:\n",
    "#             print(f\".... input to    SelfAttention: {tldr(inputs)}\")\n",
    "#         batch_size, num_tokens, d_in = inputs.shape\n",
    "\n",
    "#         keys = self.key_layer(inputs)      \n",
    "#         queries = self.query_layer(inputs)\n",
    "#         values = self.value_layer(inputs)\n",
    "        \n",
    "#         keys = tf.reshape(keys, [batch_size, num_tokens, self.num_heads, self.head_dim ]) # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim): 2, 6, 2 -> 2, 6, 2, 1        \n",
    "#         queries = tf.reshape(queries, [batch_size, num_tokens, self.num_heads, self.head_dim ]) # 2, 6, 2, 1\n",
    "#         values = tf.reshape(values, [batch_size, num_tokens, self.num_heads, self.head_dim ]) # 2, 6, 2, 1\n",
    "\n",
    "#         keys = tf.transpose(keys, perm=[0, 2, 1, 3])      # [2,6,2,1] ->  [2, 2, 6, 1]        \n",
    "#         values = tf.transpose(values, perm=[0, 2, 1, 3])      # [2,6,2,1] ->  [2, 2, 6, 1]\n",
    "#         queries = tf.transpose(queries, perm=[0, 2, 1, 3])      # [2,6,2,1] ->  [2, 2, 6, 1]\n",
    "        \n",
    "#         attn_scores = tf.matmul(queries, tf.transpose(keys, perm=[0, 1, 3, 2]))\n",
    "#         trimmed_additive_mask_applied = self.additive_mask_applied[:num_tokens, :num_tokens]\n",
    "#         attn_scores = attn_scores + trimmed_additive_mask_applied\n",
    "#         attn_weights = tf.nn.softmax(attn_scores / keys.shape[-1]**0.5, axis=-1)\n",
    "\n",
    "#         context_vec = tf.matmul(attn_weights, values)        \n",
    "#         context_vec = tf.transpose(context_vec, perm=[0, 2, 1, 3])\n",
    "#         context_vec = tf.reshape(context_vec, [batch_size, num_tokens, self.d_out]) # (b, num_tokens, self.d_out)\n",
    "\n",
    "#         if self.use_outproj: # llmfs uses this!\n",
    "#             context_vec = self.out_proj(context_vec)\n",
    "#         if self.debug:\n",
    "#             print(f\".... output from SelfAttention: {tldr(context_vec)}\")\n",
    "#         return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272ce61f-6a52-4f8e-b30d-158f3044429d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class AttentionLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, d_in, d_out, context_length, dropout, num_heads, b):\n",
    "#         super().__init__(name=\"attention\")\n",
    "#         self.layer_norm = tf.keras.layers.LayerNormalization(name=\"layer_norm\") \n",
    "#         self.self_attention = SelfAttention(d_in, d_out, context_length, dropout, num_heads, b)\n",
    "#         self.projection = tf.keras.layers.Dense(units=d_out, activation=None, name=f\"projection\")\n",
    "#         self.debug = False\n",
    "    \n",
    "#     def __call__(self, inputs):\n",
    "#         if self.debug:\n",
    "#             print(f\".... input to    layer_norm: {tldr(inputs)}\")\n",
    "#         x = self.layer_norm(inputs)\n",
    "#         if self.debug:\n",
    "#             print(f\".... output from layer_norm: {tldr(x)}\")\n",
    "#         x = self.self_attention(x)\n",
    "#         if self.debug:\n",
    "#             print(f\".... input to    projection: {tldr(x)}\")\n",
    "#         x = self.projection(x)\n",
    "#         if self.debug:\n",
    "#             print(f\".... output from projection: {tldr(x)}\")\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b5b4716-2e48-44cb-946c-122c80b032fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class MultiLayerPerceptron(tf.keras.layers.Layer):\n",
    "#     def __init__(self, d_out, b):\n",
    "#         super().__init__(name=\"mlp\")\n",
    "#         self.layer_norm = tf.keras.layers.LayerNormalization(name=f\"layer_norm\")\n",
    "#         self.perceptron = tf.keras.layers.Dense(units=d_out * 4, activation=tf.keras.activations.gelu, name=f\"perceptron\")\n",
    "#         self.projection = tf.keras.layers.Dense(units=d_out, name=f\"projection\")\n",
    "#         self.debug = False\n",
    "#     def __call__(self, inputs):\n",
    "#         if self.debug:\n",
    "#             print(f\".... input to    mlp: {tldr(inputs)}\")\n",
    "#         x = self.layer_norm(inputs)\n",
    "#         x = self.perceptron(x)\n",
    "#         x = self.projection(x)\n",
    "#         if self.debug:\n",
    "#             print(f\".... output from mlp: {tldr(x)}\")\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e617c3-fe01-4f59-9124-7d631a7daa82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Block(tf.keras.layers.Layer):\n",
    "#     def __init__(self, d_in, d_out, context_length, dropout, num_heads, b, qkv_bias=False):\n",
    "#         super().__init__(name=f'block-{b}')\n",
    "#         self.b = b       \n",
    "\n",
    "#         self.attention = AttentionLayer(d_in, d_out, context_length, dropout, num_heads, b)\n",
    "#         self.mlp = MultiLayerPerceptron(d_out, b)\n",
    "#         self.debug = False\n",
    "#     def __call__(self, inputs):\n",
    "#         if self.debug:\n",
    "#             print()\n",
    "#             print(f\".. input to    block {self.b}: {tldr(inputs)}\")\n",
    "#         x = inputs\n",
    "#         a = self.attention(x)\n",
    "#         x = x + a\n",
    "#         m = self.mlp(x)\n",
    "#         x = x + m\n",
    "#         if self.debug:\n",
    "#             print(f\".. output from block {self.b}: {tldr(x)}\")\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb57562-0eda-4a06-aa43-48d7437b0482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Transformer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, blocks_num, d_in, d_out, context_length, dropout, num_heads):\n",
    "#         super().__init__(name=\"transformer\")\n",
    "#         self.blocks_num = blocks_num\n",
    "#         self.blocks = []\n",
    "#         for b in range(blocks_num):\n",
    "#             block = Block(d_in, d_out, context_length, dropout, num_heads, b)\n",
    "#             self.blocks.append(block)\n",
    "#         self.layer_norm = tf.keras.layers.LayerNormalization(name=f\"layer_norm\")\n",
    "             \n",
    "#     def __call__(self, inputs):\n",
    "#         x = inputs\n",
    "#         for b in range(self.blocks_num):\n",
    "#             x = self.blocks[b](x)\n",
    "\n",
    "#         x = self.layer_norm(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3025f06e-a885-4114-a531-5892a7f34e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Embedding(tf.keras.layers.Layer):\n",
    "#     def __init__(self, embedding_size, vocab_size, max_position_length, dtype=tf.float32):\n",
    "#         super().__init__(name=\"embedding\", dtype=dtype)\n",
    "#         self.embedding_size = embedding_size\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.max_position_length = max_position_length\n",
    "#         self.word_embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, name=\"word_embedding\")\n",
    "#         self.word_embedding.build((None, self.embedding_size))\n",
    "#         self.position_embedding = tf.keras.layers.Embedding(input_dim=self.max_position_length, output_dim=self.embedding_size, name=\"position_embedding\")\n",
    "#         self.position_embedding.build((None, self.embedding_size))\n",
    "#         self.debug = False\n",
    "#     def __call__(self, inputs):\n",
    "#         we = self.word_embedding(inputs)\n",
    "#         pe = self.position_embedding(tf.range(1024))\n",
    "#         pe_corrected = pe[:we.shape[1], :]\n",
    "#         x = we + pe_corrected\n",
    "#         if self.debug:\n",
    "#             print(f\".. Embedding output: {tldr(x)}\")\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e45f8bb-4d0e-4e5f-93f3-e005701facae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class GPT2t(tf.keras.Model):\n",
    "\n",
    "#     def __init__(self, config, name=None, trainable=True, dtype=None):\n",
    "#         super().__init__(name=name)\n",
    "#         self.trainable = trainable\n",
    "#         self.embedding_size=config['n_embd']\n",
    "#         self.vocab_size=config['n_vocab']\n",
    "#         self.max_position_length=config['n_ctx']\n",
    "#         self.blocks_num = config[\"n_layer\"]\n",
    "#         d_in=config['n_embd']\n",
    "#         d_out=config['n_embd']\n",
    "#         context_length = config['n_ctx']\n",
    "#         num_heads = config['n_head']\n",
    "#         self.embedding = Embedding(embedding_size=self.embedding_size, vocab_size=self.vocab_size, max_position_length=self.max_position_length)\n",
    "#         self.transformer = Transformer(self.blocks_num, d_in, d_out, context_length, dropout=None, num_heads=num_heads)\n",
    "#         self.debug = False\n",
    "        \n",
    "#     def __call__(self, inputs):\n",
    "#         x = self.embedding(inputs)\n",
    "#         x = self.transformer(x)\n",
    "#         if self.debug:\n",
    "#             print(f\".. input to final matmul: {tldr(x)}\")\n",
    "#         logits = tf.matmul(x, self.embedding.word_embedding.get_weights()[0], transpose_b=True)\n",
    "#         if self.debug:\n",
    "#             print(f\".. final logits: {tldr(logits)}\")\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc59b48c-331b-49be-88b4-bd4a529deac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config124M = {'n_embd': 768,  'n_vocab': 50257, 'n_ctx': 1024, 'n_layer': 12, 'n_head': 12}\n",
    "# config355M = {'n_embd': 1024, 'n_vocab': 50257, 'n_ctx': 1024, 'n_layer': 24, 'n_head': 16}\n",
    "gpt2t=GPT2t(config124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a031c7f6-29de-42a2-be79-fa91fe8a2b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_dir=\"ch05/01_main-chapter-code/gpt2/124M\"\n",
    "model_dir=\"gpt2/124M\"\n",
    "tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, config124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ec068e-1cdd-4b78-8ab2-8a57d0e2fbf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "93089510-398a-4b0d-8efe-248997f2d092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['g'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4fa5e54b-282a-48f2-adb6-5f370236d6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['blocks'][0]['ln_1']['b'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89fba990-7b36-4404-9940-8d9f66e51689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"blocks\"][0][\"mlp\"][\"c_proj\"][\"w\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbb424-fa1d-432a-8bc0-be93e7386e8a",
   "metadata": {},
   "source": [
    "### Now make a function to load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c57f35c-b110-48e6-9b36-f1b2347dd378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def load_weights_into_gpt(gpt, params):\n",
    "#     n_embd = params['g'].shape[0]\n",
    "#     _ = gpt.embedding(tf.constant([[1]]))\n",
    "#     assert gpt.embedding.word_embedding.built\n",
    "#     assert gpt.embedding.position_embedding.built\n",
    "#     gpt.embedding.word_embedding.set_weights([np.array(params['wte'])])\n",
    "#     gpt.embedding.position_embedding.set_weights([np.array(params['wpe'])])\n",
    "#     for b in range(len(params[\"blocks\"])):\n",
    "#         gpt.transformer.blocks[b].attention.layer_norm.beta_initializer = tf.keras.initializers.Constant(params[\"blocks\"][b][\"ln_1\"][\"b\"]) \n",
    "#         gpt.transformer.blocks[b].attention.layer_norm.gamma_initializer = tf.keras.initializers.Constant(params[\"blocks\"][b][\"ln_1\"][\"g\"]) \n",
    "#         gpt.transformer.blocks[b].attention.layer_norm.build((None, None, n_embd))\n",
    "#         assert gpt.transformer.blocks[b].attention.layer_norm.built\n",
    "\n",
    "#         q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "#         q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        \n",
    "#         assert gpt.transformer.blocks[b].attention.self_attention.query_layer.built\n",
    "#         gpt.transformer.blocks[b].attention.self_attention.query_layer.set_weights([q_w, q_b])\n",
    "#         assert gpt.transformer.blocks[b].attention.self_attention.key_layer.built\n",
    "#         gpt.transformer.blocks[b].attention.self_attention.key_layer.set_weights([k_w, k_b])\n",
    "#         assert gpt.transformer.blocks[b].attention.self_attention.value_layer.built\n",
    "#         gpt.transformer.blocks[b].attention.self_attention.value_layer.set_weights([v_w, v_b])\n",
    "    \n",
    "#         # AttentionLayer projection\n",
    "#         gpt.transformer.blocks[b].attention.projection.build((None, n_embd))\n",
    "#         assert gpt.transformer.blocks[b].attention.projection.built\n",
    "#         gpt.transformer.blocks[b].attention.projection.set_weights([params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"], params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"]])      \n",
    "    \n",
    "#         # MultiLayerPerceptron layer_norm\n",
    "#         gpt.transformer.blocks[b].mlp.layer_norm.beta_initializer = tf.keras.initializers.Constant(params[\"blocks\"][b][\"ln_2\"][\"b\"]) \n",
    "#         gpt.transformer.blocks[b].mlp.layer_norm.gamma_initializer = tf.keras.initializers.Constant(params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "#         gpt.transformer.blocks[b].mlp.layer_norm.build((None, None, n_embd))\n",
    "#         assert gpt.transformer.blocks[b].mlp.layer_norm.built\n",
    "    \n",
    "#         # MultiLayerPerceptron perceptron\n",
    "#         gpt.transformer.blocks[b].mlp.perceptron.build((None, n_embd))\n",
    "#         assert gpt.transformer.blocks[b].mlp.perceptron.built\n",
    "#         gpt.transformer.blocks[b].mlp.perceptron.set_weights([params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"], params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]])\n",
    "#         # MultiLayerPerceptron projection\n",
    "#         mlp_proj_embd = params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].shape[0]\n",
    "#         gpt.transformer.blocks[b].mlp.projection.build((None, mlp_proj_embd))\n",
    "#         assert gpt.transformer.blocks[b].mlp.projection.built\n",
    "#         gpt.transformer.blocks[b].mlp.projection.set_weights([params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"], params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"]])    \n",
    "            \n",
    "#     gpt.transformer.layer_norm.beta_initializer = tf.keras.initializers.Constant(params[\"b\"])\n",
    "#     gpt.transformer.layer_norm.gamma_initializer = tf.keras.initializers.Constant(params[\"g\"])\n",
    "#     gpt.transformer.layer_norm.build((None, None, n_embd))\n",
    "#     assert gpt.transformer.layer_norm.built    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "678d444d-1e98-4a2b-be38-5906c00ea386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt2t, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1f86b0-5978-4e0e-8b0f-7fca7ec5dc84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_effort_emb float32 (1, 4, 768) [ 0.07927368 -0.2979193   0.08817437]\n"
     ]
    }
   ],
   "source": [
    "x_effort = tf.constant([[6109, 3626, 6100, 345]])\n",
    "x_effort_emb = gpt2t.embedding(x_effort)\n",
    "print(\"x_effort_emb\", tldr(x_effort_emb))\n",
    "assert x_effort_emb.shape == [1, 4, 768]\n",
    "assert np.allclose(x_effort_emb[0][0][:3], np.array([0.07927368, -0.2979193 ,  0.08817437]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6f3c0c-1079-4e15-b04b-66aa430b461a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = gpt2t(x_effort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a80b9c-af28-45a5-9428-b0daa488691c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert y.shape == [1, 4, 50257]\n",
    "assert np.allclose(y[0][0][:3], np.array([-35.52612, -34.92841, -38.39917]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df40e3c3-9b5c-4262-99f3-118250a1c604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_ids = generate_text_simple(gpt2t, idx=x_effort, max_new_tokens=5, context_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24a02ad1-606a-4f6f-bda9-865e48722034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.array_equal(token_ids.numpy(), np.array([[6109, 3626, 6100,  345, 2651,   13,  198,  198,  464]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39b5fa85-7c81-4312-87be-d82ca3735cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you forward.\n",
      "\n",
      "The first step is to understand\n"
     ]
    }
   ],
   "source": [
    "# import tiktoken\n",
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "start_context = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=gpt2t,\n",
    "    idx=text_to_token_ids(start_context),\n",
    "    max_new_tokens=10,\n",
    "    # max_new_tokens=30,\n",
    "    context_size=256\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b2265-db97-4777-8f2a-2b3a0e566406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
