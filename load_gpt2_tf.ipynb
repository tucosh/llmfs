{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5627e4-de33-4b92-be9d-916b59b379f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tensorflow gpt2 implemenation\n",
    "\n",
    "Here we define a tensorflow implementation of gpt2 as provided by https://github.com/ShenakhtPajouh/gpt2-keras\n",
    "and load the weights. The goal is to duplicate as closely as possible the Pytorch implemenation\n",
    "from LLMFS. See load_gpt.ipynb in ch05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f793526-3c5f-4f4a-a010-bf639c158852",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "!pip install -q tiktoken\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5)\n",
    "from utils import clean_up\n",
    "from utils import load_gpt2_params_from_tf_ckpt\n",
    "from utils import print_layer_structure\n",
    "from utils import tldr\n",
    "import gc\n",
    "from utils import generate_text_simple\n",
    "from utils import text_to_token_ids\n",
    "from utils import token_ids_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a60bd5cf-5fa9-4ae0-ab11-1c6c3b42550c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# cleanup_vars = ['AttentionLayer', 'Block', 'Embedding', 'LayerNormalization', 'MultiLayerPerceptron', 'SelfAttention', 'Transformer', 'GPT2'\n",
    "#                 , 'attn', 'block', 'blocks', 'final_norm_layer', 'k_b', 'k_w', 'layer_norm', 'layer_norm0', 'layer_norm11', 'q_w', 'v_w', 'k_w', 'xxxxx', 'gpt2', 'params', 'y']\n",
    "# clean_up(cleanup_vars)\n",
    "# %whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda5a7c-f36e-4384-9f61-adcec16958ef",
   "metadata": {},
   "source": [
    "## gpt2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a395063b-f082-483f-bcc9-fa7919d0288a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From https://github.com/ShenakhtPajouh/gpt2-keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_tensor_shape(x):\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    static_shape = x.shape.as_list()\n",
    "    if tf.executing_eagerly():\n",
    "        return static_shape\n",
    "    dynamic_shape = tf.shape(x)\n",
    "    if static_shape is None:\n",
    "        return dynamic_shape\n",
    "    dynamic_shape = tf.unstack(dynamic_shape)\n",
    "    shape = []\n",
    "    for st, dyn in zip(static_shape, dynamic_shape):\n",
    "        if st is None:\n",
    "            shape.append(dyn)\n",
    "        else:\n",
    "            shape.append(st)\n",
    "    return shape\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "def dropout_fn(x, dropout):\n",
    "    if dropout is None or dropout == 0.0:\n",
    "        return x\n",
    "    else:\n",
    "        return tf.nn.dropout(x, rate=dropout)\n",
    "\n",
    "\n",
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, trainable=True, name=None):\n",
    "        super().__init__(name=name, trainable=trainable)\n",
    "        self.beta = None\n",
    "        self.gamma = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.beta = self.add_weight(name=\"beta\", shape=input_shape[-1:], initializer=tf.zeros_initializer())\n",
    "        self.gamma = self.add_weight(name=\"gamma\", shape=input_shape[-1:], initializer=tf.ones_initializer())\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, axis=-1, epsilon=1e-5):\n",
    "        # mean, variance = tf.nn.moments(inputs, axis, keep_dims=True)\n",
    "        mean, variance = tf.nn.moments(inputs, axis, keepdims=True)\n",
    "        rdev = tf.math.rsqrt(variance + epsilon)\n",
    "        x = (inputs - mean) * rdev\n",
    "        output = x * self.gamma + self.beta\n",
    "        return output\n",
    "\n",
    "    def __call__(self, inputs, axis=-1, epsilon=1e-5):\n",
    "        return super().__call__(inputs=inputs,\n",
    "                                axis=axis, epsilon=epsilon)\n",
    "\n",
    "\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_attention_heads=1, size_per_head=512,\n",
    "                 one_sided=True,\n",
    "                 query_act=None,\n",
    "                 initializer_range=0.02,\n",
    "                 value_act=None,\n",
    "                 key_act=None,\n",
    "                 trainable=True,\n",
    "                 name=None):\n",
    "        super().__init__(name=name, trainable=trainable)\n",
    "        # `query_layer` = [B*F, N*H]\n",
    "        self.attention_size = num_attention_heads * size_per_head\n",
    "        self.query_layer = tf.keras.layers.Dense(\n",
    "            num_attention_heads * size_per_head,\n",
    "            activation=query_act,\n",
    "            name=\"query\",\n",
    "            kernel_initializer=tf.random_normal_initializer(stddev=initializer_range)\n",
    "        )\n",
    "        # `key_layer` = [B*T, N*H]\n",
    "        self.key_layer = tf.keras.layers.Dense(\n",
    "            num_attention_heads * size_per_head,\n",
    "            activation=key_act,\n",
    "            name=\"key\",\n",
    "            kernel_initializer=tf.random_normal_initializer(stddev=initializer_range)\n",
    "        )\n",
    "        # `value_layer` = [B*T, N*H]\n",
    "        self.value_layer = tf.keras.layers.Dense(\n",
    "            num_attention_heads * size_per_head,\n",
    "            activation=value_act,\n",
    "            name=\"value\",\n",
    "            kernel_initializer=tf.random_normal_initializer(stddev=initializer_range)\n",
    "        )\n",
    "        self.size_per_head = size_per_head\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.one_sided = one_sided\n",
    "\n",
    "    def reshape(self, x, use_2d=False, shape=None):\n",
    "        if use_2d:\n",
    "            batch_size, seq_length = shape[0], shape[1]\n",
    "        else:\n",
    "            _shape = get_tensor_shape(x)\n",
    "            batch_size, seq_length = _shape[0], _shape[1]\n",
    "        x = tf.reshape(x, [batch_size, seq_length, self.num_attention_heads, self.size_per_head])\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        return x\n",
    "\n",
    "    def final_shape(self, x, use_2d=False):\n",
    "        shape = get_tensor_shape(x)\n",
    "        batch_size, seq_length = shape[0], shape[2]\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        if use_2d:\n",
    "            x = tf.reshape(x, [batch_size * seq_length, self.num_attention_heads * self.size_per_head])\n",
    "        else:\n",
    "            x = tf.reshape(x, [batch_size, seq_length, self.num_attention_heads * self.size_per_head])\n",
    "        return x\n",
    "\n",
    "    def get_mask(self, inputs_shape, cache_length=None, mask=None):\n",
    "        batch_size, seq_length = inputs_shape[0], inputs_shape[2]\n",
    "        if self.one_sided:\n",
    "            rng = tf.range(seq_length)\n",
    "            one_sided_mask = tf.less_equal(rng, tf.expand_dims(rng, 1))\n",
    "            if cache_length is not None:\n",
    "                prev_mask = tf.ones([seq_length, cache_length], tf.bool)\n",
    "                one_sided_mask = tf.concat([prev_mask, one_sided_mask], 1)\n",
    "        if mask is not None:\n",
    "            if cache_length is not None:\n",
    "                prev_mask = tf.ones([batch_size, cache_length], tf.bool)\n",
    "                mask = tf.concat([prev_mask, mask], 1)\n",
    "            if cache_length is None:\n",
    "                cache_length = 0\n",
    "            mask = tf.reshape(mask, [batch_size, 1, 1, seq_length + cache_length])\n",
    "        if self.one_sided:\n",
    "            if mask is not None:\n",
    "                one_sided_mask = tf.logical_and(mask, one_sided_mask)\n",
    "            return one_sided_mask\n",
    "        else:\n",
    "            return mask\n",
    "\n",
    "    def attend(self, query, key, value, mask=None, dropout=None):\n",
    "        dim = tf.cast(self.size_per_head, query.dtype)\n",
    "        _sqrt = tf.math.sqrt(dim)\n",
    "        _sqrt = tf.cast(_sqrt, query.dtype)\n",
    "        coefficients = tf.matmul(query, key, transpose_b=True) / _sqrt\n",
    "        # print(\"SelfAttention: reshape coefficients.shape=\", coefficients.shape, coefficients[0][0][0][:3])\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, coefficients.dtype)\n",
    "            coefficients = coefficients * mask - (1 - mask) * 1e5\n",
    "        coefficients = tf.math.softmax(coefficients, -1)\n",
    "        coefficients = dropout_fn(coefficients, dropout)\n",
    "        results = tf.matmul(coefficients, value)\n",
    "        # print(\"SelfAttention: results coefficients.shape=\", results.shape, results[0][0][0][:3])\n",
    "        return results\n",
    "\n",
    "    def call(self, inputs, cache=None, mask=None,\n",
    "             attention_dropout=None, return_cache=False,\n",
    "             use_2d=False, shape=None):\n",
    "        \"\"\"\n",
    "        inputs: a tensor of shape [batch_size, seq_length, dim] if use_2d is false,\n",
    "                else a tensor of shape [batch_size * seq_length, dim]\n",
    "        cache: A dictionary consist of key and value from previous calls.\n",
    "        mask: a boolean tensor of shape [batch_size, seq_length]\n",
    "        attention_probs_dropout_prob: dropout use for attention mechanism\n",
    "        return_cache: if True, it returns key and values as besides layer output\n",
    "        use_2d: if it is True, the model uses 2D matrices as inputs and outputs\n",
    "        shape: if use_2d is True, then the shape is [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        print(f\".... input to    SelfAttention: {tldr(inputs)}\")\n",
    "        query = self.query_layer(inputs)\n",
    "        # print(\"SelfAttention: query.shape=\", query.shape, query[0][0][:3])\n",
    "        key = self.key_layer(inputs)\n",
    "        value = self.value_layer(inputs)\n",
    "        if use_2d and shape is None:\n",
    "            raise ValueError(\"if use_2d is True, then the shape must be specified\")\n",
    "        query = self.reshape(query, use_2d, shape)\n",
    "        # print(\"SelfAttention: reshape query.shape=\", query.shape, query[0][0][0][:3])\n",
    "        key = self.reshape(key, use_2d, shape)\n",
    "        value = self.reshape(value, use_2d, shape)\n",
    "        cache_length = None\n",
    "        if cache is not None:\n",
    "            key = tf.concat([cache[\"key\"], key], 2)\n",
    "            value = tf.concat([cache[\"value\"], value], 2)\n",
    "            cache_length = get_tensor_shape(cache[\"key\"])[2]\n",
    "        inputs_shape = get_tensor_shape(query)\n",
    "        mask = self.get_mask(inputs_shape, cache_length, mask)\n",
    "        result = self.attend(query, key, value, mask, attention_dropout)\n",
    "        result = self.final_shape(result, use_2d)\n",
    "        print(f\".... output from SelfAttention: {tldr(result)}\")\n",
    "        if return_cache:\n",
    "            cache = {\"key\": key, \"value\": value}\n",
    "            return result, cache\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def __call__(self, inputs, cache=None, mask=None,\n",
    "             attention_dropout=None, return_cache=False,\n",
    "             use_2d=False, shape=None):\n",
    "        \"\"\"\n",
    "        inputs: a tensor of shape [batch_size, seq_length, dim] if use_2d is false,\n",
    "                else a tensor of shape [batch_size * seq_length, dim]\n",
    "        cache: A dictionary consist of key and value from previous calls.\n",
    "        mask: a boolean tensor of shape [batch_size, seq_length]\n",
    "        attention_probs_dropout_prob: dropout use for attention mechanism\n",
    "        return_cache: if True, it returns key and values as besides layer output\n",
    "        use_2d: if it is True, the model uses 2D matrices as inputs and outputs\n",
    "        shape: if use_2d is True, then the shape is [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        return super().__call__(\n",
    "            inputs=inputs,\n",
    "            cache=cache,\n",
    "            mask=mask,\n",
    "            attention_dropout=attention_dropout,\n",
    "            return_cache=return_cache,\n",
    "            use_2d=use_2d,\n",
    "            shape=shape\n",
    "        )\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, config, name=None, trainable=True, initializer_range=0.02):\n",
    "        super().__init__(name=name, trainable=trainable)\n",
    "        self.layer_norm = LayerNormalization(name=\"layer_norm\")\n",
    "        self.self_attention = SelfAttention(num_attention_heads=config[\"n_head\"],\n",
    "                                            size_per_head=config[\"n_embd\"] // config[\"n_head\"],\n",
    "                                            initializer_range=initializer_range,\n",
    "                                            name=\"self\"\n",
    "                                            )\n",
    "        self.projection = tf.keras.layers.Dense(units=config[\"n_embd\"],\n",
    "                                                kernel_initializer=tf.random_normal_initializer(stddev=initializer_range),\n",
    "                                                name=\"projection\")\n",
    "\n",
    "\n",
    "    def call(self, inputs, cache=None, dropout=None, attention_dropout=None,\n",
    "             return_cache=False, use_2d=False, shape=None):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: a tensor of shape [batch_size, seq_length, dim] if use_2d is False, else [batch_size * seq_length, dim]\n",
    "        cache: (Optional): a dictionary of tensors key and value from previous calls.\n",
    "        return_cache: if True, returns a dictionary of key and value tensors besides layer output.\n",
    "        use_2d: if is True then the inputs and outputs are 2D tensors instead of 3D (for tpu performance)\n",
    "        shape: if use_2d then it's [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        print(f\".... input to    layer_norm: {tldr(inputs)}\")\n",
    "        x = self.layer_norm(inputs)\n",
    "        print(f\".... output from layer_norm: {tldr(x)}\")\n",
    "        x = self.self_attention(x, attention_dropout=attention_dropout,\n",
    "                                cache=cache,\n",
    "                                return_cache=return_cache,\n",
    "                                use_2d=use_2d,\n",
    "                                shape=shape)\n",
    "        if return_cache:\n",
    "            x, cache = x\n",
    "        print(f\".... input to    projection: {tldr(x)}\")            \n",
    "        x = self.projection(x)\n",
    "        print(f\".... output from projection: {tldr(x)}\")        \n",
    "        # x = dropout_fn(x, dropout)\n",
    "        if return_cache:\n",
    "            return x, cache\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def __call__(self, inputs, cache=None, dropout=None, attention_dropout=None,\n",
    "                 return_cache=False, use_2d=False, shape=None):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: a tensor of shape [batch_size, seq_length, dim] if use_2d is False, else [batch_size * seq_length, dim]\n",
    "        cache: (Optional): a dictionary of tensors key and value from previous calls.\n",
    "        return_cache: if True, returns a dictionary of key and value tensors besides layer output.\n",
    "        use_2d: if is True then the inputs and outputs are 2D tensors instead of 3D (for tpu performance)\n",
    "        shape: if use_2d then it's [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        return super().__call__(\n",
    "            inputs=inputs,\n",
    "            cache=cache,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            return_cache=return_cache,\n",
    "            use_2d=use_2d,\n",
    "            shape=shape\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, activation_fn=None, embedding_size=768,\n",
    "                 perceptron_size=3072, trainable=True,\n",
    "                 initializer_range=0.02, name=None):\n",
    "        super().__init__(name=name, trainable=trainable)\n",
    "        self.layer_norm = LayerNormalization(name=\"layer_norm\")\n",
    "        self.perceptron = tf.keras.layers.Dense(units=perceptron_size,\n",
    "                                                activation=activation_fn,\n",
    "                                                kernel_initializer=tf.random_normal_initializer(stddev=initializer_range),\n",
    "                                                name=\"perceptron\")\n",
    "        self.projection = tf.keras.layers.Dense(units=embedding_size,\n",
    "                                                kernel_initializer=tf.random_normal_initializer(stddev=initializer_range),\n",
    "                                                name=\"projection\")\n",
    "\n",
    "    def call(self, inputs, dropout=None):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: tensor of [batch_size, seq_length, dim]\n",
    "\n",
    "        \"\"\"\n",
    "        print(f\".... input to    mlp: {tldr(inputs)}\")\n",
    "        x = self.layer_norm(inputs)\n",
    "        x = self.perceptron(x)\n",
    "        x = self.projection(x)\n",
    "        x = dropout_fn(x, dropout)\n",
    "        print(f\".... output from mlp: {tldr(x)}\")\n",
    "        return x\n",
    "\n",
    "    def __call__(self, inputs, dropout=None):\n",
    "        return super().__call__(inputs=inputs,\n",
    "                                dropout=dropout)\n",
    "\n",
    "\n",
    "class Block(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, config, trainable=True, initializer_range=0.02, name=None):\n",
    "        super().__init__(name=name, trainable=trainable)\n",
    "        self.attention = AttentionLayer(config=config,\n",
    "                                        initializer_range=initializer_range,\n",
    "                                        name=\"attention\")\n",
    "        self.mlp = MultiLayerPerceptron(activation_fn=gelu,\n",
    "                                        embedding_size=config[\"n_embd\"],\n",
    "                                        perceptron_size=4 * config[\"n_embd\"],\n",
    "                                        initializer_range=initializer_range,\n",
    "                                        name=\"mlp\")\n",
    "\n",
    "    def call(self, inputs, cache=None, dropout=None, attention_dropout=None,\n",
    "            return_cache=False, use_2d=False, shape=None):\n",
    "        x = inputs\n",
    "        a = self.attention(inputs=x,\n",
    "                           cache=cache,\n",
    "                           dropout=dropout,\n",
    "                           attention_dropout=attention_dropout,\n",
    "                           return_cache=return_cache,\n",
    "                           use_2d=use_2d,\n",
    "                           shape=shape)\n",
    "        if return_cache:\n",
    "            a, cache = a\n",
    "        x = x + a\n",
    "        m = self.mlp(inputs=x,\n",
    "                     dropout=dropout)\n",
    "        x = x + m\n",
    "        if return_cache:\n",
    "            return x, cache\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def __call__(self, inputs, cache=None, dropout=None, attention_dropout=None,\n",
    "                 return_cache=False, use_2d=False, shape=None):\n",
    "        return super().__call__(inputs=inputs,\n",
    "                                cache=cache,\n",
    "                                dropout=dropout,\n",
    "                                attention_dropout=attention_dropout,\n",
    "                                return_cache=return_cache,\n",
    "                                use_2d=use_2d,\n",
    "                                shape=shape)\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, config, trainable=True, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.trainable = trainable\n",
    "        self.blocks = []\n",
    "        self.blocks_num = config[\"n_layer\"]\n",
    "        for ids in range(self.blocks_num):\n",
    "            block = Block(config=config,\n",
    "                          name=\"block_%d\" % ids)\n",
    "            self.blocks.append(block)\n",
    "        self.layer_norm = LayerNormalization(name=\"layer_norm\")\n",
    "\n",
    "    def call(self, inputs, cache=None, dropout=None, attention_dropout=None,\n",
    "             return_cache=False, blocks=None, use_2d=False, shape=None):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: a tensor of shape [batch_size, seq_length, dim], if use_2d is False, else [batch_size * seq_length, dim]\n",
    "        cache: a list of dictionaries. key and values from previous calls.\n",
    "        blocks: a list. if it is specified, the output will be a dictionary {layer_num: layer_output}\n",
    "        return_cache: if it is true, it will returns cache for blocks\n",
    "        use_2d: if it is True, then the operations will define base on 2D tensors. (for tpu performance)\n",
    "        shape: if use_2d is True, then it is [batch_size, seq_length]\n",
    "\n",
    "        \"\"\"\n",
    "        if blocks is None:\n",
    "            max_block = self.blocks_num - 1\n",
    "        elif len(blocks) == 0:\n",
    "            max_block = self.blocks_num - 1\n",
    "            blocks = None\n",
    "        else:\n",
    "            _blocks = []\n",
    "            for i in blocks:\n",
    "                if i >= 0:\n",
    "                    k = i\n",
    "                else:\n",
    "                    k = self.blocks_num - i\n",
    "                if k >= self.blocks_num or k < 0:\n",
    "                    raise ValueError(\"output blocks should be in range [\" + str(0) + \", \" +\n",
    "                                     str(self.blocks_num - 1) + \"]\")\n",
    "                _blocks.append(k)\n",
    "            _blocks = list(sorted(_blocks))\n",
    "            blocks = _blocks\n",
    "            max_block = blocks[-1]\n",
    "        if blocks is not None:\n",
    "            outputs = {}\n",
    "        if return_cache:\n",
    "            new_cache = []\n",
    "        output = inputs\n",
    "        for ids in range(max_block + 1):\n",
    "            if cache is None:\n",
    "                _cache = None\n",
    "            else:\n",
    "                _cache = cache[ids]           \n",
    "            print()\n",
    "            print(f\".. input to    block {ids}: {tldr(output)}\")            \n",
    "            output = self.blocks[ids](inputs=output,\n",
    "                                      cache=_cache,\n",
    "                                      dropout=dropout,\n",
    "                                      attention_dropout=attention_dropout,\n",
    "                                      return_cache=return_cache,\n",
    "                                      use_2d=use_2d,\n",
    "                                      shape=shape)\n",
    "            print(f\".. output from block {ids}: {tldr(output)}\")\n",
    "            if return_cache:\n",
    "                output, _cache = output\n",
    "                new_cache.append(_cache)\n",
    "            if blocks is not None:\n",
    "                if ids in blocks:\n",
    "                    outputs[ids] = output\n",
    "        if blocks is None:\n",
    "            output = self.layer_norm(output)\n",
    "            result = output\n",
    "        else:\n",
    "            result = outputs\n",
    "        if return_cache:\n",
    "            return result, new_cache\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def __call__(self, inputs, cache=None, dropout=None, attention_dropout=None,\n",
    "                 return_cache=False, blocks=None, use_2d=False, shape=None):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: a tensor of shape [batch_size, seq_length, dim], if use_2d is False, else [batch_size * seq_length, dim]\n",
    "        cache: a list of dictionaries. key and values from previous calls.\n",
    "        blocks: a list. if it is specified, the output will be a dictionary {layer_num: layer_output}\n",
    "        return_cache: if it is true, it will returns cache for blocks\n",
    "        use_2d: if it is True, then the operations will define base on 2D tensors. (for tpu performance)\n",
    "        shape: if use_2d is True, then it is [batch_size, seq_length]\n",
    "\n",
    "        \"\"\"\n",
    "        return super().__call__(\n",
    "            inputs=inputs,\n",
    "            cache=cache,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            return_cache=return_cache,\n",
    "            blocks=blocks,\n",
    "            use_2d=use_2d,\n",
    "            shape=shape\n",
    "        )\n",
    "\n",
    "\n",
    "class Embedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size, max_position_length,\n",
    "                 trainable=True, name=None, initializer_range=0.02,\n",
    "                 dtype=None):\n",
    "        if dtype is None:\n",
    "            dtype = tf.float32\n",
    "        super().__init__(name=name, trainable=trainable, dtype=dtype)\n",
    "        self.word_embedding = None\n",
    "        self.position_embedding = None\n",
    "        self.initializer_range = initializer_range\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_position_length = max_position_length\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(\"..build\")\n",
    "        self.word_embedding = self.add_weight(\n",
    "            name=\"word_embedding\",\n",
    "            shape=(self.vocab_size, self.embedding_size),\n",
    "            initializer=tf.random_normal_initializer(stddev=self.initializer_range),\n",
    "        )\n",
    "        self.position_embedding = self.add_weight(\n",
    "            name=\"position_embedding\",\n",
    "            shape=(self.max_position_length, self.embedding_size),\n",
    "            initializer=tf.random_normal_initializer(stddev=self.initializer_range),\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, start=None):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: integer tensor of [batch_size, seq_length]\n",
    "        start: start of positional embedding\n",
    "\n",
    "        \"\"\"\n",
    "        shape = get_tensor_shape(inputs)\n",
    "        x = tf.gather(self.word_embedding, inputs)\n",
    "        if start is None:\n",
    "            start = 0\n",
    "        end = start + shape[1]\n",
    "        pe = self.position_embedding[start:end]\n",
    "        x = x + pe\n",
    "        print(f\".. Embedding output: {tldr(x)}\")        \n",
    "        return x\n",
    "\n",
    "    def __call__(self, inputs, start=None):\n",
    "        \"\"\"\n",
    "\n",
    "        if use_one_hot_keys is True, then inputs are one_hot tensors of shape [batch_size, seq_length, vocab_size],\n",
    "        else it is an integer tensor of [batch_size, seq_length] of token ids.\n",
    "        start: start of positional embedding\n",
    "\n",
    "        \"\"\"\n",
    "        return super().__call__(inputs=inputs, start=start)\n",
    "\n",
    "\n",
    "class GPT2(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, config, name=None, trainable=True, dtype=None):\n",
    "        super().__init__(name=name)\n",
    "        self.trainable = trainable\n",
    "        self.embedding = Embedding(\n",
    "            embedding_size=config['n_embd'],\n",
    "            vocab_size=config['n_vocab'],\n",
    "            max_position_length=config['n_ctx'],\n",
    "            name=\"embedding\",\n",
    "            dtype=dtype\n",
    "        )\n",
    "        self.transformer = Transformer(config, name=\"transformer\")\n",
    "\n",
    "    def call(self, inputs, cache=None,\n",
    "             dropout=None, attention_dropout=None,\n",
    "             return_cache=False, return_logits=True, use_2d=False):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: an integer tensor of shape [batch_size, seq_length] if not use_2d is False\n",
    "                else a one_hot tensor of shape [batch_size, seq_length, vocab_size]\n",
    "        cache: a list of dictionaries {\"key\": key, \"value\": value} of previous keys and values. it uses for generation\n",
    "        use_one_hot_keys: if True it uses one hot tensors for embedding layer.\n",
    "        return_cache: if True returns new keys and values alongside output. it uses for generation.\n",
    "        return_logits: if True, return logits, else return last layer embedding.\n",
    "        use_2d: for tpu performances: use 2D tensors for operations and return the output in 2D shape: [batch_size * seq_length, -1]\n",
    "\n",
    "        \"\"\"\n",
    "        if cache is not None:\n",
    "            _cache = cache[0][\"key\"]\n",
    "            start = get_tensor_shape(_cache)[2]\n",
    "        else:\n",
    "            start = None\n",
    "        x = self.embedding(inputs, start)\n",
    "        if use_2d:\n",
    "            shape = get_tensor_shape(x)\n",
    "            x = tf.reshape(x, [shape[0] * shape[1], shape[2]])\n",
    "            shape = shape[0:2]\n",
    "        else:\n",
    "            shape = None\n",
    "        x = self.transformer(\n",
    "            inputs=x,\n",
    "            cache=cache,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            return_cache=return_cache,\n",
    "            use_2d=use_2d,\n",
    "            shape=shape\n",
    "        )\n",
    "        # print(\"after transformer x.shape=\", x.shape)\n",
    "        # print(\"after transformer x=\", x[0][0][:3].numpy())\n",
    "        if return_cache:\n",
    "            x, cache = x\n",
    "        if return_logits:\n",
    "            shape = get_tensor_shape(x)\n",
    "            print(\"shape:\", shape)\n",
    "            if not use_2d:\n",
    "                x = tf.reshape(x, [shape[0] * shape[1], shape[2]])\n",
    "                print(\"x reshaped:\", x.shape)\n",
    "            logits = tf.matmul(x, self.embedding.word_embedding, transpose_b=True)\n",
    "            print(\"logits.shape=\", logits.shape)\n",
    "            if not use_2d:\n",
    "                logits = tf.reshape(logits, [shape[0], shape[1], self.embedding.vocab_size])\n",
    "                print(\"logits reshaped:\", tldr(logits))\n",
    "            result = logits\n",
    "        else:\n",
    "            result = x\n",
    "        if return_cache:\n",
    "            return result, cache\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def __call__(self, inputs, cache=None,\n",
    "                 dropout=None, attention_dropout=None,\n",
    "                 return_cache=False, return_logits=True,\n",
    "                 use_2d=False):\n",
    "        \"\"\"\n",
    "\n",
    "        inputs: an integer tensor of shape [batch_size, seq_length]\n",
    "        cache: a list of dictionaries {\"key\": key, \"value\": value} of previous keys and values. it uses for generation\n",
    "        use_one_hot_keys: if True it uses one hot tensors for embedding layer.\n",
    "        return_cache: if True returns new keys and values alongside output. it uses for generation.\n",
    "        return_logits: if True, return logits, else return last layer embedding.\n",
    "        use_2d: for tpu performances: use 2D tensors for operations and return the output in 2D shape: [batch_size * seq_length, -1]\n",
    "\n",
    "        \"\"\"\n",
    "        return super().__call__(\n",
    "            inputs=inputs,\n",
    "            cache=cache,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            return_cache=return_cache,\n",
    "            return_logits=return_logits,\n",
    "            use_2d=use_2d\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300e973-92d0-4804-9060-29bce7a7249c",
   "metadata": {},
   "source": [
    "### Test Embedding in isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38703681-d9e6-4ad8-a52e-e564bf87ffdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..build\n",
      ".. Embedding output: float32 (1, 1, 768) [-0.01077  0.0535  -0.03079]\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(embedding_size=768, vocab_size=50257, max_position_length=1024)\n",
    "_ = embedding_layer(tf.constant([[1]]))\n",
    "assert embedding_layer.built\n",
    "# embedding_layer.build(input_shape=(None,768))\n",
    "# embedding_layer.built\n",
    "# embedding_layer.word_embedding     = copy.deepcopy(params['wte']) # word_embedding: (50257, 768) self.vocab_size, self.embedding_size\n",
    "# embedding_layer.position_embedding = copy.deepcopy(params['wpe']) # position_embedding: (1024, 768) max_position_length, self.embedding_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df1e312-409c-416a-8d2c-b7443f0a57f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62cfb7bc-9e10-41c9-a87a-1b8271b4a748",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78275d0d-76be-43d6-8160-b96d633e87bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_layer.word_embedding     = copy.deepcopy(params['wte']) # word_embedding: (50257, 768) self.vocab_size, self.embedding_size\n",
    "embedding_layer.position_embedding = copy.deepcopy(params['wpe']) # position_embedding: (1024, 768) max_position_length, self.embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f44ba9e-a4ff-42c7-989d-9b0465ae17df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert embedding_layer.word_embedding.shape == (50257, 768)\n",
    "assert np.allclose(embedding_layer.word_embedding[0][:3], np.array([ -0.1101, -0.03927,  0.03311]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "713d1f0a-9d23-4d8a-9dce-8301f00eae36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Embedding output: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n"
     ]
    }
   ],
   "source": [
    "x_effort = tf.constant([[6109, 3626, 6100, 345]])\n",
    "x_effort_emb = embedding_layer(x_effort)\n",
    "assert x_effort_emb.shape == [1, 4, 768]\n",
    "assert np.allclose(x_effort_emb[0][0][:3], np.array([0.0793, -0.2979 ,  0.0882]), rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98f72660-ef8e-4002-82aa-a54b2874e5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config124M = {'n_embd': 768, 'n_vocab': 50257, 'n_ctx': 1024, 'n_layer': 12, 'n_head': 12}\n",
    "# # config = {'n_embd': 3, 'n_vocab': 10, 'n_ctx': 5, 'n_layer': 12, 'n_head': 4}\n",
    "gpt2 = GPT2(name=\"mygpt2\", config=config124M, trainable=False)\n",
    "# x=tf.constant([[1]])\n",
    "# gpt2.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3b889a6-3dd2-48a8-bd4a-106c5bb1dd58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gpt2.summary(expand_nested=True)\n",
    "# print_layer_structure(gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33fbdc7b-271d-4e09-b108-07c69fffa529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checkpoint = tf.train.Checkpoint(gpt2)\n",
    "# model_dir = 'tf_ckpts'\n",
    "# save_path = checkpoint.save(model_dir + \"/ckpt\")\n",
    "# # list all the variables in the model\n",
    "# tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "# for name, v in tf.train.list_variables(tf_ckpt_path):\n",
    "#     # print(name)\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acb0a9-1bcd-4ab1-93f8-1255a812ffcb",
   "metadata": {},
   "source": [
    "## Load weights into params dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7cb6561-3175-4f95-bf0b-34ecbf8258de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "settings = {\"n_layer\": 12}\n",
    "\n",
    "model_dir=\"ch05/01_main-chapter-code/gpt2/124M\"\n",
    "tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6d42da0-3ab4-4142-8cda-d46c4a9bdd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_w.shape: (768, 768)\n",
      "q_b.shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "# Let's study this \"params\" thing..\n",
    "params.keys() # ['blocks', 'b', 'g', 'wpe', 'wte']\n",
    "len(params[\"blocks\"]) # 12\n",
    "params_block0 = params[\"blocks\"][0]\n",
    "params_block0.keys() # ['attn', 'ln_1', 'ln_2', 'mlp']\n",
    "params_block0_attn = params_block0['attn']\n",
    "params_block0_attn.keys() # ['c_attn', 'c_proj']\n",
    "params_block0_attn_c_attn = params_block0_attn['c_attn']\n",
    "params_block0_attn_c_attn.keys() # ['b', 'w']\n",
    "x = (params[\"blocks\"][0][\"attn\"][\"c_attn\"])[\"w\"]\n",
    "x.shape # (768, 2304)\n",
    "q_w, k_w, v_w = np.split((params[\"blocks\"][0][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "q_b, k_b, v_b = np.split((params[\"blocks\"][0][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "print(\"q_w.shape:\", q_w.shape) # (768, 768)\n",
    "print(\"q_b.shape:\", q_b.shape) # (768,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8babea10-7191-4e0d-a266-a93cc13db58d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, 768)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params['wpe'].shape # (1024, 768) position embedding\n",
    "params['wte'].shape # (50257, 768) token embedding, out_head.weight\n",
    "#len(params['blocks']) 12\n",
    "#params['blocks'][0].keys() # dict_keys(['attn', 'ln_1', 'ln_2', 'mlp'])\n",
    "# params['b'].shape # (768,) final_norm.shift (beta)\n",
    "# params['g'].shape # (768,) final_norm.scale (gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8fd68f45-d155-4b8d-b6b4-3accd2f6e5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 0\n",
    "params[\"blocks\"][b][\"ln_1\"][\"b\"].shape # (768,) beta\n",
    "params[\"blocks\"][b][\"ln_1\"][\"g\"].shape  # (768,) gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47c611bc-bdfa-4cb6-a4a2-f602ae14b77d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'mygpt2',\n",
       " 'trainable': False,\n",
       " 'dtype': 'float32',\n",
       " 'config': {'n_embd': 768,\n",
       "  'n_vocab': 50257,\n",
       "  'n_ctx': 1024,\n",
       "  'n_layer': 12,\n",
       "  'n_head': 12}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b7c642-a108-4e24-a6c1-187c75101c1f",
   "metadata": {},
   "source": [
    "## Here we load all the weights!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f99fbf07-14c6-4dab-9996-2feaf1cc81f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..build\n",
      ".. Embedding output: float32 (1, 1, 768) [ 0.06137 -0.02885  0.03488]\n"
     ]
    }
   ],
   "source": [
    "# a GPT Model has an Embedding layer and a Transformer Model\n",
    "# embedding_layer   = gpt2.embedding\n",
    "\n",
    "# embedding_layer = Embedding(embedding_size=768, vocab_size=50257, max_position_length=1024)\n",
    "_ = gpt2.embedding(tf.constant([[1]]))\n",
    "assert gpt2.embedding.built\n",
    "# The Embedding Layer has word_embedding, position_embedding, initializer_range, embedding_size, vocab_size, max_position_length\n",
    "gpt2.embedding.word_embedding     = copy.deepcopy(params['wte']) # word_embedding: (50257, 768) self.vocab_size, self.embedding_size\n",
    "gpt2.embedding.position_embedding = copy.deepcopy(params['wpe']) # position_embedding: (1024, 768) max_position_length, self.embedding_size\n",
    "assert gpt2.embedding.word_embedding.shape == (50257, 768)\n",
    "assert np.allclose(gpt2.embedding.word_embedding[0][:3], np.array([ -0.1101, -0.03927,  0.03311]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "272123b7-a3fc-4d84-9fa0-d78f66d0e5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_768ones = np.ones((1, 768) , dtype=np.float32)\n",
    "_ = gpt2.transformer.blocks[0].attention.layer_norm(tf.constant(x_768ones))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e387f8c4-c9a0-4388-9c06-2f79d6565637",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.transformer.blocks[0].attention.layer_norm.built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7885541f-bde8-4f62-a3da-bfd5d08cc072",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..loading block 0\n",
      "..loading block 1\n",
      "..loading block 2\n",
      "..loading block 3\n",
      "..loading block 4\n",
      "..loading block 5\n",
      "..loading block 6\n",
      "..loading block 7\n",
      "..loading block 8\n",
      "..loading block 9\n",
      "..loading block 10\n",
      "..loading block 11\n"
     ]
    }
   ],
   "source": [
    "# transformer_layer = gpt2.transformer\n",
    "x_768ones = np.ones((1, 768) , dtype=np.float32)\n",
    "blocks = []\n",
    "for b in range(gpt2.get_config()['config']['n_layer']): # = transformer_layer.blocks_num (12)\n",
    "  # A transformer_layer has a list of blocks\n",
    "  print(f\"..loading block {b}\")\n",
    "  block = gpt2.transformer.blocks[b]\n",
    "  blocks.append(block)\n",
    "\n",
    "  _ = gpt2.transformer.blocks[b].attention.layer_norm(tf.constant(x_768ones))\n",
    "  assert gpt2.transformer.blocks[0].attention.layer_norm.built\n",
    "  gpt2.transformer.blocks[b].attention.layer_norm.beta = copy.deepcopy(params[\"blocks\"][b][\"ln_1\"][\"b\"]) # Not real sure about these..\n",
    "  gpt2.transformer.blocks[b].attention.layer_norm.gamma = copy.deepcopy(params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "\n",
    "  gpt2.transformer.blocks[b].attention.self_attention.query_layer.build((None, 768))\n",
    "  assert gpt2.transformer.blocks[b].attention.self_attention.query_layer.built\n",
    "  gpt2.transformer.blocks[b].attention.self_attention.key_layer.build((None, 768))\n",
    "  assert gpt2.transformer.blocks[b].attention.self_attention.key_layer.built\n",
    "  gpt2.transformer.blocks[b].attention.self_attention.value_layer.build((None, 768))\n",
    "  assert gpt2.transformer.blocks[b].attention.self_attention.value_layer.built\n",
    "  q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "  q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "  gpt2.transformer.blocks[b].attention.self_attention.query_layer.set_weights([q_w, q_b])\n",
    "  gpt2.transformer.blocks[b].attention.self_attention.key_layer.set_weights([k_w, k_b])\n",
    "  gpt2.transformer.blocks[b].attention.self_attention.value_layer.set_weights([v_w, v_b])\n",
    "\n",
    "  gpt2.transformer.blocks[b].attention.projection.build((None, 768))\n",
    "  assert gpt2.transformer.blocks[b].attention.projection.built\n",
    "  gpt2.transformer.blocks[b].attention.projection.set_weights([params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"], params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"]])\n",
    "\n",
    "  _ = gpt2.transformer.blocks[b].mlp.layer_norm(tf.constant(x_768ones))\n",
    "  assert gpt2.transformer.blocks[b].mlp.layer_norm.built\n",
    "  gpt2.transformer.blocks[b].mlp.layer_norm.beta = params[\"blocks\"][b][\"ln_2\"][\"b\"] # Not real sure about these..\n",
    "  gpt2.transformer.blocks[b].mlp.layer_norm.gamma = params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "\n",
    "    \n",
    "  gpt2.transformer.blocks[b].mlp.perceptron.build((None, 768))\n",
    "  assert gpt2.transformer.blocks[b].mlp.perceptron.built\n",
    "  gpt2.transformer.blocks[b].mlp.perceptron.set_weights([params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"], params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]])\n",
    "  gpt2.transformer.blocks[b].mlp.projection.build((None, 3072))\n",
    "  assert gpt2.transformer.blocks[b].mlp.projection.built\n",
    "  gpt2.transformer.blocks[b].mlp.projection.set_weights([params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"], params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"]])\n",
    "\n",
    "gpt2.transformer.layer_norm(tf.constant(x_768ones))\n",
    "assert gpt2.transformer.layer_norm.built\n",
    "gpt2.transformer.layer_norm.beta = copy.deepcopy(params[\"b\"])\n",
    "gpt2.transformer.layer_norm.gamma = copy.deepcopy(params[\"g\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017971c5-c758-4ba8-bd22-a4556469d585",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test each layer, comparing to corresponding LLMFS Pytorch layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d2aed8-01ac-4de5-9189-b05ee322ed86",
   "metadata": {},
   "source": [
    "### test embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ec62ac-9f2a-4547-ba93-efd786cdb7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Embedding output: float32 (1, 3, 768) [ 0.02152 -0.24603  0.05028]\n",
      ".. Embedding output: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n"
     ]
    }
   ],
   "source": [
    "assert np.allclose(gpt2.embedding.word_embedding[0][:3], np.array([ -0.1101, -0.03927,  0.03311]), rtol=1e-3, atol=1e-3)\n",
    "\n",
    "x_123 = tf.constant([[1, 2, 3]])\n",
    "x = gpt2.embedding(x_123)\n",
    "assert x.shape == [1, 3, 768]\n",
    "assert np.allclose(x[0][0][:3], np.array([ 0.02151961, -0.24603364,  0.05027542]), rtol=1e-3, atol=1e-3)\n",
    "\n",
    "x_effort = tf.constant([[6109, 3626, 6100, 345]])\n",
    "x_effort_emb = gpt2.embedding(x_effort)\n",
    "assert x_effort_emb.shape == [1, 4, 768]\n",
    "assert np.allclose(x_effort_emb[0][0][:3], np.array([0.0793, -0.2979 ,  0.0882]), rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17bf0df0-106e-4632-a8de-f4b8e7a73a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.allclose(gpt2.embedding.word_embedding[0][:3], np.array([ -0.1101, -0.03927,  0.03311]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21730fc-f12e-40e4-b22a-21949d07a7a6",
   "metadata": {},
   "source": [
    "### test layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81ffaed7-5622-4a33-8bf5-14ba5fe54567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b=0\n",
    "assert np.allclose(gpt2.transformer.blocks[b].attention.layer_norm.beta[:3], np.array([-3.67733e-03,  2.71967e-02, -6.40409e-02 ]), rtol=1e-4, atol=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd0d2720-95b0-448d-a1b5-821860928807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gpt2.transformer.blocks[b].attention.layer_norm.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d834231f-dedc-4db8-a74c-ad89d127f57d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test layer_norm\n",
    "x_768ones = np.ones((1, 768) , dtype=np.float32)\n",
    "\n",
    "layer_norm0 = blocks[0].attention.layer_norm\n",
    "# layer_norm0(x) # [-3.6773e-03,  2.7197e-02, -6.4041e-02\n",
    "# layer_norm11 = blocks[11].attention.layer_norm\n",
    "# layer_norm11(x) # [ 5.0957e-02,  5.3063e-03,  7.1952e-02\n",
    "x = layer_norm0(x_768ones)\n",
    "assert x.shape == [1, 768]\n",
    "assert np.allclose(x[0][:3], np.array([-3.67732509e-03,  2.71967370e-02, -6.40409067e-02]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6c619b8-dc62-4043-ab1b-bdacc37b29b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = gpt2.transformer.blocks[0].attention.layer_norm(x_768ones)\n",
    "assert x.shape == [1, 768]\n",
    "assert np.allclose(x[0][:3], np.array([-3.67732509e-03,  2.71967370e-02, -6.40409067e-02]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea26a991-cb06-4fe3-88d0-26ee118b6e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test query_layer, key_layer, value_layer, layer_proj\n",
    "x = tf.constant(np.ones((1, 768) , dtype=np.float32))\n",
    "# block 0\n",
    "query_layer0 = blocks[0].attention.self_attention.query_layer\n",
    "#query_layer0(x) # [-1.3708e+01,  1.3385e+01,  1.4323e+01\n",
    "key_layer0 = blocks[0].attention.self_attention.key_layer\n",
    "# key_layer0(x) [ 1.8049e-01, -1.4381e-01,  6.2964e-01\n",
    "value_layer0 = blocks[0].attention.self_attention.value_layer               \n",
    "# value_layer0(x) # [-6.1687e-02, -1.3786e-01, -3.0145e-01\n",
    "layer_proj0 = blocks[0].attention.projection\n",
    "# layer_proj0(x) # [-9.7561e+00, -1.7296e+01, -6.7800e-01\n",
    "# layer_proj0(value_layer0(key_layer0(query_layer0(x)))) # [-2.3273e+01, -7.9272e+02,  5.6245e+02\n",
    "\n",
    "# block 11\n",
    "query_layer11 = blocks[11].attention.self_attention.query_layer\n",
    "# query_layer11(x) # [-5.4209e+00,  4.6236e+00,  4.5401e+00\n",
    "key_layer11 = blocks[11].attention.self_attention.key_layer\n",
    "# key_layer11(x) # [ 5.8911e+00, -3.3184e-01,  6.3656e-01\n",
    "value_layer11 = blocks[11].attention.self_attention.value_layer               \n",
    "# value_layer11(x) # [-1.2480e+00, -3.0783e+00,  5.9679e+00\n",
    "layer_proj11 = blocks[11].attention.projection\n",
    "#layer_proj11(x) # [-4.1535e-01,  2.1763e+00,  4.7958e-01\n",
    "\n",
    "# layer_proj11(value_layer11(key_layer11(query_layer11(x)))) # [ 3.4414e+02,  4.9568e+02,  3.8639e+02\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a864e3a-c093-4693-ad7d-c967d9216efb",
   "metadata": {},
   "source": [
    "## Validate each layer to compare with LLMFS and gpt2s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3fba62-e4a8-43df-8026-11967ecbee13",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d2939bf-bb44-4d31-91a9-6cc88036e3ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Embedding output: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n"
     ]
    }
   ],
   "source": [
    "x_effort = tf.constant([[6109, 3626, 6100, 345]])\n",
    "\n",
    "x_effort_emb = gpt2.embedding(x_effort)\n",
    "assert x_effort_emb.shape == [1, 4, 768]\n",
    "assert x_effort_emb.shape == [1, 4, 768]\n",
    "assert np.allclose(x_effort_emb[0][0][:3], np.array([0.0793, -0.2979 ,  0.0882]), rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ed771-9a43-4e85-807c-2b85e7c770f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test AttentionLayer layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfa36b94-6d22-4669-951a-ebe74f95258c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b=0\n",
    "\n",
    "y = gpt2.transformer.blocks[b].attention.layer_norm(x_effort_emb)\n",
    "assert y.shape == [1, 4, 768]\n",
    "assert np.allclose(y[0][0][:3], np.array([0.047223  , -0.11664161, -0.02536647]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed59022-bdf7-4746-8ef4-0f284ca7c10e",
   "metadata": {},
   "source": [
    "#### Test AttentionLayer SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4050df4-4a25-47a4-b6a0-c0bca888ec91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.28434 -0.00881  0.34211]\n"
     ]
    }
   ],
   "source": [
    "b = 0\n",
    "self_attention = gpt2.transformer.blocks[b].attention.self_attention\n",
    "y = self_attention(x_effort_emb)\n",
    "assert y.shape == [1, 4, 768]\n",
    "assert np.allclose(y[0][0][:3], np.array([0.28434375, -0.00881347,  0.34210888]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d58c58-2969-4e84-8d51-5ac9ffb6231c",
   "metadata": {},
   "source": [
    "#### Test AttentionLayer projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84cc3749-eaae-4130-8b6d-dfdfc2ce37ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b=0\n",
    "# gpt2s.transformer.blocks[b].attention.projection.build((None, 768))\n",
    "# gpt2s.transformer.blocks[b].attention.projection.set_weights([params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"], params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"]])\n",
    "\n",
    "y = gpt2.transformer.blocks[b].attention.projection(x_effort_emb)\n",
    "assert y.shape == [1, 4, 768]\n",
    "assert np.allclose(y[0][0][:3], np.array([2.5602593 ,  0.34704542,  0.3729586]), rtol=1e-3, atol=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f77be9a-c364-4b0b-9164-17e31175326f",
   "metadata": {},
   "source": [
    "#### Test the entire AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0758b524-36ad-42bb-8ec9-8a47ee463e50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.09438  0.02312 -0.01932]\n"
     ]
    }
   ],
   "source": [
    "y = gpt2.transformer.blocks[b].attention.projection(self_attention(gpt2.transformer.blocks[b].attention.layer_norm(x_effort_emb)))\n",
    "assert y.shape == [1, 4, 768]\n",
    "assert np.allclose(y[0][0][:3], np.array([5.4214954e-01, -1.1554953e-01,  2.5736535e-01]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8b59475-23c4-4984-b095-e3684b6c1633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... input to    layer_norm: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 4, 768) [ 0.54215 -0.11555  0.25737]\n"
     ]
    }
   ],
   "source": [
    "b=0\n",
    "y = gpt2.transformer.blocks[b].attention(x_effort_emb)\n",
    "assert y.shape == [1, 4, 768]\n",
    "assert np.allclose(y[0][0][:3], np.array([5.4214954e-01, -1.1554953e-01,  2.5736535e-01]), rtol=1e-3, atol=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43316b6d-b890-4e9d-a963-484e39e168f6",
   "metadata": {},
   "source": [
    "#### Test MultiLayerPerceptron layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17b8c9f6-ccef-4c6a-afb2-a2be4151ca72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = gpt2.transformer.blocks[b].mlp.layer_norm(x_effort_emb)\n",
    "assert y.shape == [1, 4, 768]\n",
    "assert np.allclose(y[0][0][:3], np.array([0.0723421 , -0.1328541 ,  0.0565621]), rtol=1e-3, atol=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f19f4c-4cfe-4f21-9322-8fce774e9fcc",
   "metadata": {},
   "source": [
    "#### Test MultiLayerPerceptron perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bb7f532-a251-4de7-9ab0-cbaa53f1f4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b=0\n",
    "# x_effort_emb_3072 = tf.concat([tf.concat([x_effort_emb, x_effort_emb], axis=-1), tf.concat([x_effort_emb, x_effort_emb], axis=-1)], axis=-1)\n",
    "y = gpt2.transformer.blocks[b].mlp.perceptron(x_effort_emb)\n",
    "assert y.shape == [1, 4, 3072]\n",
    "assert np.allclose(y[0][0][:3], np.array([-1.26867130e-01, -5.54500334e-02, -1.59711763e-02]), rtol=1e-3, atol=1e-3)\n",
    "#y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb336266-bc3a-47da-a6a9-7787e5f3cda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test MultiLayerPerceptron projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bda24fd1-3de2-4d9a-806a-7b8636fcc5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b=0\n",
    "\n",
    "x_effort_emb_3072 = tf.concat([tf.concat([x_effort_emb, x_effort_emb], axis=-1), tf.concat([x_effort_emb, x_effort_emb], axis=-1)], axis=-1)\n",
    "y = gpt2.transformer.blocks[b].mlp.projection(x_effort_emb_3072)\n",
    "assert y.shape == [1, 4, 768]\n",
    "y\n",
    "assert np.allclose(y[0][0][:3], np.array([-0.8513732 ,  1.9853125 , -2.6772308]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931c440-08a0-4e2e-b4de-ed0380bb04a8",
   "metadata": {},
   "source": [
    "#### Test the entire layer MultiLayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff30bd29-6d3f-41f2-b19a-9aa0d2944c57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... input to    mlp: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from mlp: float32 (1, 4, 768) [3.2066  2.26237 1.45178]\n"
     ]
    }
   ],
   "source": [
    "b = 0\n",
    "y = gpt2.transformer.blocks[b].mlp(x_effort_emb)\n",
    "assert y.shape == [1, 4, 768]\n",
    "assert np.allclose(y[0][0][:3], np.array([3.2065992,   2.262373 ,   1.4517794]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f5f5c-fcb5-4dc2-bf6f-1d5a651fccfe",
   "metadata": {},
   "source": [
    "#### Test Transformer layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc1b867f-e1df-4acd-a452-285e7422191e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "y = gpt2.transformer.layer_norm(x_effort_emb)\n",
    "assert y.shape == [1, 4, 768]\n",
    "assert np.allclose(y[0][0][:3], np.array([0.3196596 , -1.050371  ,  0.4083333]), rtol=1e-3, atol=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db28f41-032a-4a26-b1f0-92cf15582041",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test the whole model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "768c0cba-1a33-461a-be7a-6d1526bd3a77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Embedding output: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 4, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 4, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 4, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 4, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 4, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 4, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 4, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 4, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 4, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 4, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 4, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 4, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 4, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 4, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 4, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 4, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 4, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.27819 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 4, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 4, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 4, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 4, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 4, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 4, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.14069 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 4, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 4, 768) [ 1.14149 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 4, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.13292 -0.58439  1.20297]\n",
      ".... output from mlp: float32 (1, 4, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 4, 768) [ 1.2002  -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 4, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 4, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 4, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 4, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 4, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.12762  0.10683 -0.23868]\n",
      ".. output from block 10: float32 (1, 4, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 4, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 4, 768) [ 0.47795  1.00912 -1.00978]\n",
      "shape: [1, 4, 768]\n",
      "x reshaped: (4, 768)\n",
      "logits.shape= (4, 50257)\n",
      "logits reshaped: float32 (1, 4, 50257) [-35.58201 -34.98038 -38.45218]\n"
     ]
    }
   ],
   "source": [
    "y = gpt2(x_effort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe556dc2-c1ae-4098-b091-5bf238c66996",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32 (1, 4, 50257) [-35.58201 -34.98038 -38.45218]'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tldr(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a7561eb-5017-409e-a839-2472735a7f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab822b3-bfc9-4366-af8c-22ebfa6206e1",
   "metadata": {},
   "source": [
    "## Here we are validating SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee63cc54-df18-4ff2-a1cc-ad7e51190e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_norm0 = blocks[0].attention.layer_norm\n",
    "\n",
    "# x = layer_norm0(x_123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "406b0450-1919-4d4a-bb31-57be722e9116",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[1, 2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4dbfbc0b-3d6c-46d5-8ad0-1df9c88ab217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention: inputs.shape= (1, 3, 768) tf.Tensor([ 0.02151961 -0.24603364  0.05027542], shape=(3,), dtype=float32)\n",
      "SelfAttention: query.shape= (1, 3, 768) tf.Tensor([-1.4599288  2.6167424  3.0795689], shape=(3,), dtype=float32)\n",
      "SelfAttention: reshape query.shape= (1, 12, 3, 64) tf.Tensor([-1.4599288  2.6167424  3.0795689], shape=(3,), dtype=float32)\n",
      "SelfAttention: reshape coefficients.shape= (1, 12, 3, 3) tf.Tensor([37.56014  20.718346 18.429935], shape=(3,), dtype=float32)\n",
      "SelfAttention: results coefficients.shape= (1, 12, 3, 64) tf.Tensor([0.37184218 0.04303181 0.43206325], shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 768), dtype=float32, numpy=\n",
       "array([[[ 0.37184218,  0.04303181,  0.43206325, ...,  0.21111687,\n",
       "         -0.346839  ,  1.209221  ],\n",
       "        [ 0.371842  ,  0.0430319 ,  0.4320631 , ...,  0.4688711 ,\n",
       "          0.6618678 ,  0.8360132 ],\n",
       "        [ 0.37184218,  0.04303183,  0.43206328, ...,  0.61793643,\n",
       "          0.34492874,  0.9636562 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 0\n",
    "self_attention = gpt2.transformer.blocks[b].attention.self_attention\n",
    "\n",
    "x = gpt2.embedding(x_123)\n",
    "\n",
    "self_attention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eb6f533-ac54-4a8d-8899-bead49f7e85e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 768), dtype=float32, numpy=\n",
       "array([[[ 2.9546063 ,  1.2646755 , -0.8381394 , ..., -0.03054229,\n",
       "         -0.21869148,  0.3622729 ],\n",
       "        [ 0.1328235 , -0.6140044 ,  1.6673647 , ...,  0.14359668,\n",
       "         -0.05004777, -0.01445162],\n",
       "        [-1.7971708 , -0.2732838 ,  1.5349535 , ...,  0.06378165,\n",
       "         -0.03412376,  0.03880764]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = gpt2.embedding(x_123)\n",
    "x = gpt2.transformer.blocks[b].attention.projection(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648b2a60-9540-425f-bdfa-05ece314b3db",
   "metadata": {},
   "source": [
    "### Here we are validating layer_norm in the AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e454bd28-2a77-4124-841c-992191726357",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 768), dtype=float32, numpy=\n",
       "array([[[ 0.00984592, -0.09290446, -0.04286208, ...,  0.01140726,\n",
       "          0.00143049,  0.03473373],\n",
       "        [-0.10779594,  0.02115111, -0.00442008, ...,  0.09210561,\n",
       "         -0.11231511, -0.09120307],\n",
       "        [-0.09531845, -0.30332777,  0.12632102, ...,  0.16344029,\n",
       "          0.03355676, -0.19331908]]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = gpt2.embedding(x_123)\n",
    "gpt2.transformer.blocks[b].attention.layer_norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df01536-a852-4148-aec2-5363bb742cc2",
   "metadata": {},
   "source": [
    "#### test the entire Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb071eb9-54cc-4351-96c7-02445ac96f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5e50e40-5f55-4781-be41-ccb01337dc90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention: inputs.shape= (1, 3, 768) tf.Tensor([ 0.00984592 -0.09290446 -0.04286208], shape=(3,), dtype=float32)\n",
      "SelfAttention: query.shape= (1, 3, 768) tf.Tensor([-0.04680172 -0.4413777   0.29562825], shape=(3,), dtype=float32)\n",
      "SelfAttention: reshape query.shape= (1, 12, 3, 64) tf.Tensor([-0.04680172 -0.4413777   0.29562825], shape=(3,), dtype=float32)\n",
      "SelfAttention: reshape coefficients.shape= (1, 12, 3, 3) tf.Tensor([-0.6131798 -1.2038919 -1.8434243], shape=(3,), dtype=float32)\n",
      "SelfAttention: results coefficients.shape= (1, 12, 3, 64) tf.Tensor([0.14607082 0.05066063 0.02101883], shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 768), dtype=float32, numpy=\n",
       "array([[[ 3.3282498e-01,  8.7829292e-02,  1.8114735e-01, ...,\n",
       "         -3.9580278e-02, -8.0668032e-03,  3.5296157e-02],\n",
       "        [ 6.3773024e-01, -8.8449240e-02, -1.6967435e-01, ...,\n",
       "         -4.4448502e-02,  5.2359477e-03,  5.8977678e-04],\n",
       "        [-4.9767217e-01,  1.8594694e-01,  1.2540229e-01, ...,\n",
       "         -2.5778674e-02, -7.1540968e-03,  4.3813903e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = gpt2.transformer.blocks[b].attention\n",
    "x = gpt2.embedding(x_123)\n",
    "\n",
    "attention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "039502a2-b4ec-4b40-b269-21c7fb195d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test mlp_layer_norm, mlp_perceptron, mlp_projection\n",
    "x = np.ones((1, 768) , dtype=np.float32)\n",
    "mlp_layer_norm0 = blocks[0].mlp.layer_norm\n",
    "# mlp_layer_norm0(x) # [ 4.2478e-02,  3.2627e-02,  4.4881e-03\n",
    "\n",
    "mlp_perceptron0 = blocks[0].mlp.perceptron\n",
    "mlp_projection0 = blocks[0].mlp.projection\n",
    "# mlp_projection0(mlp_perceptron0(x)) # [-1.6735e+01, -6.9883e+00,  4.1138e+00\n",
    "\n",
    "mlp_layer_norm11 = blocks[11].mlp.layer_norm\n",
    "# mlp_layer_norm11(x) # [-1.9770e-03,  2.0055e-02,  3.8334e-02\n",
    "\n",
    "mlp_perceptron11 = blocks[11].mlp.perceptron\n",
    "mlp_projection11 = blocks[11].mlp.projection\n",
    "#mlp_projection11(mlp_perceptron11(x)) # [ 1.3675e+01,  2.2839e+01, -1.7306e+01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95267c35-3b8e-4ce3-a9bc-0384aa33e3da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 x: [ 0.02151961 -0.24603364  0.05027542]\n",
      "2 x: [-0.05177143  0.12072419 -0.42840737]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 50257), dtype=float32, numpy=\n",
       "array([[[-32.90102 , -31.202356, -34.662197, ..., -39.48669 ,\n",
       "         -39.873096, -32.23865 ],\n",
       "        [-55.520763, -53.42853 , -56.476704, ..., -68.153885,\n",
       "         -66.77085 , -58.60061 ],\n",
       "        [-61.79686 , -60.538612, -59.550335, ..., -75.32061 ,\n",
       "         -72.773125, -65.57064 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trivial = tf.constant([[1, 2, 3]])\n",
    "gpt2(x_trivial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a4729d1-5cc8-4532-af94-777a358573b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 768), dtype=float32, numpy=\n",
       "array([[[ 0.02151961, -0.24603364,  0.05027542, ...,  0.04301079,\n",
       "          0.03080702,  0.09767969],\n",
       "        [-0.10350236, -0.00585408,  0.0892228 , ...,  0.12408535,\n",
       "         -0.11955193, -0.08801492],\n",
       "        [-0.08849797, -0.39009592,  0.26571876, ...,  0.19665493,\n",
       "          0.055693  , -0.2011495 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_emb = gpt2.word_emb\n",
    "# pos_emb = gpt2.pos_emb\n",
    "# we = word_emb(x_trivial) # .shape # TensorShape([1, 3, 768])\n",
    "# pe = pos_emb(tf.range(1024)) # .shape # TensorShape([1024, 768])\n",
    "# x = we + pe\n",
    "emb = gpt2.embedding\n",
    "emb(x_trivial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46f31bd9-8a69-4dce-824a-b96a5c17d171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    idx = tf.cast(idx, dtype=tf.int64)\n",
    "    for i in range(max_new_tokens):\n",
    "        idx_cond = idx\n",
    "        # print(\"i=\", i, \"idx_cond=\", idx_cond)\n",
    "        logits = model(idx)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = tf.nn.softmax(logits, -1)\n",
    "        idx_next = tf.argmax(logits)\n",
    "        idx_next = tf.argmax(probas, -1)        \n",
    "        # print(\"idx_next:\", idx_next)\n",
    "        idx_next_expanded = tf.expand_dims(idx_next, axis=0)\n",
    "        # print(\"idx_next_expanded:\", idx_next_expanded)\n",
    "        idx = tf.concat((idx, idx_next_expanded), axis=1)\n",
    "        # print(\"idx_next_expanded after concat:\", idx_next_expanded)\n",
    "        # print(\"idx               after concat:\", idx)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43d2896f-e9d5-4bc8-b70d-d849ce274eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Embedding output: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 4, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 4, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 4, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 4, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 4, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 4, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 4, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 4, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 4, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 4, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 4, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 4, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 4, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 4, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 4, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 4, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 4, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.27819 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 4, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 4, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 4, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 4, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 4, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 4, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.14069 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 4, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 4, 768) [ 1.14149 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 4, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.13292 -0.58439  1.20297]\n",
      ".... output from mlp: float32 (1, 4, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 4, 768) [ 1.2002  -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 4, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 4, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 4, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 4, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 4, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.12762  0.10683 -0.23868]\n",
      ".. output from block 10: float32 (1, 4, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 4, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 4, 768) [ 0.47795  1.00912 -1.00978]\n",
      "shape: [1, 4, 768]\n",
      "x reshaped: (4, 768)\n",
      "logits.shape= (4, 50257)\n",
      "logits reshaped: (1, 4, 50257)\n",
      ".. Embedding output: float32 (1, 5, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 5, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 5, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 5, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 5, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 5, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 5, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 5, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 5, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 5, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 5, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 5, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 5, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 5, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 5, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 5, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 5, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 5, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 5, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.27819 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 5, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 5, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 5, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 5, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 5, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 5, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.14069 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 5, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 5, 768) [ 1.14149 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 5, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.13292 -0.58439  1.20297]\n",
      ".... output from mlp: float32 (1, 5, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 5, 768) [ 1.2002  -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 5, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 5, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 5, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 5, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 5, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.12762  0.10683 -0.23868]\n",
      ".. output from block 10: float32 (1, 5, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 5, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 5, 768) [ 0.47795  1.00912 -1.00978]\n",
      "shape: [1, 5, 768]\n",
      "x reshaped: (5, 768)\n",
      "logits.shape= (5, 50257)\n",
      "logits reshaped: (1, 5, 50257)\n",
      ".. Embedding output: float32 (1, 6, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 6, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 6, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 6, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 6, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 6, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 6, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 6, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 6, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 6, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 6, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 6, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 6, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 6, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 6, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 6, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 6, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 6, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 6, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.27819 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 6, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 6, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 6, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 6, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 6, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 6, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.14069 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 6, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 6, 768) [ 1.14149 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 6, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.13292 -0.58439  1.20297]\n",
      ".... output from mlp: float32 (1, 6, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 6, 768) [ 1.2002  -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 6, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 6, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 6, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 6, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 6, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.12762  0.10683 -0.23868]\n",
      ".. output from block 10: float32 (1, 6, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 6, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 6, 768) [ 0.47795  1.00912 -1.00978]\n",
      "shape: [1, 6, 768]\n",
      "x reshaped: (6, 768)\n",
      "logits.shape= (6, 50257)\n",
      "logits reshaped: (1, 6, 50257)\n",
      ".. Embedding output: float32 (1, 7, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 7, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 7, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 7, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 7, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 7, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 7, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 7, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 7, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 7, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 7, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 7, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 7, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 7, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 7, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 7, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 7, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 7, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 7, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.27819 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 7, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 7, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 7, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 7, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 7, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 7, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.14069 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 7, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 7, 768) [ 1.14149 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 7, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.13292 -0.58439  1.20297]\n",
      ".... output from mlp: float32 (1, 7, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 7, 768) [ 1.2002  -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 7, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 7, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 7, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 7, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 7, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.12762  0.10683 -0.23868]\n",
      ".. output from block 10: float32 (1, 7, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 7, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 7, 768) [ 0.47795  1.00912 -1.00978]\n",
      "shape: [1, 7, 768]\n",
      "x reshaped: (7, 768)\n",
      "logits.shape= (7, 50257)\n",
      "logits reshaped: (1, 7, 50257)\n",
      ".. Embedding output: float32 (1, 8, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 8, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 8, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 8, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 8, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 8, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 8, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 8, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 8, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 8, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 8, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 8, 768) [ 1.42395 -0.49186  1.25903]\n",
      "\n",
      ".. input to    block 3: float32 (1, 8, 768) [ 1.42395 -0.49186  1.25903]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.42395 -0.49186  1.25903]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 8, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 8, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 8, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 8, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 8, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 8, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.27819 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 8, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 8, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 8, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 8, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 8, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 8, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.14069 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 8, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 8, 768) [ 1.14149 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 8, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.13292 -0.58438  1.20297]\n",
      ".... output from mlp: float32 (1, 8, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 8, 768) [ 1.2002  -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 8, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 8, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 8, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 8, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 8, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.09859 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.12762  0.10683 -0.23868]\n",
      ".. output from block 10: float32 (1, 8, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 8, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 8, 768) [ 0.47794  1.00912 -1.00978]\n",
      "shape: [1, 8, 768]\n",
      "x reshaped: (8, 768)\n",
      "logits.shape= (8, 50257)\n",
      "logits reshaped: (1, 8, 50257)\n",
      "token_ids: tf.Tensor([[6109 3626 6100  345 2651   13  198  198  464]], shape=(1, 9), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "idx = tf.constant([[6109 , 3626, 6100,  345]])\n",
    "token_ids = generate_text_simple(gpt2, idx=idx, max_new_tokens=5, context_size=256)\n",
    "print(\"token_ids:\", token_ids) # [6109, 3626, 6100,  345, 2651,   13,  198,  198,  464]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd5ef5b7-2092-4cdf-a24e-adb7535bb5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[6109 3626 6100  345]], shape=(1, 4), dtype=int32)\n",
      "1 x: [ 0.07927368 -0.2979193   0.08817437]\n",
      "2 x: [-0.00448901  0.08728749 -0.2910273 ]\n",
      "logits:\n",
      " tf.Tensor(\n",
      "[[[ -35.58201   -34.980377  -38.45218  ...  -42.09592   -41.85323\n",
      "    -35.59654 ]\n",
      "  [ -76.96015   -76.69698   -81.930885 ...  -88.798386  -86.76316\n",
      "    -78.962685]\n",
      "  [-125.34872  -126.2704   -135.09477  ... -132.31728  -135.25441\n",
      "   -127.65115 ]\n",
      "  [-136.6002   -137.38036  -146.5556   ... -148.2978   -147.21553\n",
      "   -139.56772 ]]], shape=(1, 4, 50257), dtype=float32)\n",
      "logits:\n",
      " tf.Tensor([[-136.6002  -137.38036 -146.5556  ... -148.2978  -147.21553 -139.56772]], shape=(1, 50257), dtype=float32)\n",
      "logits[-1]:\n",
      " tf.Tensor([-136.6002  -137.38036 -146.5556  ... -148.2978  -147.21553 -139.56772], shape=(50257,), dtype=float32)\n",
      "probas:\n",
      " tf.Tensor(\n",
      "[[1.6012554e-03 7.3391397e-04 7.6012654e-08 ... 1.3312417e-08\n",
      "  3.9290100e-08 8.2354178e-05]], shape=(1, 50257), dtype=float32)\n",
      "idx_next:\n",
      " tf.Tensor([2651], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "idx = tf.constant([[6109 , 3626, 6100,  345]])\n",
    "print(idx)\n",
    "# print(idx[:, -1, :])\n",
    "# generate(gpt2, idx=idx, max_new_tokens=3, context_size=256)\n",
    "logits = gpt2(idx)\n",
    "print(\"logits:\\n\", logits)\n",
    "logits = logits[:, -1, :]\n",
    "print(\"logits:\\n\", logits)\n",
    "print(\"logits[-1]:\\n\", logits[-1])\n",
    "probas = tf.nn.softmax(logits, -1)\n",
    "print(\"probas:\\n\", probas)\n",
    "idx_next = tf.argmax(probas, -1)\n",
    "print(\"idx_next:\\n\", idx_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8eaf98b0-74ca-4d40-8216-a2405a6a0eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids: tf.Tensor([[6109 3626 6100  345]], shape=(1, 4), dtype=int32)\n",
      "Every effort moves you\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    # encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    encoded_tensor = tf.constant(encoded) #.unsqueeze(0) # add batch dimension\n",
    "    encoded_tensor = tf.expand_dims(encoded_tensor, axis=0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    # flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    # return tokenizer.decode(flat.tolist())\n",
    "    return tokenizer.decode(token_ids[-1])\n",
    "\n",
    "token_ids = text_to_token_ids(\"Every effort moves you\", tokenizer)\n",
    "print(\"token_ids:\", token_ids)\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65e6f95d-43cc-45ab-b3f5-4a53456a72aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. Embedding output: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 4, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 4, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 4, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 4, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 4, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 4, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 4, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 4, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 4, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 4, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 4, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 4, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 4, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 4, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 4, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 4, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 4, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.27819 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 4, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 4, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 4, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 4, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 4, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 4, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.14069 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 4, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 4, 768) [ 1.14149 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 4, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.13292 -0.58439  1.20297]\n",
      ".... output from mlp: float32 (1, 4, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 4, 768) [ 1.2002  -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 4, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 4, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 4, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 4, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 4, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.12762  0.10683 -0.23868]\n",
      ".. output from block 10: float32 (1, 4, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 4, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 4, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 4, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 4, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 4, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 4, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 4, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 4, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 4, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 4, 768) [ 0.47795  1.00912 -1.00978]\n",
      "shape: [1, 4, 768]\n",
      "x reshaped: (4, 768)\n",
      "logits.shape= (4, 50257)\n",
      "logits reshaped: (1, 4, 50257)\n",
      ".. Embedding output: float32 (1, 5, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 5, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 5, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 5, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 5, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 5, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 5, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 5, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 5, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 5, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 5, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 5, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 5, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 5, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 5, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 5, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 5, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 5, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 5, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.27819 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 5, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 5, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 5, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 5, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 5, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 5, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.14069 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 5, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 5, 768) [ 1.14149 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 5, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.13292 -0.58439  1.20297]\n",
      ".... output from mlp: float32 (1, 5, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 5, 768) [ 1.2002  -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 5, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 5, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 5, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 5, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 5, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.12762  0.10683 -0.23868]\n",
      ".. output from block 10: float32 (1, 5, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 5, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 5, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 5, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 5, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 5, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 5, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 5, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 5, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 5, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 5, 768) [ 0.47795  1.00912 -1.00978]\n",
      "shape: [1, 5, 768]\n",
      "x reshaped: (5, 768)\n",
      "logits.shape= (5, 50257)\n",
      "logits reshaped: (1, 5, 50257)\n",
      ".. Embedding output: float32 (1, 6, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 6, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 6, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 6, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 6, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 6, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 6, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 6, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 6, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 6, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 6, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 6, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 6, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 6, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 6, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 6, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 6, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 6, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 6, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.27819 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 6, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 6, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 6, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 6, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 6, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 6, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.14069 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 6, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 6, 768) [ 1.14149 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 6, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.13292 -0.58439  1.20297]\n",
      ".... output from mlp: float32 (1, 6, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 6, 768) [ 1.2002  -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 6, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 6, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 6, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 6, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 6, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.12762  0.10683 -0.23868]\n",
      ".. output from block 10: float32 (1, 6, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 6, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 6, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 6, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 6, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 6, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 6, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 6, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 6, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 6, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 6, 768) [ 0.47795  1.00912 -1.00978]\n",
      "shape: [1, 6, 768]\n",
      "x reshaped: (6, 768)\n",
      "logits.shape= (6, 50257)\n",
      "logits reshaped: (1, 6, 50257)\n",
      ".. Embedding output: float32 (1, 7, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 7, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 7, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 7, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 7, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 7, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 7, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 7, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 7, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 7, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 7, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 7, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 7, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 7, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 7, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 7, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 7, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 7, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 7, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.27819 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 7, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 7, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 7, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 7, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 7, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 7, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.14069 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 7, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 7, 768) [ 1.14149 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 7, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.13292 -0.58439  1.20297]\n",
      ".... output from mlp: float32 (1, 7, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 7, 768) [ 1.2002  -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 7, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 7, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 7, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 7, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 7, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.12762  0.10683 -0.23868]\n",
      ".. output from block 10: float32 (1, 7, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 7, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 7, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 7, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 7, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 7, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 7, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 7, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 7, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 7, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 7, 768) [ 0.47795  1.00912 -1.00978]\n",
      "shape: [1, 7, 768]\n",
      "x reshaped: (7, 768)\n",
      "logits.shape= (7, 50257)\n",
      "logits reshaped: (1, 7, 50257)\n",
      ".. Embedding output: float32 (1, 8, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 8, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 8, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 8, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 8, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 8, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 8, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 8, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 8, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 8, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 8, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 8, 768) [ 1.42395 -0.49186  1.25903]\n",
      "\n",
      ".. input to    block 3: float32 (1, 8, 768) [ 1.42395 -0.49186  1.25903]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.42395 -0.49186  1.25903]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 8, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 8, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 8, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 8, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 8, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 8, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.27819 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 8, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 8, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 8, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 8, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 8, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 8, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.14069 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 8, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 8, 768) [ 1.14149 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 8, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.13292 -0.58438  1.20297]\n",
      ".... output from mlp: float32 (1, 8, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 8, 768) [ 1.2002  -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 8, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 8, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 8, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 8, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 8, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 1.09859 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.12762  0.10683 -0.23868]\n",
      ".. output from block 10: float32 (1, 8, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 8, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 8, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 8, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 8, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 8, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 8, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 8, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 8, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 8, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 8, 768) [ 0.47794  1.00912 -1.00978]\n",
      "shape: [1, 8, 768]\n",
      "x reshaped: (8, 768)\n",
      "logits.shape= (8, 50257)\n",
      "logits reshaped: (1, 8, 50257)\n",
      ".. Embedding output: float32 (1, 9, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 9, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 9, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 9, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 9, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 9, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 9, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 9, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 9, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 9, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 9, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 9, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 9, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 9, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 9, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 9, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 9, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 9, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 9, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 9, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 9, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 9, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 9, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 9, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 9, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 9, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 9, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 9, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 9, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 9, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 9, 768) [ 1.42395 -0.49186  1.25903]\n",
      "\n",
      ".. input to    block 3: float32 (1, 9, 768) [ 1.42395 -0.49186  1.25903]\n",
      ".... input to    layer_norm: float32 (1, 9, 768) [ 1.42395 -0.49186  1.25903]\n",
      ".... output from layer_norm: float32 (1, 9, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 9, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 9, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 9, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 9, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 9, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 9, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 9, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 9, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 9, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 9, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 9, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 9, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 9, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 9, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 9, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 9, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 9, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 9, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 9, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 9, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 9, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 9, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 9, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 9, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 9, 768) [ 1.27819 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 9, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 9, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 9, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 9, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 9, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 9, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 9, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 9, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 9, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 9, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 9, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 9, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 9, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 9, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 9, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 9, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 9, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 9, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 9, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 9, 768) [ 1.14069 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 9, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 9, 768) [ 1.14149 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 9, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 9, 768) [ 1.14149 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 9, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 9, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 9, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 9, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 9, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 9, 768) [ 1.13292 -0.58438  1.20297]\n",
      ".... output from mlp: float32 (1, 9, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 9, 768) [ 1.2002  -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 9, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 9, 768) [ 1.2002  -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 9, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 9, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 9, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 9, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 9, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 9, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 9, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 9, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 9, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 9, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 9, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 9, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 9, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 9, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 9, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 9, 768) [ 1.09859 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 9, 768) [-0.12762  0.10683 -0.23868]\n",
      ".. output from block 10: float32 (1, 9, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 9, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 9, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 9, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 9, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 9, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 9, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 9, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 9, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 9, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 9, 768) [ 0.47794  1.00912 -1.00978]\n",
      "shape: [1, 9, 768]\n",
      "x reshaped: (9, 768)\n",
      "logits.shape= (9, 50257)\n",
      "logits reshaped: (1, 9, 50257)\n",
      ".. Embedding output: float32 (1, 10, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 10, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 10, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 10, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 10, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 10, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 10, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 10, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 10, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 10, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 10, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 10, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 10, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 10, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 10, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 10, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 10, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 10, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 10, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 10, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 10, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 10, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 10, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 10, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 10, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 10, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 10, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 10, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 10, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 10, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 10, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 10, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 10, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 10, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 10, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 10, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 10, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 10, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 10, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 10, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 10, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 10, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 10, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 10, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 10, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 10, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 10, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 10, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 10, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 10, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 10, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 10, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 10, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 10, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 10, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 10, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 10, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 10, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 10, 768) [ 1.27818 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 10, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 10, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 10, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 10, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 10, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 10, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 10, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 10, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 10, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 10, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 10, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 10, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 10, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 10, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 10, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 10, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 10, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 10, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 10, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 10, 768) [ 1.14068 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 10, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 10, 768) [ 1.14148 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 10, 768) [ 1.14148 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 10, 768) [ 1.14148 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 10, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 10, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 10, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 10, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 10, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 10, 768) [ 1.13291 -0.58438  1.20297]\n",
      ".... output from mlp: float32 (1, 10, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 10, 768) [ 1.20019 -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 10, 768) [ 1.20019 -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 10, 768) [ 1.20019 -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 10, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 10, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 10, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 10, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 10, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 10, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 10, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 10, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 10, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 10, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 10, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 10, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 10, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 10, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 10, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 10, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 10, 768) [-0.12762  0.10682 -0.23868]\n",
      ".. output from block 10: float32 (1, 10, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 10, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 10, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 10, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 10, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 10, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 10, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 10, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 10, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 10, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 10, 768) [ 0.47794  1.00912 -1.00978]\n",
      "shape: [1, 10, 768]\n",
      "x reshaped: (10, 768)\n",
      "logits.shape= (10, 50257)\n",
      "logits reshaped: (1, 10, 50257)\n",
      ".. Embedding output: float32 (1, 11, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 11, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 11, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 11, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 11, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 11, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 11, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 11, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 11, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 11, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 11, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 11, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 11, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 11, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 11, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 11, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 11, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 11, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 11, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 11, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 11, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 11, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 11, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 11, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 11, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 11, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 11, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 11, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 11, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 11, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 11, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 11, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 11, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 11, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 11, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 11, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 11, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 11, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 11, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 11, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 11, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 11, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 11, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 11, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 11, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 11, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 11, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 11, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 11, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 11, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 11, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 11, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 11, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 11, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 11, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 11, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 11, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 11, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 11, 768) [ 1.27818 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 11, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 11, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 11, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 11, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 11, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 11, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 11, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 11, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 11, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 11, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 11, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 11, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 11, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 11, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 11, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 11, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 11, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 11, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 11, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 11, 768) [ 1.14068 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 11, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 11, 768) [ 1.14148 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 11, 768) [ 1.14148 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 11, 768) [ 1.14148 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 11, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 11, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 11, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 11, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 11, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 11, 768) [ 1.13291 -0.58438  1.20297]\n",
      ".... output from mlp: float32 (1, 11, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 11, 768) [ 1.20019 -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 11, 768) [ 1.20019 -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 11, 768) [ 1.20019 -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 11, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 11, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 11, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 11, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 11, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 11, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 11, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 11, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 11, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 11, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 11, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 11, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 11, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 11, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 11, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 11, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 11, 768) [-0.12762  0.10682 -0.23868]\n",
      ".. output from block 10: float32 (1, 11, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 11, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 11, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 11, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 11, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 11, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 11, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 11, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 11, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 11, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 11, 768) [ 0.47794  1.00912 -1.00978]\n",
      "shape: [1, 11, 768]\n",
      "x reshaped: (11, 768)\n",
      "logits.shape= (11, 50257)\n",
      "logits reshaped: (1, 11, 50257)\n",
      ".. Embedding output: float32 (1, 12, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 12, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 12, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 12, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 12, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 12, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 12, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 12, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 12, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 12, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 12, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 12, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 12, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 12, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 12, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 12, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 12, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 12, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 12, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 12, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 12, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 12, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 12, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 12, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 12, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 12, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 12, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 12, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 12, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 12, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 12, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 12, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 12, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 12, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 12, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 12, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 12, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 12, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 12, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 12, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 12, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 12, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 12, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 12, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 12, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 12, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 12, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 12, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 12, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 12, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 12, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 12, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 12, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 12, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 12, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 12, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 12, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 12, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 12, 768) [ 1.27818 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 12, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 12, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 12, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 12, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 12, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 12, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 12, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 12, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 12, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 12, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 12, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 12, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 12, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 12, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 12, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 12, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 12, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 12, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 12, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 12, 768) [ 1.14068 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 12, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 12, 768) [ 1.14148 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 12, 768) [ 1.14148 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 12, 768) [ 1.14148 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 12, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 12, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 12, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 12, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 12, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 12, 768) [ 1.13291 -0.58438  1.20297]\n",
      ".... output from mlp: float32 (1, 12, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 12, 768) [ 1.20019 -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 12, 768) [ 1.20019 -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 12, 768) [ 1.20019 -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 12, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 12, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 12, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 12, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 12, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 12, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 12, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 12, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 12, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 12, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 12, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 12, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 12, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 12, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 12, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 12, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 12, 768) [-0.12762  0.10682 -0.23868]\n",
      ".. output from block 10: float32 (1, 12, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 12, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 12, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 12, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 12, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 12, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 12, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 12, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 12, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 12, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 12, 768) [ 0.47794  1.00912 -1.00978]\n",
      "shape: [1, 12, 768]\n",
      "x reshaped: (12, 768)\n",
      "logits.shape= (12, 50257)\n",
      "logits reshaped: (1, 12, 50257)\n",
      ".. Embedding output: float32 (1, 13, 768) [ 0.07927 -0.29792  0.08817]\n",
      "\n",
      ".. input to    block 0: float32 (1, 13, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... input to    layer_norm: float32 (1, 13, 768) [ 0.07927 -0.29792  0.08817]\n",
      ".... output from layer_norm: float32 (1, 13, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... input to    SelfAttention: float32 (1, 13, 768) [ 0.04722 -0.11664 -0.02537]\n",
      ".... output from SelfAttention: float32 (1, 13, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... input to    projection: float32 (1, 13, 768) [ 0.09438  0.02312 -0.01932]\n",
      ".... output from projection: float32 (1, 13, 768) [ 0.54215 -0.11555  0.25737]\n",
      ".... input to    mlp: float32 (1, 13, 768) [ 0.62142 -0.41347  0.34554]\n",
      ".... output from mlp: float32 (1, 13, 768) [0.76306 0.73464 0.57444]\n",
      ".. output from block 0: float32 (1, 13, 768) [1.38448 0.32117 0.91998]\n",
      "\n",
      ".. input to    block 1: float32 (1, 13, 768) [1.38448 0.32117 0.91998]\n",
      ".... input to    layer_norm: float32 (1, 13, 768) [1.38448 0.32117 0.91998]\n",
      ".... output from layer_norm: float32 (1, 13, 768) [0.06055 0.00679 0.03987]\n",
      ".... input to    SelfAttention: float32 (1, 13, 768) [0.06055 0.00679 0.03987]\n",
      ".... output from SelfAttention: float32 (1, 13, 768) [0.14954 0.51014 0.14025]\n",
      ".... input to    projection: float32 (1, 13, 768) [0.14954 0.51014 0.14025]\n",
      ".... output from projection: float32 (1, 13, 768) [ 0.47291 -0.78425  0.58976]\n",
      ".... input to    mlp: float32 (1, 13, 768) [ 1.85739 -0.46308  1.50974]\n",
      ".... output from mlp: float32 (1, 13, 768) [-0.43655  0.24762  0.02813]\n",
      ".. output from block 1: float32 (1, 13, 768) [ 1.42084 -0.21546  1.53787]\n",
      "\n",
      ".. input to    block 2: float32 (1, 13, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... input to    layer_norm: float32 (1, 13, 768) [ 1.42084 -0.21546  1.53787]\n",
      ".... output from layer_norm: float32 (1, 13, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... input to    SelfAttention: float32 (1, 13, 768) [ 0.00962 -0.0118   0.009  ]\n",
      ".... output from SelfAttention: float32 (1, 13, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... input to    projection: float32 (1, 13, 768) [ 0.00988  0.01906 -0.16104]\n",
      ".... output from projection: float32 (1, 13, 768) [ 0.03123 -0.15645 -0.01772]\n",
      ".... input to    mlp: float32 (1, 13, 768) [ 1.45207 -0.37192  1.52015]\n",
      ".... output from mlp: float32 (1, 13, 768) [-0.02812 -0.11995 -0.26112]\n",
      ".. output from block 2: float32 (1, 13, 768) [ 1.42395 -0.49186  1.25904]\n",
      "\n",
      ".. input to    block 3: float32 (1, 13, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... input to    layer_norm: float32 (1, 13, 768) [ 1.42395 -0.49186  1.25904]\n",
      ".... output from layer_norm: float32 (1, 13, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... input to    SelfAttention: float32 (1, 13, 768) [ 0.01607 -0.01085  0.00152]\n",
      ".... output from SelfAttention: float32 (1, 13, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... input to    projection: float32 (1, 13, 768) [ 0.0341   0.06416 -0.00552]\n",
      ".... output from projection: float32 (1, 13, 768) [0.06516 0.01836 0.11782]\n",
      ".... input to    mlp: float32 (1, 13, 768) [ 1.48911 -0.4735   1.37686]\n",
      ".... output from mlp: float32 (1, 13, 768) [-0.03184 -0.03519 -0.04828]\n",
      ".. output from block 3: float32 (1, 13, 768) [ 1.45728 -0.50869  1.32858]\n",
      "\n",
      ".. input to    block 4: float32 (1, 13, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... input to    layer_norm: float32 (1, 13, 768) [ 1.45728 -0.50869  1.32858]\n",
      ".... output from layer_norm: float32 (1, 13, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... input to    SelfAttention: float32 (1, 13, 768) [ 0.01392 -0.01004  0.00562]\n",
      ".... output from SelfAttention: float32 (1, 13, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... input to    projection: float32 (1, 13, 768) [ 0.00018 -0.05062  0.0249 ]\n",
      ".... output from projection: float32 (1, 13, 768) [-0.12963 -0.13992  0.02832]\n",
      ".... input to    mlp: float32 (1, 13, 768) [ 1.32765 -0.64861  1.3569 ]\n",
      ".... output from mlp: float32 (1, 13, 768) [-0.02862  0.05716 -0.1048 ]\n",
      ".. output from block 4: float32 (1, 13, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      "\n",
      ".. input to    block 5: float32 (1, 13, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... input to    layer_norm: float32 (1, 13, 768) [ 1.29903 -0.59145  1.2521 ]\n",
      ".... output from layer_norm: float32 (1, 13, 768) [0.00946 0.00112 0.00496]\n",
      ".... input to    SelfAttention: float32 (1, 13, 768) [0.00946 0.00112 0.00496]\n",
      ".... output from SelfAttention: float32 (1, 13, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... input to    projection: float32 (1, 13, 768) [-0.01855 -0.02682  0.00128]\n",
      ".... output from projection: float32 (1, 13, 768) [-0.02085 -0.12538  0.09911]\n",
      ".... input to    mlp: float32 (1, 13, 768) [ 1.27818 -0.71683  1.35121]\n",
      ".... output from mlp: float32 (1, 13, 768) [ 0.00075  0.02107 -0.08904]\n",
      ".. output from block 5: float32 (1, 13, 768) [ 1.27893 -0.69576  1.26217]\n",
      "\n",
      ".. input to    block 6: float32 (1, 13, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... input to    layer_norm: float32 (1, 13, 768) [ 1.27893 -0.69576  1.26217]\n",
      ".... output from layer_norm: float32 (1, 13, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... input to    SelfAttention: float32 (1, 13, 768) [ 0.01298 -0.00263  0.01241]\n",
      ".... output from SelfAttention: float32 (1, 13, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... input to    projection: float32 (1, 13, 768) [ 0.03137 -0.04837  0.01207]\n",
      ".... output from projection: float32 (1, 13, 768) [-0.01304 -0.01774  0.07926]\n",
      ".... input to    mlp: float32 (1, 13, 768) [ 1.26589 -0.71349  1.34143]\n",
      ".... output from mlp: float32 (1, 13, 768) [-0.07322  0.03609 -0.05765]\n",
      ".. output from block 6: float32 (1, 13, 768) [ 1.19267 -0.6774   1.28378]\n",
      "\n",
      ".. input to    block 7: float32 (1, 13, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... input to    layer_norm: float32 (1, 13, 768) [ 1.19267 -0.6774   1.28378]\n",
      ".... output from layer_norm: float32 (1, 13, 768) [0.01891 0.00337 0.02008]\n",
      ".... input to    SelfAttention: float32 (1, 13, 768) [0.01891 0.00337 0.02008]\n",
      ".... output from SelfAttention: float32 (1, 13, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... input to    projection: float32 (1, 13, 768) [-0.03964  0.04888 -0.05002]\n",
      ".... output from projection: float32 (1, 13, 768) [-0.05198  0.02849  0.05123]\n",
      ".... input to    mlp: float32 (1, 13, 768) [ 1.14068 -0.64891  1.33501]\n",
      ".... output from mlp: float32 (1, 13, 768) [ 0.0008   0.1006  -0.18067]\n",
      ".. output from block 7: float32 (1, 13, 768) [ 1.14148 -0.54831  1.15433]\n",
      "\n",
      ".. input to    block 8: float32 (1, 13, 768) [ 1.14148 -0.54831  1.15433]\n",
      ".... input to    layer_norm: float32 (1, 13, 768) [ 1.14148 -0.54831  1.15433]\n",
      ".... output from layer_norm: float32 (1, 13, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... input to    SelfAttention: float32 (1, 13, 768) [ 0.01511 -0.00211  0.02452]\n",
      ".... output from SelfAttention: float32 (1, 13, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... input to    projection: float32 (1, 13, 768) [ 0.05814 -0.01419 -0.01491]\n",
      ".... output from projection: float32 (1, 13, 768) [-0.00857 -0.03607  0.04864]\n",
      ".... input to    mlp: float32 (1, 13, 768) [ 1.13291 -0.58438  1.20297]\n",
      ".... output from mlp: float32 (1, 13, 768) [ 0.06728  0.01519 -0.08793]\n",
      ".. output from block 8: float32 (1, 13, 768) [ 1.20019 -0.56919  1.11504]\n",
      "\n",
      ".. input to    block 9: float32 (1, 13, 768) [ 1.20019 -0.56919  1.11504]\n",
      ".... input to    layer_norm: float32 (1, 13, 768) [ 1.20019 -0.56919  1.11504]\n",
      ".... output from layer_norm: float32 (1, 13, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... input to    SelfAttention: float32 (1, 13, 768) [ 0.0182  -0.00647  0.03108]\n",
      ".... output from SelfAttention: float32 (1, 13, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... input to    projection: float32 (1, 13, 768) [-0.04656 -0.09386  0.0085 ]\n",
      ".... output from projection: float32 (1, 13, 768) [-0.08596  0.07275 -0.0108 ]\n",
      ".... input to    mlp: float32 (1, 13, 768) [ 1.11423 -0.49644  1.10423]\n",
      ".... output from mlp: float32 (1, 13, 768) [ 0.02671  0.05731 -0.24935]\n",
      ".. output from block 9: float32 (1, 13, 768) [ 1.14094 -0.43913  0.85489]\n",
      "\n",
      ".. input to    block 10: float32 (1, 13, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... input to    layer_norm: float32 (1, 13, 768) [ 1.14094 -0.43913  0.85489]\n",
      ".... output from layer_norm: float32 (1, 13, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... input to    SelfAttention: float32 (1, 13, 768) [ 0.01647 -0.01187  0.03497]\n",
      ".... output from SelfAttention: float32 (1, 13, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... input to    projection: float32 (1, 13, 768) [ 0.01163  0.03214 -0.07898]\n",
      ".... output from projection: float32 (1, 13, 768) [-0.04235  0.08199 -0.10805]\n",
      ".... input to    mlp: float32 (1, 13, 768) [ 1.09858 -0.35714  0.74684]\n",
      ".... output from mlp: float32 (1, 13, 768) [-0.12762  0.10682 -0.23868]\n",
      ".. output from block 10: float32 (1, 13, 768) [ 0.97096 -0.25032  0.50816]\n",
      "\n",
      ".. input to    block 11: float32 (1, 13, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... input to    layer_norm: float32 (1, 13, 768) [ 0.97096 -0.25032  0.50816]\n",
      ".... output from layer_norm: float32 (1, 13, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... input to    SelfAttention: float32 (1, 13, 768) [ 0.03263 -0.0169   0.05305]\n",
      ".... output from SelfAttention: float32 (1, 13, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... input to    projection: float32 (1, 13, 768) [ 0.07365 -0.11771 -0.18177]\n",
      ".... output from projection: float32 (1, 13, 768) [-0.03339  0.05015 -0.24095]\n",
      ".... input to    mlp: float32 (1, 13, 768) [ 0.93757 -0.20016  0.26721]\n",
      ".... output from mlp: float32 (1, 13, 768) [-0.45963  1.20928 -1.27699]\n",
      ".. output from block 11: float32 (1, 13, 768) [ 0.47794  1.00912 -1.00978]\n",
      "shape: [1, 13, 768]\n",
      "x reshaped: (13, 768)\n",
      "logits.shape= (13, 50257)\n",
      "logits reshaped: (1, 13, 50257)\n",
      "Output text:\n",
      " Every effort moves you forward.\n",
      "\n",
      "The first step is to understand\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=gpt2,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=256\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7242dc-e52b-4b19-92aa-64ddda75e6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
