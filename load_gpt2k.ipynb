{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0201b516-a46c-4d04-bb57-95994710fbfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe46ed5fd31405389414f139d7c3c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c828576db8747628d72aeadca47bdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d7a18c321741a8851ef9443e0e904d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29ca531158446dba194dd9a7f312d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e383bcb2e443a194afe6cc424ace89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import tldr\n",
    "from utils import load_gpt2_params_from_tf_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "48f0a2b1-4d75-48fe-aa30-c16bfc4ebd6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Embedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size, vocab_size, max_position_length, dtype=tf.float32):\n",
    "        super().__init__(name=\"embedding\", dtype=dtype)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_position_length = max_position_length\n",
    "        self.word_embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, name=\"word_embedding\")\n",
    "        # self.word_embedding.build((None, self.embedding_size))\n",
    "        self.position_embedding = tf.keras.layers.Embedding(input_dim=self.max_position_length, output_dim=self.embedding_size, name=\"position_embedding\")\n",
    "        # self.position_embedding.build((None, self.embedding_size))\n",
    "        self.debug = False\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape[-1] is the number of features from the previous layer\n",
    "        if self.debug:       \n",
    "            print(\"..Embedding input_shape=\", input_shape)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        we = self.word_embedding(inputs)\n",
    "        pe = self.position_embedding(tf.range(self.max_position_length))\n",
    "        pe_corrected = pe[:we.shape[1], :]\n",
    "        x = we + pe_corrected\n",
    "        if self.debug:\n",
    "            print(f\".. Embedding output: {tldr(x)}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f554b397-940a-4a3e-ba12-fa315618e0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, b, use_outproj=False): # , use_outproj=False):\n",
    "        super().__init__(name=\"self\")\n",
    "        self.debug = False\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        qkv_bias=True # ?\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        # self.query_layer = tf.keras.layers.Dense(units=self.head_dim, activation=None, name=f\"query-{b}\", use_bias=qkv_bias)\n",
    "        self.query_layer = tf.keras.layers.Dense(units=self.d_out, activation=None, name=f\"query-{b}\", use_bias=qkv_bias)\n",
    "        # self.query_layer.build((None, d_in))\n",
    "        self.key_layer = tf.keras.layers.Dense(units=self.d_out, activation=None, name=f\"key-{b}\", use_bias=qkv_bias)\n",
    "        # self.key_layer.build((None, d_in))\n",
    "        self.value_layer = tf.keras.layers.Dense(units=self.d_out, activation=None, name=f\"value-{b}\", use_bias=qkv_bias)\n",
    "        # self.value_layer.build((None, d_in))\n",
    "        self.use_outproj = use_outproj\n",
    "\n",
    "        self.out_proj = tf.keras.layers.Dense(units=d_out, activation=None, name=f\"proj-{b}\", use_bias=True)\n",
    "        # self.out_proj.build((None, d_out))\n",
    "\n",
    "        mask = tf.ones((context_length, context_length), dtype=tf.bool) # square matrix of True\n",
    "        causal_mask = tf.linalg.band_part(mask, num_lower=-1, num_upper=0) # upper right becomes False\n",
    "        additive_mask = 1.0 - tf.cast(causal_mask, dtype=tf.float32) # upper right becomes 1.0\n",
    "        self.additive_mask_applied = additive_mask * -1e9   # upper right is large negative value\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape[-1] is the number of features from the previous layer\n",
    "        if self.debug:         \n",
    "            print(\"..SelfAttention input_shape=\", input_shape)\n",
    "        super().build(input_shape) \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        if self.debug:\n",
    "            print(f\".... input to    SelfAttention: {tldr(inputs)}\")\n",
    "        batch_size, num_tokens, d_in = inputs.shape\n",
    "\n",
    "        keys = self.key_layer(inputs)      \n",
    "        queries = self.query_layer(inputs)\n",
    "        values = self.value_layer(inputs)\n",
    "        \n",
    "        keys = tf.reshape(keys, [batch_size, num_tokens, self.num_heads, self.head_dim ]) # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim): 2, 6, 2 -> 2, 6, 2, 1        \n",
    "        queries = tf.reshape(queries, [batch_size, num_tokens, self.num_heads, self.head_dim ]) # 2, 6, 2, 1\n",
    "        values = tf.reshape(values, [batch_size, num_tokens, self.num_heads, self.head_dim ]) # 2, 6, 2, 1\n",
    "\n",
    "        keys = tf.transpose(keys, perm=[0, 2, 1, 3])      # [2,6,2,1] ->  [2, 2, 6, 1]        \n",
    "        values = tf.transpose(values, perm=[0, 2, 1, 3])      # [2,6,2,1] ->  [2, 2, 6, 1]\n",
    "        queries = tf.transpose(queries, perm=[0, 2, 1, 3])      # [2,6,2,1] ->  [2, 2, 6, 1]\n",
    "        \n",
    "        attn_scores = tf.matmul(queries, tf.transpose(keys, perm=[0, 1, 3, 2]))\n",
    "        trimmed_additive_mask_applied = self.additive_mask_applied[:num_tokens, :num_tokens]\n",
    "        attn_scores = attn_scores + trimmed_additive_mask_applied\n",
    "        attn_weights = tf.nn.softmax(attn_scores / keys.shape[-1]**0.5, axis=-1)\n",
    "\n",
    "        context_vec = tf.matmul(attn_weights, values)        \n",
    "        context_vec = tf.transpose(context_vec, perm=[0, 2, 1, 3])\n",
    "        context_vec = tf.reshape(context_vec, [batch_size, num_tokens, self.d_out]) # (b, num_tokens, self.d_out)\n",
    "\n",
    "        if self.use_outproj: # llmfs uses this!\n",
    "            context_vec = self.out_proj(context_vec)\n",
    "        if self.debug:\n",
    "            print(f\".... output from SelfAttention: {tldr(context_vec)}\")\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "416d41e0-ca08-4117-b89b-5163dcbd890b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, b):\n",
    "        super().__init__(name=\"attention\")\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(name=\"layer_norm\") \n",
    "        self.self_attention = SelfAttention(d_in, d_out, context_length, dropout, num_heads, b)\n",
    "        self.projection = tf.keras.layers.Dense(units=d_out, activation=None, name=f\"projection\")\n",
    "        self.debug = False\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape[-1] is the number of features from the previous layer\n",
    "        if self.debug:         \n",
    "            print(\"..AttentionLayer input_shape=\", input_shape)\n",
    "        super().build(input_shape)  \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        if self.debug:\n",
    "            print(f\".... input to    layer_norm: {tldr(inputs)}\")\n",
    "        x = self.layer_norm(inputs)\n",
    "        if self.debug:\n",
    "            print(f\".... output from layer_norm: {tldr(x)}\")\n",
    "        x = self.self_attention(x)\n",
    "        if self.debug:\n",
    "            print(f\".... input to    projection: {tldr(x)}\")\n",
    "        x = self.projection(x)\n",
    "        if self.debug:\n",
    "            print(f\".... output from projection: {tldr(x)}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3702cc92-69b0-4e5b-935b-7bfe2c34a864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_out, b):\n",
    "        super().__init__(name=\"mlp\")\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(name=f\"layer_norm\")\n",
    "        self.perceptron = tf.keras.layers.Dense(units=d_out * 4, activation=tf.keras.activations.gelu, name=f\"perceptron\")\n",
    "        self.projection = tf.keras.layers.Dense(units=d_out, name=f\"projection\")\n",
    "        self.debug = False\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.debug:\n",
    "            print(f\".... input to    mlp: {tldr(inputs)}\")\n",
    "        x = self.layer_norm(inputs)\n",
    "        x = self.perceptron(x)\n",
    "        x = self.projection(x)\n",
    "        if self.debug:\n",
    "            print(f\".... output from mlp: {tldr(x)}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "daae6dd5-5c1a-4fed-8866-a00022f95507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, b, qkv_bias=False):\n",
    "        super().__init__(name=f'block-{b}')\n",
    "        self.b = b       \n",
    "\n",
    "        self.attention = AttentionLayer(d_in, d_out, context_length, dropout, num_heads, b)\n",
    "        self.mlp = MultiLayerPerceptron(d_out, b)\n",
    "        self.debug = False\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape[-1] is the number of features from the previous layer\n",
    "        if self.debug:         \n",
    "            print(\"..Block input_shape=\", input_shape)\n",
    "        super().build(input_shape)          \n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.debug:\n",
    "            print()\n",
    "            print(f\".. input to    block {self.b}: {tldr(inputs)}\")\n",
    "        x = inputs\n",
    "        a = self.attention(x)\n",
    "        x = x + a\n",
    "        m = self.mlp(x)\n",
    "        x = x + m\n",
    "        if self.debug:\n",
    "            print(f\".. output from block {self.b}: {tldr(x)}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "00c1abfa-3391-4512-a635-5e9c215d69d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, blocks_num, d_in, d_out, context_length, dropout, num_heads):\n",
    "        super().__init__(name=\"transformer\")\n",
    "        self.blocks_num = blocks_num\n",
    "        self.blocks = []\n",
    "        self.debug = False\n",
    "        for b in range(blocks_num):\n",
    "            block = Block(d_in, d_out, context_length, dropout, num_heads, b)\n",
    "            self.blocks.append(block)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(name=f\"layer_norm\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape[-1] is the number of features from the previous layer\n",
    "        if self.debug:        \n",
    "            print(\"..Transformer input_shape=\", input_shape)\n",
    "        super().build(input_shape)        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for b in range(self.blocks_num):\n",
    "            x = self.blocks[b](x)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d4c17dc9-c93d-4d8d-b5ee-34ed50b6684a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.keras.saving.register_keras_serializable()\n",
    "class GPT2k(tf.keras.Model):\n",
    "    def __init__(self, config, name=None, trainable=True, dtype=None):\n",
    "        super().__init__(name=name)\n",
    "        self.trainable = trainable\n",
    "        self.embedding_size=config['n_embd']\n",
    "        self.vocab_size=config['n_vocab']\n",
    "        self.max_position_length=config['n_ctx']\n",
    "        self.blocks_num = config[\"n_layer\"]\n",
    "        d_in=config['n_embd']\n",
    "        d_out=config['n_embd']\n",
    "        context_length = config['n_ctx']\n",
    "        num_heads = config['n_head']\n",
    "        self.embedding = Embedding(embedding_size=self.embedding_size, vocab_size=self.vocab_size, max_position_length=self.max_position_length)\n",
    "        self.transformer = Transformer(self.blocks_num, d_in, d_out, context_length, dropout=None, num_heads=num_heads)\n",
    "        self.debug = False\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        # return x\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\".. input to final matmul: {tldr(x)}\")\n",
    "        logits = tf.matmul(x, self.embedding.word_embedding.weights[0], transpose_b=True)\n",
    "        if self.debug:\n",
    "            print(f\".. final logits: {tldr(logits)}\")\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "6edd6e7e-f4cb-4e85-a5e4-57bb77d5f7e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config124M = {'n_embd': 768, 'n_vocab': 50257, 'n_ctx': 1024, 'n_layer': 12, 'n_head': 12}\n",
    "gpt2k=GPT2k(config124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2ed6ff75-05a9-463b-877d-d8ef50af26dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpt2k.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "57b2a8b3-e0c2-420e-9ef1-e2dc0360f562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gpt2k.embedding.build((None, 768))\n",
    "gpt2k.build((2, 768))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8654ced4-3c78-42ab-92f6-11d73e290ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gpt2k_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  39383808  \n",
      "                                                                 \n",
      " transformer (Transformer)   multiple                  85056000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124439808 (474.70 MB)\n",
      "Trainable params: 124439808 (474.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gpt2k.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "7cd642dc-b2be-4ad5-a86b-b3ecda73fd4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'gpt2k_25',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'config': {'n_embd': 768,\n",
       "  'n_vocab': 50257,\n",
       "  'n_ctx': 1024,\n",
       "  'n_layer': 12,\n",
       "  'n_head': 12}}"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2k.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "010e2e9e-e59e-4fb3-a295-3829a8337ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2k.embedding.word_embedding.built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b72abb1f-78ba-455b-b6d5-4c892313c61a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=0\n",
    "gpt2k.transformer.blocks[b].attention.self_attention.query_layer.built\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "894b62c7-2d12-41f0-808b-ce96912d6043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir=\"openai_gpt2_weights/124M\"\n",
    "tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, config124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c8b1f636-8663-47ac-8b0f-2420a8e6b653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_weights_into_gpt2k(model, params):\n",
    "    assert model.built\n",
    "    model.embedding.word_embedding.set_weights([np.array(params['wte'])])\n",
    "    model.embedding.position_embedding.set_weights([np.array(params['wpe'])])\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        assert model.transformer.blocks[b].attention.layer_norm.built\n",
    "        model.transformer.blocks[b].attention.layer_norm.beta = tf.Variable(params[\"blocks\"][b][\"ln_1\"][\"b\"]) \n",
    "        model.transformer.blocks[b].attention.layer_norm.gamma = tf.Variable(params[\"blocks\"][b][\"ln_1\"][\"g\"]) \n",
    "\n",
    "\n",
    "        # gpt.transformer.blocks[b].attention.layer_norm.beta_initializer = tf.keras.initializers.Constant(params[\"blocks\"][b][\"ln_1\"][\"b\"]) \n",
    "        # gpt.transformer.blocks[b].attention.layer_norm.gamma_initializer = tf.keras.initializers.Constant(params[\"blocks\"][b][\"ln_1\"][\"g\"]) \n",
    "        # gpt.transformer.blocks[b].attention.layer_norm.build((None, None, n_embd))\n",
    "\n",
    "\n",
    "        q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        \n",
    "        # assert gpt.transformer.blocks[b].attention.self_attention.query_layer.built\n",
    "        model.transformer.blocks[b].attention.self_attention.query_layer.set_weights([q_w, q_b])\n",
    "        # assert gpt.transformer.blocks[b].attention.self_attention.key_layer.built\n",
    "        model.transformer.blocks[b].attention.self_attention.key_layer.set_weights([k_w, k_b])\n",
    "        # assert gpt.transformer.blocks[b].attention.self_attention.value_layer.built\n",
    "        model.transformer.blocks[b].attention.self_attention.value_layer.set_weights([v_w, v_b])\n",
    "    \n",
    "        # AttentionLayer projection\n",
    "        # model.transformer.blocks[b].attention.projection.build((None, n_embd))\n",
    "        # assert gpt.transformer.blocks[b].attention.projection.built\n",
    "        model.transformer.blocks[b].attention.projection.set_weights([params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"], params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"]])      \n",
    "    \n",
    "        # MultiLayerPerceptron layer_norm\n",
    "        model.transformer.blocks[b].mlp.layer_norm.beta = tf.Variable(params[\"blocks\"][b][\"ln_2\"][\"b\"]) \n",
    "        model.transformer.blocks[b].mlp.layer_norm.gamma = tf.Variable(params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        # model.transformer.blocks[b].mlp.layer_norm.build((None, None, n_embd))\n",
    "        # assert gpt.transformer.blocks[b].mlp.layer_norm.built\n",
    "    \n",
    "        # MultiLayerPerceptron perceptron\n",
    "        # model.transformer.blocks[b].mlp.perceptron.build((None, n_embd))\n",
    "        # assert gpt.transformer.blocks[b].mlp.perceptron.built\n",
    "        model.transformer.blocks[b].mlp.perceptron.set_weights([params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"], params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]])\n",
    "        # MultiLayerPerceptron projection\n",
    "        mlp_proj_embd = params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].shape[0]\n",
    "        # gpt.transformer.blocks[b].mlp.projection.build((None, mlp_proj_embd))\n",
    "        # assert gpt.transformer.blocks[b].mlp.projection.built\n",
    "        model.transformer.blocks[b].mlp.projection.set_weights([params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"], params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"]])           \n",
    "        \n",
    "        \n",
    "        \n",
    "    model.transformer.layer_norm.beta = tf.Variable(params[\"b\"])\n",
    "    model.transformer.layer_norm.gamma = tf.Variable(params[\"g\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d5871c54-55a2-4d52-875a-3ced63bce79a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_weights_into_gpt2k(gpt2k, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8cc815-a3f0-4081-8a7a-60265bdb0c8e",
   "metadata": {},
   "source": [
    "### Validate embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7d012d3b-3165-4869-98d4-8f5b65332a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_effort = tf.constant([[6109, 3626, 6100, 345]])\n",
    "x_effort_emb = gpt2k.embedding(x_effort)\n",
    "assert x_effort_emb.shape == [1, 4, 768]\n",
    "assert np.allclose(x_effort_emb[0][0][:3], np.array([0.07927368, -0.2979193 ,  0.08817437]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9724c-49df-4391-a349-7fd5b905f4b8",
   "metadata": {},
   "source": [
    "### Validate attention.layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "ecf66489-35da-476f-90d6-f1d97cb612dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = gpt2k.transformer.blocks[0].attention.layer_norm(x_effort_emb)\n",
    "assert y.shape == [1, 4, 768]\n",
    "assert np.allclose(y[0][0][:3], np.array([0.047223  , -0.11664161, -0.02536647]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3618f-ba8d-4a23-ace3-87441e510922",
   "metadata": {},
   "source": [
    "### Validate self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "efdb38ba-11f2-42da-9c66-01b806eece82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = gpt2k.transformer.blocks[0].attention.self_attention(x_effort_emb)\n",
    "assert y.shape == [1, 4, 768]\n",
    "assert np.allclose(y[0][0][:3], np.array([0.28434375, -0.00881347,  0.34210888]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ae102-e6df-4852-9d4e-e92d86ef06b9",
   "metadata": {},
   "source": [
    "### Validate the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "fc26cf8f-4fdb-488a-b1e4-750142adb401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32 (1, 4, 50257) [-35.526115 -34.928413 -38.399166]'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = gpt2k(x_effort)\n",
    "assert y.shape == [1, 4, 50257]\n",
    "assert np.allclose(y[0][0][:3], np.array([-35.521214, -34.924126, -38.39469]), rtol=1e-2, atol=1e-2)\n",
    "tldr(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a142cf-9e44-4328-b723-0640090ac893",
   "metadata": {},
   "source": [
    "### Write to checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4b84c040-ba25-4dab-9764-a8c9dddd5042",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_checkpoint'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chkp_path = \"my_checkpoint\"\n",
    "checkpoint = tf.train.Checkpoint(model=gpt2k)\n",
    "checkpoint.write(chkp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "09e76275-d06a-4ce1-8611-b2709e2eb0ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpt2k.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99b188-ef33-4433-ba7e-cee259a968cf",
   "metadata": {},
   "source": [
    "### Reload checkpoint into new model gpt2z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "959a0ab1-85de-46bc-b50f-64ae10392f85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f5df8392110>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config124M = {'n_embd': 768, 'n_vocab': 50257, 'n_ctx': 1024, 'n_layer': 12, 'n_head': 12}\n",
    "gpt2z=GPT2k(config124M)\n",
    "checkpoint = tf.train.Checkpoint(model=gpt2z)\n",
    "checkpoint.restore(chkp_path)\n",
    "# gpt2z.build((2, 768))\n",
    "# assert len(gpt2z.variables) == len(gpt2k.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de822f2-0b50-4dba-8931-81d7a914c261",
   "metadata": {},
   "source": [
    "### Revalidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "83e2f4aa-5867-4dd4-a54e-13373469c5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_effort = tf.constant([[6109, 3626, 6100, 345]])\n",
    "x_effort_emb = gpt2z.embedding(x_effort)\n",
    "assert x_effort_emb.shape == [1, 4, 768]\n",
    "assert np.allclose(x_effort_emb[0][0][:3], np.array([0.07927368, -0.2979193 ,  0.08817437]), rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ab1960e1-fa6e-4aed-b460-5bec7b0186e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32 (1, 4, 50257) [-35.526115 -34.928413 -38.399166]'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = gpt2z(x_effort)\n",
    "assert y.shape == [1, 4, 50257]\n",
    "assert np.allclose(y[0][0][:3], np.array([-35.521214, -34.924126, -38.39469]), rtol=1e-2, atol=1e-2)\n",
    "tldr(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b93f6e92-137b-4b47-a510-2e1f56394afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2k.save('gpt2k.keras')  # The file needs to end with the .keras extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "51295637-9b83-4363-b632-f21a1e5a4228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('gpt2k.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "dce4f22a-e250-423e-a523-feda9c82b957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = model(x_effort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "922914de-c104-41c6-ab93-f95f292bb5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32 (1, 4, 50257) [-35.526115 -34.928413 -38.399166]'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert y.shape == [1, 4, 50257]\n",
    "assert np.allclose(y[0][0][:3], np.array([-35.521214, -34.924126, -38.39469]), rtol=1e-2, atol=1e-2)\n",
    "tldr(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b96a345f-2c7e-43d9-acb1-bebb2e1cede5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47f04e-4273-449b-84fd-9a1b08f771bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
